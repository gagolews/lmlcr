<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->

# Shallow and Deep Neural Networks




## Introduction

### Binary Logistic Regression: Recap




Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be an input matrix
that consists of $n$ points in a $p$-dimensional space.

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\]

In other words, we have a database on $n$ objects.
Each object is described by means of $p$ numerical features.



With each input $\mathbf{x}_{i,\cdot}$ we associate the desired output
$y_i$ which is a categorical label -- hence we
will be dealing with **classification** tasks again.





To recall, in **binary logistic regression** we model
the probabilities that
a given input belongs to either of the two classes:

\[
\begin{array}{lr}
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=& \phi(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)\\
\Pr(Y=0|\mathbf{X},\boldsymbol\beta)=& 1-\phi(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)\\
\end{array}
\]
where $\phi(t) = \frac{1}{1+e^{-t}}=\frac{e^t}{1+e^t}$ is the logistic sigmoid function.

It holds:
\[
\begin{array}{ll}
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=&\displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}},\\
\Pr(Y=0|\mathbf{X},\boldsymbol\beta)=&
\displaystyle\frac{1}{1+e^{+(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}=\displaystyle\frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}.\\
\end{array}
\]




The fitting of the model was performed by minimising the cross-entropy (log-loss):
\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\Bigg(y_i\log \hat{y}_i + (1-y_i)\log (1-\hat{y}_i)\Bigg).
\]
where $\hat{y}_i=\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)$.

This is equivalent to:
\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\Bigg(y_i\log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta) + (1-y_i)\log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)\Bigg).
\]

Note that for each $i$,
either the left or the right term (in the bracketed expression) vanishes.

Hence, we may also write the above as:
\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\boldsymbol\beta).
\]


<!--Taking into account the fact that
$\log \frac{a}{b} = \log{a}-\log{b}$,
$\log e^{a} = a$ and $\log 1 = 0$, we can rewrite the above as:-->



<!--\[
-\frac{1}{n} \sum_{i=1}^n \left(
    y_i\log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta) +
    \log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
    - y_i \log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
\right)
\]-->


<!--\[
-\frac{1}{n} \sum_{i=1}^n \left(
    y_i\log \frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}} +
    \log \frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
    - y_i \log \frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
\right)
\]-->

<!--\[
-\frac{1}{n} \sum_{i=1}^n \left(
    -y_i\log \left({1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\right)
    +       \log \left({e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\right)
    -       \log \left({1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}\right)
    -y_i\log \left({e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\right)
    +y_i\log \left({1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}\right)
\right)
\]-->

<!--\[
\frac{1}{n} \sum_{i=1}^n \left(
    (1-y_i)    \left(\beta_0 + \beta_1 x_{i,1} +  \dots + \beta_p x_{i,p})\right)
    +       \log \left({1+e^{-(\beta_0 + \beta_1 x_{i,1}+ \dots + \beta_p x_{i,p})}}\right)
\right)
\]-->










In this chapter we will generalise the binary logistic regression model:

- First we will consider the case of many classes
(multiclass classification).
This will lead to the multinomial logistic regression model.

- Then we will note that the multinomial logistic regression is a special
case of a feed-forward neural network.





### Data


We will study the famous classic -- the MNIST image classification dataset
(Modified National Institute of Standards and Technology database),
see http://yann.lecun.com/exdb/mnist/

It consists of 28×28 pixel images of handwritten digits:

* `train`: 60,000 training images,
* `t10k`: 10,000 testing images.





A few image instances from each class are depicted in
Figure \@ref(fig:mnist-demo).

```{r mnist-demo,cache=TRUE,echo=FALSE,fig.height=3,fig.cap="Example images in the MNIST database"}
library("keras")
mnist <- dataset_mnist()
par(mar=c(0,0,0,0))
par(ann=FALSE)
set.seed(123)
par(mfrow=c(10,20))
for (i in 0:9) {
    ids <- sample(which(mnist$train$y == i))
    for (j in 1:20) {
        id <- ids[j]
        image(z=t(mnist$train$x[id,,])/255, col=grey.colors(256, 0, 1),
    axes=FALSE, asp=1, ylim=c(1, 0))
    }
}
```

There are 10 unique digits, so this is a multiclass classification problem.


Remark.

: The dataset is already "too easy" for testing of the state-of-the-art
classifiers (see the notes below), but it's a great educational example.






Accessing MNIST via the `keras` package
(which we will use throughout this chapter anyway) is easy:



```{r mnist_download,cache=TRUE}
library("keras")
mnist <- dataset_mnist()
X_train <- mnist$train$x
Y_train <- mnist$train$y
X_test  <- mnist$test$x
Y_test  <- mnist$test$y
```



<!--


```{cache=TRUE,mnist_download,warning=FALSE}
dir.create("mnist")
files <- c("train-images-idx3-ubyte.gz",
           "train-labels-idx1-ubyte.gz",
           "t10k-images-idx3-ubyte.gz",
           "t10k-labels-idx1-ubyte.gz")
for (file in files) {
    cat(sprintf("downloading %s...\n", file))
    download.file(sprintf("http://yann.lecun.com/exdb/mnist/%s", file),
              sprintf("mnist/%s", file))
}
```
-->





`X_train` and `X_test` consist of 28×28 pixel images.

```{r mnist_info,cache=TRUE}
dim(X_train)
dim(X_test)
```

`X_train` and `X_test` are 3-dimensional arrays, think
of them as vectors of 60000 and 10000 matrices of size 28×28, respectively.






These are grey-scale images, with 0 = black, ..., 255 = white:

```{r mnist_info2,cache=TRUE}
range(X_train)
```

Numerically, it's more convenient to work with colour values
converted to 0.0 = black, ..., 1.0 = white:

```{r mnist_info2a,cache=TRUE}
X_train <- X_train/255
X_test  <- X_test/255
```






`Y_train` and `Y_test` are the corresponding integer labels:

```{r mnist_info3,cache=TRUE}
length(Y_train)
length(Y_test)
table(Y_train) # label distribution in the training sample
table(Y_test)  # label distribution in the test sample
```


Here is how we can plot one of the digits (see Figure \@ref(fig:mnist-info2b)):


```{r mnist-info2b,cache=TRUE,echo=-(1:2),fig.cap="Example image from the MNIST dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
id <- 123 # image ID to show
image(z=t(X_train[id,,]), col=grey.colors(256, 0, 1),
    axes=FALSE, asp=1, ylim=c(1, 0))
legend("topleft", bg="white",
    legend=sprintf("True label=%d", Y_train[id]))
```








## Multinomial Logistic Regression

### A Note on Data Representation





So... you may now be wondering "how do we construct an image classifier,
this seems so complicated!".

For a computer, (almost) everything is just numbers.

Instead of playing with $n$ matrices, each of size 28×28,
we may "flatten" the images so as to get
$n$ "long" vectors of length $p=784$.

```{r mnist_flatten,cache=TRUE}
X_train2 <- matrix(X_train, ncol=28*28)
X_test2  <- matrix(X_test, ncol=28*28)
```

The classifiers studied here do not take the "spatial" positioning of
the pixels into account anyway. Hence, now we're back to our "comfort zone".


Remark.

: (*) See, however, convolutional neural networks (CNNs),
e.g., in [@deeplearn].




### Extending Logistic Regression





Let us generalise the binary logistic regression model
to a 10-class one (or, more generally, $K$-class one).

This time we will be modelling ten probabilities,
with
$\Pr(Y=k|\mathbf{X},\mathbf{B})$ denoting the *confidence* that a given image $\mathbf{X}$
is in fact the $k$-th digit:

\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&=&\dots\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&=&\dots\\
&\vdots&\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&=&\dots\\
\end{array}
\]

where $\mathbf{B}$ is the set of underlying model parameters
(to be determined soon).





In binary logistic regression,
the class probabilities are obtained by "cleverly normalising" (by means of the logistic sigmoid)
the outputs of a linear model (so that we obtain a value in $[0,1]$).

\[
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=\phi(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)=\displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}
\]

In the multinomial case, we can use a separate linear model for each digit
so that each $\Pr(Y=k|\mathbf{X},\mathbf{B})$, $k=0,1,\dots,9$,
is given as a function of:
\[\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p}.\]

Therefore, instead of a parameter vector of length $(p+1)$,
we will need a parameter matrix of size $(p+1)\times 10$
 representing the model's definition.

Side note.

: The upper case of $\beta$ is $B$.


Then, these 10 numbers will have to be normalised
so as to they are all greater than $0$ and sum to $1$.






To maintain the spirit of the original model,
we can apply $e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}$
to get a positive value,
because the co-domain of the exponential function $t\mapsto e^t$
is $(0,\infty)$.

Then, dividing each output by the sum of all the outputs will guarantee that
the total sum equals 1.

This leads to:
\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&=&\displaystyle\frac{e^{-(\beta_{0,0} + \beta_{1,0} X_{1} +  \dots + \beta_{p,0} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_p)}},\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&=&\displaystyle\frac{e^{-(\beta_{0,1} + \beta_{1,1} X_{1} +  \dots + \beta_{p,1} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}},\\
&\vdots&\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&=&\displaystyle\frac{e^{-(\beta_{0,9} + \beta_{1,9} X_{1} +  \dots + \beta_{p,9} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}}.\\
\end{array}
\]

This reduces to the binary logistic regression
if we consider only the classes $0$ and $1$ and
fix $\beta_{0,0}=\beta_{1,0}=\dots=\beta_{p,0}=0$ (as $e^0=1$).





### Softmax Function




The above transformation (that maps 10 arbitrary real numbers
to positive ones that sum to 1)
is called the **softmax** function (or *softargmax*).


```{r softmax_example}
softmax <- function(T) {
    T2 <- exp(T) # ignore the minus sign above
    T2/sum(T2)
}
round(rbind(
    softmax(c(0, 0, 10, 0, 0, 0, 0,  0, 0, 0)),
    softmax(c(0, 0, 10, 0, 0, 0, 10, 0, 0, 0)),
    softmax(c(0, 0, 10, 0, 0, 0, 9,  0, 0, 0)),
    softmax(c(0, 0, 10, 0, 0, 0, 9,  0, 0, 8))), 2)
```



### One-Hot Encoding and Decoding




The ten class-belongingness-degrees can be decoded
to obtain a single label by simply choosing
the class that is assigned the highest probability.



```{r one_hot_example_1}
y_pred <- softmax(c(0, 0, 10, 0, 0, 0, 9,  0, 0, 8))
round(y_pred, 2) # probabilities of Y=0, 1, 2, ..., 9
which.max(y_pred)-1 # 1..10 -> 0..9
```


Remark.

: `which.max(y)` returns an index `k` such that
`y[k]==max(y)` (recall that in R the first element in
a vector is at index `1`).
Mathematically, we denote this operation
as $\mathrm{arg}\max_{k=1,\dots,K} y_k$.





To make processing the outputs of a logistic regression model more convenient,
we will apply the so-called **one-hot-encoding** of the labels.

Here, each label will be represented as a 0-1 vector
of 10 probabilities -- with probability 1 corresponding
to the true class only.

For instance:

```{r one_hot_example_2}
y <- 2 # true class (this is just an example)
y2 <- rep(0, 10)
y2[y+1] <- 1 # +1 because we need 0..9 -> 1..10
y2  # one-hot-encoded y
```




To one-hot encode *all* the reference outputs in R,
we start with a matrix of size $n\times 10$ populated with "0"s:

```{r mnist_onehot,cache=TRUE}
Y_train2 <- matrix(0, nrow=length(Y_train), ncol=10)
```

Next, for every $i$, we insert a "1" in the $i$-th row
and the (`Y_train[`$i$`]+1`)-th column:

```{r mnist_onehot2,cache=TRUE}
# Note the "+1" 0..9 -> 1..10
Y_train2[cbind(1:length(Y_train), Y_train+1)] <- 1
```

Remark.

: In R, indexing a matrix `A` with a 2-column matrix `B`, i.e., `A[B]`,
allows for an easy access to
`A[B[1,1], B[1,2]]`, `A[B[2,1], B[2,2]]`, `A[B[3,1], B[3,2]]`, ...





Sanity check:

```{r mnist_onehot3}
head(Y_train)
head(Y_train2)
```






Let us generalise the above idea and write a function
that can one-hot-encode any vector of integer labels:

```{r mnist_onehot4,cache=TRUE}
one_hot_encode <- function(Y) {
    stopifnot(is.numeric(Y))
    c1 <- min(Y) # first class label
    cK <- max(Y) # last class label
    K <- cK-c1+1 # number of classes

    Y2 <- matrix(0, nrow=length(Y), ncol=K)
    Y2[cbind(1:length(Y), Y-c1+1)] <- 1
    Y2
}
```

Encode `Y_train` and `Y_test`:

```{r mnist_onehot_final,cache=TRUE}
Y_train2 <- one_hot_encode(Y_train)
Y_test2 <- one_hot_encode(Y_test)
```



### Cross-entropy Revisited




Our classifier will be outputting $K=10$ probabilities.

The true class labels are not one-hot-encoded so that they
are represented as vectors of $K-1$ zeros and a single one.

How to measure the "agreement" between these two?

. . .

In essence, we will  be  comparing
the probability vectors as generated by a classifier, $\hat{Y}$:

```{r cross_entropy_revisited_example1}
round(y_pred, 2)
```

with the one-hot-encoded true probabilities, $Y$:

```{r cross_entropy_revisited_example2}
y2
```




It turns out that one of the definitions of cross-entropy introduced above
already handles the case of multiclass classification:
\[
E(\mathbf{B}) =
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]
The smaller the probability corresponding to the ground-truth class
outputted by the classifier, the higher the penalty, see
Figure \@ref(fig:cross-entropy-revisited-example3).

```{r cross-entropy-revisited-example3,echo=FALSE,fig.cap="The less the classifier is confident about the prediction of the actually true label, the greater the penalty"}
x <- seq(0.1, 1, length.out=101)
y <- -log(x)
plot(x, y, xlab="probability outputted by the classifier",
    ylab="penalty = -log(probability)", type="l")
```









To sum up, we will be solving the optimisation problem:
\[
\min_{\mathbf{B}\in\mathbb{R}^{(p+1)\times 10}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]
This has no analytical solution,
but can be solved using iterative methods
(see the chapter on optimisation).






(*) Side note: A single term in the above formula,
\[
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B})
\]
given:

* `y_pred` -- a vector of 10 probabilities
generated by the model:
\[
\left[\Pr(Y=0|\mathbf{x}_{i,\cdot},\mathbf{B})\  \Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})\ \cdots\ \Pr(Y=9|\mathbf{x}_{i,\cdot},\mathbf{B})\right]
\]
* `y2`  -- a one-hot-encoded version of the true label, $y_i$, of the form:
\[
\left[0\ 0\ \cdots\ 0\ 1\ 0\ \cdots\ 0\right]
\]

can be computed as:

```{r cross_entropy_revisited_example4}
sum(y2*log(y_pred))
```


###  Problem Formulation in Matrix Form  (\*\*)




The definition of a multinomial logistic regression
model for a multiclass classification task involving
classes $\{1,2,\dots,K\}$
is slightly bloated.

Assuming that $\mathbf{X}\in\mathbb{R}^{n\times p}$ is the input matrix,
to compute the $K$ predicted probabilities for the $i$-th input,
\[
\left[
\hat{y}_{i,1}\ \hat{y}_{i,2}\ \cdots\ \hat{y}_{i,K}
\right],
\]
given a parameter matrix $\mathbf{B}^{(p+1)\times K}$, we apply:
\[
\begin{array}{lcl}
\hat{y}_{i,1}=\Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})&=&\displaystyle\frac{e^{\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}},\\
&\vdots&\\
\hat{y}_{i,K}=\Pr(Y=K|\mathbf{x}_{i,\cdot},\mathbf{B})&=&\displaystyle\frac{e^{\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}}.\\
\end{array}
\]

Remark.

: We have dropped the minus sign in the exponentiation for
brevity of notation.
Note that we can always map $b_{j,k}'=-b_{j,k}$.

It turns out we can make use of matrix notation
to tidy the above formulas.




Denote the linear combinations prior to computing the softmax function with:
\[
\begin{array}{lcl}
t_{i,1}&=&\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p},\\
&\vdots&\\
t_{i,K}&=&\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}.\\
\end{array}
\]

We have:

* $x_{i,j}$ -- the $i$-th observation, the $j$-th feature;
* $\hat{y}_{i,k}$ -- the $i$-th observation, the $k$-th class probability;
* $\beta_{j,k}$ -- the coefficient for the $j$-th feature when computing the $k$-th class.

Note that by augmenting $\mathbf{\dot{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}$ by adding a column of 1s, i.e.,
where $\dot{x}_{i,0}=1$ and $\dot{x}_{i,j}=x_{i,j}$ for all $j\ge 1$ and all $i$, we can write the above as:
\[
\begin{array}{lclcl}
t_{i,1}&=&\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,1} &=& \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,1},\\
&\vdots&\\
t_{i,K}&=&\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,K} &=& \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,K}.\\
\end{array}
\]




We can get the $K$ linear combinations all at once
in the form of a row vector by writing:
\[
\left[
t_{i,1}\ t_{i,2}\ \cdots\ t_{i,K}
\right]
=
{\mathbf{x}_{i,\cdot}}\, \mathbf{B}.
\]

Moreover, we can do that for all the $n$ inputs by writing:
\[
\mathbf{T}=\dot{\mathbf{X}}\,\mathbf{B}.
\]
Yes yes yes! This is a single matrix multiplication,
we have $\mathbf{T}\in\mathbb{R}^{n\times K}$.

To obtain $\hat{\mathbf{Y}}$, we have to apply the softmax function
on every row of $\mathbf{T}$:
\[
\hat{\mathbf{Y}}=\mathrm{softmax}\left(
\dot{\mathbf{X}}\,\mathbf{B}
\right).
\]

That's it. Take some time to appreciate the elegance of this notation.

Methods for minimising cross-entropy expressed in matrix form
will be discussed in the next chapter.




## Artificial Neural Networks

### Artificial Neuron




A neuron can be thought of as a mathematical function,
see Figure \@ref(fig:neuron), which has its specific inputs
and an output.

![(\#fig:neuron) Neuron as a mathematical (black box) function; image based on: https://en.wikipedia.org/wiki/File:Neuron3.png by Egm4313.s12 at English Wikipedia, licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license](figures/neuron)

The Linear Threshold Unit (McCulloch and Pitts, 1940s),
the Perceptron (Rosenblatt, 1958) and the Adaptive Linear Neuron
(Widrow and Hoff, 1960) were amongst the first
models of an artificial neuron that could be used
for the purpose of pattern recognition, see Figure \@ref(fig:perceptron).
They can be thought of as processing units that compute
a weighted sum of the inputs,
which is then transformed by means of a nonlinear "activation" function.

![(\#fig:perceptron) A simple model of an artificial neuron](figures/perceptron)




### Logistic Regression as a Neural Network


The above resembles our binary logistic regression model,
where we determine a linear combination (a weighted sum) of $p$ inputs
and then transform it using the logistic sigmoid "activation" function.
We can easily depict it in the Figure \@ref(fig:neuron)-style,
see Figure \@ref(fig:logistic-regression-binary).


![(\#fig:logistic-regression-binary) Binary logistic regression](figures/logistic_regression_binary)



On the other hand, a multiclass logistic regression can be depicted as
in Figure \@ref(fig:logistic-regression-multiclass).
In fact, we can consider it as an instance of a:

- **single layer** (there is only one processing step that consists of 10 units),
- **densely connected** (all the inputs are connected to all the components below),
- **feed-forward** (the outputs are generated by processing the inputs from "top" to "bottom", there are no loops in the graph etc.)

*artificial* **neural network**
that uses the softmax as the activation function.

![(\#fig:logistic-regression-multiclass) Multinomial logistic regression](figures/logistic_regression_multiclass)


### Example in R


```{r metrics,cache=TRUE,echo=FALSE}
get_metrics <- function(Y_test, Y_pred)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}


plot_metrics_digits <- function(res, acc, main) {
  matplot(res$i, res[,3:5], type="b",
          xlab="digit", ylab="metric", lty=c(4,2,1), col=c(4,2,1),
          pch=c(4,2,1), ylim=c(0.8, 1.0),
          sub=sprintf("Accuracy=%.3f", acc))
  legend("bottom", legend=names(res)[3:5], lty=c(4,2,1), col=c(4,2,1), pch=c(4,2,1), ncol=3, bg="white")
  abline(h=colMeans(res[3:5]), lty=c(4,2,1), col=c("#0000ff33", "#00ff0033", "#00000033"))
  abline(h=acc, col="#00000066")
}
```




To train such a neural network (i.e., fit a multinomial
logistic regression model),
we will use the `keras` package,
a wrapper around the (GPU-enabled) TensorFlow library.

The training of the model takes a few minutes (for more complex
models and bigger datasets -- it could take hours/days).
Thus, it is wise to store the computed model (the $\mathbf{B}$
coefficient matrix and the accompanying `keras`'s auxiliary data)
for further reference:


```{r logistic1,cache=TRUE,dependson='mnist_onehot_final',warning=FALSE}
file_name <- "datasets/mnist_keras_model1.h5"
if (!file.exists(file_name)) { # File doesn't exist -> compute
    set.seed(123)
    # Start with an empty model
    model1 <- keras_model_sequential()
    # Add a single layer with 10 units and softmax activation
    layer_dense(model1, units=10, activation="softmax")
    # We will be minimising the cross-entropy,
    # sgd == stochastic gradient descent, see the next chapter
    compile(model1, optimizer="sgd",
            loss="categorical_crossentropy")
    # Fit the model (slooooow!)
    fit(model1, X_train2, Y_train2, epochs=10)
    # Save the model for future reference
    save_model_hdf5(model1, file_name)
} else { # File exists -> reload the model
    model1 <- load_model_hdf5(file_name)
}
```




Let's make predictions over the test set:

```{r logistic2,cache=TRUE,dependson='logistic1'}
Y_pred2 <- predict(model1, X_test2)
round(head(Y_pred2), 2) # predicted class probabilities
```




Then, we can one-hot-decode the output probabilities:

```{r logistic2a,cache=TRUE,dependson='logistic2'}
Y_pred <- apply(Y_pred2, 1, which.max)-1 # 1..10 -> 0..9
head(Y_pred, 20) # predicted outputs
head(Y_test, 20) # true outputs
```


Accuracy on the test set:

```{r logistic3,cache=TRUE,dependson='logistic2a'}
mean(Y_test == Y_pred)
```




Performance metrics for each digit separately (see also Figure \@ref(fig:logistic6)):

```{r logistic4,cache=TRUE,dependson='logistic3',echo=FALSE}
res_logistic <- data.frame(t(sapply(0:9, function(i) {
    c(i=i, get_metrics(Y_test==i, Y_pred==i))
})))
knitr::kable(res_logistic)
```




Note how misleading the individual accuracies are! Averaging over
the above table's columns gives:

```{r logistic5,cache=TRUE,dependson='logistic4',echo=FALSE}
colMeans(res_logistic[2:5])
```




```{r logistic6,cache=TRUE,dependson="logistic5",echo=FALSE,fig.cap="Performance metrics for multinomial logistic regression on MNIST"}
plot_metrics_digits(res_logistic, mean(Y_test == Y_pred), "Multinomial Logistic Regression")
```







## Deep Neural Networks

### Introduction





In a brain, a neuron's output is an input a bunch of other neurons.
We could try aligning neurons into many interconnected layers.
This leads to a structure like the one in Figure \@ref(fig:nnet).

![(\#fig:nnet) A multi-layer neural network](figures/nnet)



### Activation Functions





Each layer's outputs should be transformed by some non-linear
activation function. Otherwise, we'd end up with linear combinations of linear combinations,
which are linear combinations themselves.

<!-- Apart from `softmax` -->

<!--linear
Linear (i.e. identity) activation function.
Not for hidden layers - use for the output layer in regression tasks-->

Example activation functions
that can be used in hidden (inner) layers:

* `relu` -- The rectified linear unit:
\[\psi(t)=\max(t, 0),\]
* `sigmoid` -- The logistic sigmoid:
\[\phi(t)= \frac{1}{1 + \exp(-t)},\]
* `tanh` -- The hyperbolic function:
\[\mathrm{tanh}(t) = \frac{\exp(t) - \exp(-t)}{\exp(t) + \exp(-t)}.\]

There is not much difference between them, but some might be more convenient
to handle numerically than the others, depending on the implementation.









### Example in R - 2 Layers




Let's construct a 2-layer Neural Network of the type 784-800-10:

```{r deep21,cache=TRUE,dependson='mnist_onehot_final',warning=FALSE}
file_name <- "datasets/mnist_keras_model2.h5"
if (!file.exists(file_name)) {
    set.seed(123)
    model2 <- keras_model_sequential()
    layer_dense(model2, units=800, activation="relu")
    layer_dense(model2, units=10,  activation="softmax")
    compile(model2, optimizer="sgd",
            loss="categorical_crossentropy")
    fit(model2, X_train2, Y_train2, epochs=10)
    save_model_hdf5(model2, file_name)
} else {
    model2 <- load_model_hdf5(file_name)
}

Y_pred2 <- predict(model2, X_test2)
Y_pred <- apply(Y_pred2, 1, which.max)-1 # 1..10 -> 0..9
mean(Y_test == Y_pred) # accuracy on the test set
```




Performance metrics for each digit separately,
see also Figure \@ref(fig:deep23):

```{r deep22,cache=TRUE,dependson='deep21',echo=FALSE}
res_2 <- data.frame(t(sapply(0:9, function(i) {
  c(i=i, get_metrics(Y_test==i, Y_pred==i))
})))
knitr::kable(res_2)
```





```{r deep23,cache=TRUE,dependson='deep22',echo=FALSE,fig.cap="Performance metrics for a 2-layer net 784-800-10 [relu] on MNIST"}
plot_metrics_digits(res_2, mean(Y_test == Y_pred), "2-layer net 784-800-10 [relu]")
```


### Example in R - 6 Layers



How about a 6-layer *Deep* Neural Network
like 784-2500-2000-1500-1000-500-10? Here you are:


```{r deep61,cache=TRUE,dependson='mnist_onehot_final',warning=FALSE}
file_name <- "datasets/mnist_keras_model3.h5"
if (!file.exists(file_name)) {
    set.seed(123)
    model3 <- keras_model_sequential()
    layer_dense(model3, units=2500, activation="relu")
    layer_dense(model3, units=2000, activation="relu")
    layer_dense(model3, units=1500, activation="relu")
    layer_dense(model3, units=1000, activation="relu")
    layer_dense(model3, units=500,  activation="relu")
    layer_dense(model3, units=10,   activation="softmax")
    compile(model3, optimizer="sgd",
            loss="categorical_crossentropy")
    fit(model3, X_train2, Y_train2, epochs=10)
    save_model_hdf5(model3, file_name)
} else {
    model3 <- load_model_hdf5(file_name)
}

Y_pred2 <- predict(model3, X_test2)
Y_pred <- apply(Y_pred2, 1, which.max)-1 # 1..10 -> 0..9
mean(Y_test == Y_pred) # accuracy on the test set
```




Performance metrics for each digit separately,
see also Figure \@ref(fig:deep63).

```{r deep62,cache=TRUE,dependson='deep61',echo=FALSE}
res_6 <- data.frame(t(sapply(0:9, function(i) {
  c(i=i, get_metrics(Y_test==i, Y_pred==i))
})))
knitr::kable(res_6)
```




```{r deep63,cache=TRUE,dependson='deep62',echo=FALSE,fig.cap="Performance metrics for a 6-layer net 784-2500-2000-1500-1000-500-10 [relu] on MNIST"}
plot_metrics_digits(res_6, mean(Y_test == Y_pred), "6-layer net 784-2500-2000-1500-1000-500-10 [relu]")
```


{ BEGIN exercise }
Test the performance of different neural network
architectures (different number of layers, different number
of neurons in each layer etc.). Yes, it's more art than science!
Many tried to come up with various "rules of thumb",
see, for example, the `comp.ai.neural-nets` FAQ [@aifaq] at
http://www.faqs.org/faqs/ai-faq/neural-nets/part3/preamble.html,
but what works well in one problem might not be generalisable
to another one.
{ END exercise }


## Preprocessing of Data

### Introduction




Do not underestimate the power of appropriate data preprocessing ---
deep neural networks are not a universal replacement for a data engineer's hard work!

On top of that, they are not interpretable -- these are merely black-boxes.

Among the typical transformations of the input images we can find:

- normalisation of colours (setting brightness, stretching contrast, etc.),
- repositioning of the image (centring),
- deskewing (see below),
- denoising (e.g., by blurring).

Another frequently applied technique concerns an expansion of the training data
--- we can add "artificially contaminated" images to the training
set (e.g., slightly rotated digits) so as to be more ready to whatever
will be provided in the test test.



### Image Deskewing




Deskewing of images ("straightening" of the digits)
is amongst the most typical transformations
that can be applied on MNIST.

Unfortunately, we don't have (yet) the necessary
mathematical background to discuss this operation
in very detail.

Luckily, we can apply it on each image anyway.

See the GitHub repository at https://github.com/gagolews/Playground.R
for an example notebook and the `deskew.R` script.

```{r deskew1,cache=TRUE,dependson='mnist_onehot_final'}
# See https://github.com/gagolews/Playground.R
source("~/R/Playground.R/deskew.R")
# new_image <- deskew(old_image)
```







```{r deskew2,cache=TRUE,dependson='deskew1',echo=FALSE,fig.height=3,fig.cap="Deskewing of the MNIST digits"}
set.seed(123)
par(mar=c(0,0,0,0))
par(mfrow=c(10,20))
for (i in 0:9) {
    ids <- sample(which(Y_train == i))
    for (j in 1:10) {
        id <- ids[j]
        I <- X_train[id,,]
        image(1:ncol(I), 1:nrow(I), z=t(I), col=grey.colors(256, 0, 1), axes=FALSE, asp=1, ylim=c(nrow(I), 1))
        I2 <- deskew(I)
        image(1:ncol(I), 1:nrow(I), z=t(I2), col=grey.colors(256, 1, 0), axes=FALSE, asp=1, ylim=c(nrow(I), 1))
    }
}
```

Let's take a look at Figure \@ref(fig:deskew2).
In each pair, the left image (black background) is the original one,
and the right image (palette inverted for purely dramatic effects)
is its deskewed version.




Below we deskew each image in the training as well as in the test sample.
This also takes a long time, so let's store the resulting objects
for further reference:

```{r deskew3,cache=TRUE,dependson='deskew2',warning=FALSE}
file_name <- "datasets/mnist_deskewed_train.rds"
if (!file.exists(file_name)) {
    Z_train <- X_train
    for (i in 1:dim(Z_train)[1]) {
        Z_train[i,,] <- deskew(Z_train[i,,])
    }
    Z_train2 <- matrix(Z_train, ncol=28*28)
    saveRDS(Z_train2, file_name)
} else {
    Z_train2 <- readRDS(file_name)
}

file_name <- "datasets/mnist_deskewed_test.rds"
if (!file.exists(file_name)) {
    Z_test <- X_test
    for (i in 1:dim(Z_test)[1]) {
        Z_test[i,,] <- deskew(Z_test[i,,])
    }
    Z_test2 <- matrix(Z_test, ncol=28*28)
    saveRDS(Z_test2, file_name)
} else {
    Z_test2 <- readRDS(file_name)
}
```

Remark.

: RDS in a compressed file format used by R for object serialisation
(quickly storing its verbatim copies so that they can be reloaded
at any time).



Multinomial logistic regression model (1-layer NN):


```{r deskew4,cache=TRUE,dependson='deskew3',warning=FALSE}
file_name <- "datasets/mnist_keras_model1d.h5"
if (!file.exists(file_name)) {
    set.seed(123)
    model1d <- keras_model_sequential()
    layer_dense(model1d, units=10, activation="softmax")
    compile(model1d, optimizer="sgd",
            loss="categorical_crossentropy")
    fit(model1d, Z_train2, Y_train2, epochs=10)
    save_model_hdf5(model1d, file_name)
} else {
    model1d <- load_model_hdf5(file_name)
}

Y_pred2 <- predict(model1d, Z_test2)
Y_pred <- apply(Y_pred2, 1, which.max)-1 # 1..10 -> 0..9
mean(Y_test == Y_pred) # accuracy on the test set
```




Performance metrics for each digit separately,
see also Figure \@ref(fig:deskew6).

```{r deskew5,cache=TRUE,dependson='deskew4',echo=FALSE}
res_logistic_deskewed <- data.frame(t(sapply(0:9, function(i) {
    c(i=i, get_metrics(Y_test==i, Y_pred==i))
})))
knitr::kable(res_logistic_deskewed)
```





```{r deskew6,cache=TRUE,dependson='deskew5',echo=FALSE,fig.cap="Performance of Multinomial Logistic Regression on the deskewed MNIST"}
plot_metrics_digits(res_logistic_deskewed, mean(Y_test == Y_pred),
"Multinomial Logistic Regression [deskewed]")
```







```{r deep_deskew,cache=TRUE,dependson='mnist_onehot_final',echo=FALSE,warning=FALSE}
file_name <- "datasets/mnist_keras_model2d.h5"
if (!file.exists(file_name)) {
    set.seed(123)
    model2d <- keras_model_sequential()
    layer_dense(model2d, units=800, activation="relu")
    layer_dense(model2d, units=10,  activation="softmax")
    compile(model2d, optimizer="sgd",
            loss="categorical_crossentropy")
    fit(model2d, Z_train2, Y_train2, epochs=10)
    save_model_hdf5(model2d, file_name)
} else {
    model2d <- load_model_hdf5(file_name)
}

Y_pred2 <- predict(model2d, Z_test2)
Y_pred <- apply(Y_pred2, 1, which.max)-1 # 1..10 -> 0..9
res_2_deskewed <- data.frame(t(sapply(0:9, function(i) {
    c(i=i, get_metrics(Y_test==i, Y_pred==i))
})))


file_name <- "datasets/mnist_keras_model3d.h5"
if (!file.exists(file_name)) {
    set.seed(123)
    model3d <- keras_model_sequential()
    layer_dense(model3d, units=2500, activation="relu")
    layer_dense(model3d, units=2000, activation="relu")
    layer_dense(model3d, units=1500, activation="relu")
    layer_dense(model3d, units=1000, activation="relu")
    layer_dense(model3d, units=500,  activation="relu")
    layer_dense(model3d, units=10,   activation="softmax")
    compile(model3d, optimizer="sgd",
            loss="categorical_crossentropy")
    fit(model3d, Z_train2, Y_train2, epochs=10)
    save_model_hdf5(model3d, file_name)
} else {
    model3d <- load_model_hdf5(file_name)
}
Y_pred2 <- predict(model3d, Z_test2)
Y_pred <- apply(Y_pred2, 1, which.max)-1 # 1..10 -> 0..9
res_6_deskewed <- data.frame(t(sapply(0:9, function(i) {
    c(i=i, get_metrics(Y_test==i, Y_pred==i))
})))
```


### Summary of All the Models Considered




Let's summarise the quality of all the considered classifiers.
Figure \@ref(fig:globalsum) gives the F-measures, for each digit
separately.

```{r globalsum,cache=TRUE,dependson=c('deep_deskew','deskew6','deep63','deep23','logistic6'),echo=FALSE,fig.cap="Summary of F-measures for each classified digit and every method"}
res_F_combined <- as.data.frame(cbind(
    "Logistic"=res_logistic$F,
    "2-Layer"=res_2$F,
    "6-Layer"=res_6$F,
    "Logistic [deskewed]"=res_logistic_deskewed$F,
    "2-Layer [deskewed]"=res_2_deskewed$F,
    "6-Layer [deskewed]"=res_6_deskewed$F
))
matplot(0:9, res_F_combined, type="b",
        xlab="digit",
        ylab="metric",
        lty=c(1,1,1,2,2,2),
        col=c(1,2,4,1,2,4),
        pch=c(1,2,4,1,2,4),
        ylim=c(0.8, 1.0))
legend("bottom", legend=names(res_F_combined),
        lty=c(1,1,1,2,2,2),
        col=c(1,2,4,1,2,4),
        pch=c(1,2,4,1,2,4), ncol=2, bg="white")
#abline(h=colMeans(res[3:5]), lty=c(4,2,1), col=c("#0000ff33", "#00ff0033", "#00000033"))
#abline(h=acc, col="#00000066")
```

Note that the applied preprocessing of data increased
the prediction accuracy.


The same information can also be included on a heat map
which is depicted in Figure \@ref(fig:globalsum2)
(see the `image()` function in R).

```{r globalsum2,cache=TRUE,dependson='globalsum',echo=FALSE,fig.cap="A heat map of F-measures for each classified digit and each method"}
par(mar=c(3,10,2,1))
par(ann=FALSE)
image(0:9, 1:length(res_F_combined), z=as.matrix(res_F_combined), axes=FALSE, ann=FALSE)
axis(1, labels=0:9, at=0:9)
axis(2, labels=names((res_F_combined)), at=1:length(res_F_combined), las=1)
box()
for (i in 0:9)
    for (j in 1:length(res_F_combined))
        text(i, j, sprintf("%.2f", res_F_combined[i+1,j]))
```





## Outro

### Remarks





We have discussed a multinomial logistic regression model
as a generalisation of the binary one.

This in turn is a special case of feed-forward neural networks.

There's a lot of hype (again...) for deep neural networks in many applications,
including vision, self-driving cars, natural language processing,
speech recognition etc.




Many different architectures of neural networks and types of units
are being considered in theory and in practice, e.g.:

- convolutional neural networks apply a series of signal (e.g., image)
transformations in first layers, they might actually "discover"
deskewing automatically etc.;
- recurrent neural networks can  imitate  long/short-term memory
that  can be used for speech synthesis and time series prediction.




Main drawbacks of deep neural networks:

- learning is very slow, especially with very deep architectures (days, weeks);
- models are not explainable (black boxes) and hard to debug;
- finding good architectures is more art than science
(maybe: more of a craftsmanship even);
- sometimes using deep neural network is just an excuse for being too lazy
to do proper data cleansing and pre-processing.

There are many issues and challenges that are tackled in more advanced
AI/ML courses and books, such as [@deeplearn].





### Beyond MNIST




The MNIST dataset is a classic, although its use in deep learning
research is nowadays discouraged
-- the dataset is not considered challenging anymore -- state of the art classifiers
can reach $99.8\%$ accuracy.

See Zalando's Fashion-MNIST (by Kashif Rasul & Han Xiao) at
https://github.com/zalandoresearch/fashion-mnist for a modern replacement.

Alternatively, take a look at CIFAR-10 and CIFAR-100 (https://www.cs.toronto.edu/~kriz/cifar.html)
by A. Krizhevsky et al.
or at ImageNet (http://image-net.org/index) for an even greater challenge.



<!--

Common tricks with NNets: ...

This is kind of "disgusting" --
let engineer do whatever it takes to increase some performance metric;
You only optimise wrt a single criterion, are you controlling other ones?


Some people tend to attack every problem with NNets,
but actually their scope is very limited (computer vision and
signal processing, ...)


-->




<!-- https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/cheatsheet-deep-learning-tips-tricks.pdf -->



### Further Reading


```{R load_tensorflow_nn,echo=FALSE}
library("tensorflow")
```


Recommended further reading: [@islr: Chapter 11], [@aifaq] and [@deeplearn]

See also the `keras` package tutorials available at:
https://cran.r-project.org/web/packages/keras/index.html
and https://keras.rstudio.com.
