<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Optimisation with Genetic Algorithms (*) | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="pandoc, pandoc-citeproc, knitr, bookdown (custom), GitBook, and many more" />

  <meta property="og:title" content="8 Optimisation with Genetic Algorithms (*) | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clustering.html"/>
<link rel="next" href="recommender-systems.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<!-- Use KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<!-- /Use KaTeX -->

<style type="text/css">
/* Marek's custom CSS */

@import url("https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic,700italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans:400,500,600,700,800,900,400italic,500italic,600italic,700italic,800italic,900italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:400,600,700,400italic,600italic,700italic");

span.katex {
    font-size: 100%;
}

</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2.1 2021-05-10 10:04 (0e09aab)</a></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#exercises-in-r-2"><i class="fa fa-check"></i><b>4.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-preparing-data"><i class="fa fa-check"></i><b>4.4.1</b> EdStats – Preparing Data</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.4.2</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.4.3</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>4.4.4</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>4.4.5</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.5</b> Outro</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks (*)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="5.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>5.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Continuous Optimisation with Iterative Algorithms (*)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="6.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>6.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="6.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.5</b> Outro</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>7.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3</b> Agglomerative Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>7.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#exercises-in-r-3"><i class="fa fa-check"></i><b>7.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>7.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>7.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>7.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.5</b> Outro</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms (*)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.2</b> Genetic Algorithms</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.2.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.3</b> Outro</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.3.1</b> Remarks</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems (*)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-16"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> Exercise: The MovieLens Dataset (*)</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a>
<ul>
<li class="chapter" data-level="B.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a>
<ul>
<li class="chapter" data-level="C.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a>
<ul>
<li class="chapter" data-level="C.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a>
<ul>
<li class="chapter" data-level="C.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>C.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a>
<ul>
<li class="chapter" data-level="D.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>D.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a>
<ul>
<li class="chapter" data-level="E.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a>
<ul>
<li class="chapter" data-level="E.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>E.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimisation-with-genetic-algorithms" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Optimisation with Genetic Algorithms (*)</h1>
<blockquote>
<p>This is a slightly older (distributed in the hope that it will be useful)
version of the forthcoming textbook (ETA 2022) preliminarily entitled
<em>Machine Learning in R from Scratch</em> by
<a href="https://www.gagolewski.com">Marek Gagolewski</a>, which is now undergoing
a major revision (when I am not busy with other projects). There will be
not much work on-going in this repository anymore, as its sources have
moved elsewhere; however, if you happen to find any bugs or typos,
please drop me an
<a href="https://github.com/gagolews/lmlcr/blob/master/CODE_OF_CONDUCT.md">email</a>.
I will share a new draft once it’s ripe. Stay tuned.</p>
</blockquote>
<!--

TODO : actually do some combinatorial optimisation

variable selection for lm/glm????

tabu search???

-->
<div id="introduction-14" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Introduction</h2>
<div id="recap" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Recap</h3>
<div style="margin-top: 1em">

</div>
<p>Recall that an <strong>optimisation task</strong> deals with finding an element <span class="math inline">\(\mathbf{x}\)</span>
in a <strong>search space</strong> <span class="math inline">\(\mathbb{D}\)</span>,
that minimises or maximises an <strong>objective function</strong> <span class="math inline">\(f:\mathbb{D}\to\mathbb{R}\)</span>:
<span class="math display">\[
\min_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}) \quad\text{or}\quad\max_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}),
\]</span></p>
<p>In one of the previous chapters, we were dealing with
<strong>unconstrained continuous optimisation</strong>,
i.e., we assumed the search space is <span class="math inline">\(\mathbb{D}=\mathbb{R}^p\)</span> for some <span class="math inline">\(p\)</span>.</p>
<p> <br />
</p>
<p>Example problems of this kind: minimising mean squared error
in linear regression
or minimising cross-entropy in logistic regression.</p>
<div style="margin-top: 1em">

</div>
<p>The class of general-purpose iterative algorithms we’ve previously studied
fit into the following scheme:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{x}^{(0)}\)</span> – initial guess (e.g., generated at random)</p></li>
<li><p>for <span class="math inline">\(i=1,...,M\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}+\text{[guessed direction, e.g.,}-\eta\nabla f(\mathbf{x})\text{]}\)</span></li>
<li>if <span class="math inline">\(|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| &lt; \varepsilon\)</span> break</li>
</ol></li>
<li><p>return <span class="math inline">\(\mathbf{x}^{(i)}\)</span> as result</p></li>
</ol>
<p>where:</p>
<ul>
<li><span class="math inline">\(M\)</span> = maximum number of iterations</li>
<li><span class="math inline">\(\varepsilon\)</span> = tolerance, e.g, <span class="math inline">\(10^{-8}\)</span></li>
<li><span class="math inline">\(\eta&gt;0\)</span> = learning rate</li>
</ul>
<div style="margin-top: 1em">

</div>
<p>The algorithms such as gradient descent and BFGS (see <code>optim()</code>)
give satisfactory results in the case of <strong>smooth and well-behaving objective functions</strong>.</p>
<p>However, if an objective has, e.g., many plateaus (regions where it is almost constant),
those methods might easily get stuck in local minima.</p>
<p>The K-means clustering’s objective function is a not particularly pleasant
one – it involves a nested search for the closest cluster, with the use of the <span class="math inline">\(\min\)</span> operator.</p>
</div>
<div id="k-means-revisited" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> K-means Revisited</h3>
<div style="margin-top: 1em">

</div>
<p>In <strong>K-means clustering</strong> we are minimising the squared Euclidean distance
to each point’s cluster centre:
<span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]</span></p>
<p>This is an (NP-)hard problem!
There is no efficient exact algorithm.</p>
<p>We need approximations. In the last chapter, we have
discussed the iterative Lloyd’s algorithm (1957),
which is amongst a few procedures implemented in the <code>kmeans()</code> function.</p>
<div style="margin-top: 1em">

</div>
<p>To recall, Lloyd’s algorithm (1957) is sometimes referred to as “the” K-means algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Start with random cluster centres <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p></li>
<li><p>For each point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>, determine its closest centre <span class="math inline">\(C(i)\in\{1,\dots,K\}\)</span>.</p></li>
<li><p>For each cluster <span class="math inline">\(k\in\{1,\dots,K\}\)</span>, compute the new cluster centre <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> as the componentwise arithmetic mean
of the coordinates of all the point indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(C(i)=k\)</span>.</p></li>
<li><p>If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.</p></li>
</ol>
<p> <br />
</p>
<p>As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).</p>
<p>Hence, we are used to calling:</p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="optimisation-with-genetic-algorithms.html#cb729-1" aria-hidden="true"></a><span class="kw">kmeans</span>(X, <span class="dt">centers=</span>k, <span class="dt">nstart=</span><span class="dv">10</span>)</span></code></pre></div>
</div>
<div id="optim-vs.-kmeans" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> optim() vs. kmeans()</h3>
<div style="margin-top: 1em">

</div>
<p>Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in <code>optim()</code> compares with a customised, problem-specific solver.</p>
<p>We will need some benchmark data.</p>
<div class="sourceCode" id="cb730"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb730-1"><a href="optimisation-with-genetic-algorithms.html#cb730-1" aria-hidden="true"></a>gen_cluster &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, m, s) {</span>
<span id="cb730-2"><a href="optimisation-with-genetic-algorithms.html#cb730-2" aria-hidden="true"></a>    vectors &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</span>
<span id="cb730-3"><a href="optimisation-with-genetic-algorithms.html#cb730-3" aria-hidden="true"></a>    unit_vectors &lt;-<span class="st"> </span>vectors<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>(vectors<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb730-4"><a href="optimisation-with-genetic-algorithms.html#cb730-4" aria-hidden="true"></a>    unit_vectors<span class="op">*</span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, s)<span class="op">+</span><span class="kw">rep</span>(m, <span class="dt">each=</span>n)</span>
<span id="cb730-5"><a href="optimisation-with-genetic-algorithms.html#cb730-5" aria-hidden="true"></a>}</span></code></pre></div>
<p>The above function generates <span class="math inline">\(n\)</span> points in <span class="math inline">\(\mathbb{R}^p\)</span>
from a distribution centred at <span class="math inline">\(\mathbf{m}\in\mathbb{R}^p\)</span>,
spread randomly in every possible direction with scale factor <span class="math inline">\(s\)</span>.</p>
<div style="margin-top: 1em">

</div>
<p>Two example clusters in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb731-1"><a href="optimisation-with-genetic-algorithms.html#cb731-1" aria-hidden="true"></a><span class="co"># plot the &quot;black&quot; cluster</span></span>
<span id="cb731-2"><a href="optimisation-with-genetic-algorithms.html#cb731-2" aria-hidden="true"></a><span class="kw">plot</span>(<span class="kw">gen_cluster</span>(<span class="dv">500</span>, <span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">1</span>), <span class="dt">col=</span><span class="st">&quot;#00000022&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</span>
<span id="cb731-3"><a href="optimisation-with-genetic-algorithms.html#cb731-3" aria-hidden="true"></a>    <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>)</span>
<span id="cb731-4"><a href="optimisation-with-genetic-algorithms.html#cb731-4" aria-hidden="true"></a><span class="co"># plot the &quot;red&quot; cluster</span></span>
<span id="cb731-5"><a href="optimisation-with-genetic-algorithms.html#cb731-5" aria-hidden="true"></a><span class="kw">points</span>(<span class="kw">gen_cluster</span>(<span class="dv">250</span>, <span class="dv">2</span>, <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="dv">1</span>), <span class="fl">0.5</span>), <span class="dt">col=</span><span class="st">&quot;#ff000022&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<div class="figure">
<img src="08-optimisation-genetic-figures/gendata_example-1.png" alt="" />
<p class="caption">(#fig:gendata_example) plot of chunk gendata_example</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Let’s generate the benchmark dataset <span class="math inline">\(\mathbf{X}\)</span>
that consists of three clusters in a high-dimensional space.</p>
<div class="sourceCode" id="cb732"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb732-1"><a href="optimisation-with-genetic-algorithms.html#cb732-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb732-2"><a href="optimisation-with-genetic-algorithms.html#cb732-2" aria-hidden="true"></a>p  &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb732-3"><a href="optimisation-with-genetic-algorithms.html#cb732-3" aria-hidden="true"></a>Ns &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">20</span>)</span>
<span id="cb732-4"><a href="optimisation-with-genetic-algorithms.html#cb732-4" aria-hidden="true"></a>Ms &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb732-5"><a href="optimisation-with-genetic-algorithms.html#cb732-5" aria-hidden="true"></a>s  &lt;-<span class="st"> </span><span class="fl">1.5</span><span class="op">*</span>p</span>
<span id="cb732-6"><a href="optimisation-with-genetic-algorithms.html#cb732-6" aria-hidden="true"></a>K  &lt;-<span class="st"> </span><span class="kw">length</span>(Ns)</span>
<span id="cb732-7"><a href="optimisation-with-genetic-algorithms.html#cb732-7" aria-hidden="true"></a></span>
<span id="cb732-8"><a href="optimisation-with-genetic-algorithms.html#cb732-8" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="cf">function</span>(k)</span>
<span id="cb732-9"><a href="optimisation-with-genetic-algorithms.html#cb732-9" aria-hidden="true"></a>    <span class="kw">gen_cluster</span>(Ns[k], p, <span class="kw">rep</span>(Ms[k], p), s))</span>
<span id="cb732-10"><a href="optimisation-with-genetic-algorithms.html#cb732-10" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, X) <span class="co"># rbind(X[[1]], X[[2]], X[[3]])</span></span></code></pre></div>
<!--
X <- as.matrix(read.csv("datasets/sipu_unbalance.csv",
    header=FALSE, sep=" ", comment.char="#"))
X <- X/10000-30 # a more user-friendly scale
K <- 8
p <- 2

library("FNN")
get_loss <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])^2)
}

Storn R, Price K (1997). “Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces.”Journal of Global Optimization,11(4), 341–359.

Mullen K, Ardia D, Gil D, Windover D, Cline J (2011). “DEoptim:  An R Package for GlobalOptimization by Differential Evolution.”Journal of Statistical Software,40(6), 1–26. URLhttp://www.jstatsoft.org/v40/i06/

library("DEoptim")
obj <- function(mu) {
            get_loss(matrix(mu, nrow=K), X)
        }
res <- DEoptim(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$optim$bestmem, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_loss(mu_res, X)
km <- kmeans(X, mu_res)
get_loss(km$centers, X)




res <- optim(X[sample(nrow(X), K),],
    fn=obj,
        #lower=rep(apply(X, 2, min), each=K),
        #upper=rep(apply(X, 2, max), each=K),
        method="SANN",
        control = list(maxit = 20000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_loss(mu_res, X)
km <- kmeans(X, mu_res)
get_loss(km$centers, X)

Zambrano-Bigiarini, M., Rojas, R. (2013). “A model-independent Particle Swarm Optimisation software for model calibration.” Environmental Modelling & Software, 43, 5-25. doi: 10.1016/j.envsoft.2013.01.004, http://dx.doi.org/10.1016/j.envsoft.2013.01.004.

library("hydroPSO")
res <- hydroPSO(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_loss(mu_res, X)
km <- kmeans(X, mu_res)
get_loss(km$centers, X)




cntr <- matrix(ncol=2, byrow=TRUE, c( # initial guess
   -15,   5,
   -12,   10,
   -10,   5,
    15,   0,
    15,   10,
    20,   5,
    25,   0,
    25,   10))
km <- kmeans(X, cntr)
get_loss(km$centers, X)

km <- kmeans(X, K, nstart=10)
get_loss(km$centers, X)

-->
<div style="margin-top: 1em">

</div>
<p>The objective function for the K-means clustering problem:</p>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb733-1"><a href="optimisation-with-genetic-algorithms.html#cb733-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</span>
<span id="cb733-2"><a href="optimisation-with-genetic-algorithms.html#cb733-2" aria-hidden="true"></a>get_loss &lt;-<span class="st"> </span><span class="cf">function</span>(mu, X) {</span>
<span id="cb733-3"><a href="optimisation-with-genetic-algorithms.html#cb733-3" aria-hidden="true"></a>    <span class="co"># For each point in X,</span></span>
<span id="cb733-4"><a href="optimisation-with-genetic-algorithms.html#cb733-4" aria-hidden="true"></a>    <span class="co"># get the index of the closest point in mu:</span></span>
<span id="cb733-5"><a href="optimisation-with-genetic-algorithms.html#cb733-5" aria-hidden="true"></a>    memb &lt;-<span class="st"> </span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(mu, X, <span class="dv">1</span>)<span class="op">$</span>nn.index</span>
<span id="cb733-6"><a href="optimisation-with-genetic-algorithms.html#cb733-6" aria-hidden="true"></a></span>
<span id="cb733-7"><a href="optimisation-with-genetic-algorithms.html#cb733-7" aria-hidden="true"></a>    <span class="co"># compute the sum of squared distances</span></span>
<span id="cb733-8"><a href="optimisation-with-genetic-algorithms.html#cb733-8" aria-hidden="true"></a>    <span class="co"># between each point and its closes cluster centre:</span></span>
<span id="cb733-9"><a href="optimisation-with-genetic-algorithms.html#cb733-9" aria-hidden="true"></a>    <span class="kw">sum</span>((X<span class="op">-</span>mu[memb,])<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb733-10"><a href="optimisation-with-genetic-algorithms.html#cb733-10" aria-hidden="true"></a>}</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>Setting up the solvers:</p>
<div class="sourceCode" id="cb734"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb734-1"><a href="optimisation-with-genetic-algorithms.html#cb734-1" aria-hidden="true"></a>min_HartiganWong &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</span>
<span id="cb734-2"><a href="optimisation-with-genetic-algorithms.html#cb734-2" aria-hidden="true"></a>    <span class="kw">get_loss</span>(</span>
<span id="cb734-3"><a href="optimisation-with-genetic-algorithms.html#cb734-3" aria-hidden="true"></a>        <span class="co"># algorithm=&quot;Hartigan-Wong&quot;</span></span>
<span id="cb734-4"><a href="optimisation-with-genetic-algorithms.html#cb734-4" aria-hidden="true"></a>        <span class="kw">kmeans</span>(X, mu0, <span class="dt">iter.max=</span><span class="dv">100</span>)<span class="op">$</span>centers,</span>
<span id="cb734-5"><a href="optimisation-with-genetic-algorithms.html#cb734-5" aria-hidden="true"></a>    X)</span>
<span id="cb734-6"><a href="optimisation-with-genetic-algorithms.html#cb734-6" aria-hidden="true"></a>min_Lloyd &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</span>
<span id="cb734-7"><a href="optimisation-with-genetic-algorithms.html#cb734-7" aria-hidden="true"></a>    <span class="kw">get_loss</span>(</span>
<span id="cb734-8"><a href="optimisation-with-genetic-algorithms.html#cb734-8" aria-hidden="true"></a>        <span class="kw">kmeans</span>(X, mu0, <span class="dt">iter.max=</span><span class="dv">100</span>, <span class="dt">algorithm=</span><span class="st">&quot;Lloyd&quot;</span>)<span class="op">$</span>centers,</span>
<span id="cb734-9"><a href="optimisation-with-genetic-algorithms.html#cb734-9" aria-hidden="true"></a>    X)</span>
<span id="cb734-10"><a href="optimisation-with-genetic-algorithms.html#cb734-10" aria-hidden="true"></a>min_optim &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</span>
<span id="cb734-11"><a href="optimisation-with-genetic-algorithms.html#cb734-11" aria-hidden="true"></a>    <span class="kw">optim</span>(mu0,</span>
<span id="cb734-12"><a href="optimisation-with-genetic-algorithms.html#cb734-12" aria-hidden="true"></a>        <span class="cf">function</span>(mu, X) {</span>
<span id="cb734-13"><a href="optimisation-with-genetic-algorithms.html#cb734-13" aria-hidden="true"></a>            <span class="kw">get_loss</span>(<span class="kw">matrix</span>(mu, <span class="dt">nrow=</span><span class="kw">nrow</span>(mu0)), X)</span>
<span id="cb734-14"><a href="optimisation-with-genetic-algorithms.html#cb734-14" aria-hidden="true"></a>        }, <span class="dt">X=</span>X, <span class="dt">method=</span><span class="st">&quot;BFGS&quot;</span>, <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">reltol=</span><span class="fl">1e-16</span>)</span>
<span id="cb734-15"><a href="optimisation-with-genetic-algorithms.html#cb734-15" aria-hidden="true"></a>    )<span class="op">$</span>val</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>Running the simulation:</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="optimisation-with-genetic-algorithms.html#cb735-1" aria-hidden="true"></a>nstart &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb735-2"><a href="optimisation-with-genetic-algorithms.html#cb735-2" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb735-3"><a href="optimisation-with-genetic-algorithms.html#cb735-3" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">replicate</span>(nstart, {</span>
<span id="cb735-4"><a href="optimisation-with-genetic-algorithms.html#cb735-4" aria-hidden="true"></a>  mu0 &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),]</span>
<span id="cb735-5"><a href="optimisation-with-genetic-algorithms.html#cb735-5" aria-hidden="true"></a>    <span class="kw">c</span>(</span>
<span id="cb735-6"><a href="optimisation-with-genetic-algorithms.html#cb735-6" aria-hidden="true"></a>        <span class="dt">HartiganWong=</span><span class="kw">min_HartiganWong</span>(mu0, X),</span>
<span id="cb735-7"><a href="optimisation-with-genetic-algorithms.html#cb735-7" aria-hidden="true"></a>        <span class="dt">Lloyd=</span><span class="kw">min_Lloyd</span>(mu0, X),</span>
<span id="cb735-8"><a href="optimisation-with-genetic-algorithms.html#cb735-8" aria-hidden="true"></a>        <span class="dt">optim=</span><span class="kw">min_optim</span>(mu0, X)</span>
<span id="cb735-9"><a href="optimisation-with-genetic-algorithms.html#cb735-9" aria-hidden="true"></a>    )</span>
<span id="cb735-10"><a href="optimisation-with-genetic-algorithms.html#cb735-10" aria-hidden="true"></a>})</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>Notice a considerable variability of the
objective function at the local minima found:</p>
<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb736-1"><a href="optimisation-with-genetic-algorithms.html#cb736-1" aria-hidden="true"></a><span class="kw">boxplot</span>(<span class="kw">as.data.frame</span>(<span class="kw">t</span>(res)), <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:gendata5"></span>
<img src="08-optimisation-genetic-figures/gendata5-1.png" alt="" />
<p class="caption">Figure 8.1:  plot of chunk gendata5</p>
</div>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb737"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb737-1"><a href="optimisation-with-genetic-algorithms.html#cb737-1" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">apply</span>(res, <span class="dv">1</span>, <span class="cf">function</span>(x)</span>
<span id="cb737-2"><a href="optimisation-with-genetic-algorithms.html#cb737-2" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="kw">summary</span>(x), <span class="dt">sd=</span><span class="kw">sd</span>(x))</span>
<span id="cb737-3"><a href="optimisation-with-genetic-algorithms.html#cb737-3" aria-hidden="true"></a>))</span></code></pre></div>
<pre><code>##         HartiganWong    Lloyd  optim
## Min.          421889 425119.5 422989
## 1st Qu.       424663 433669.3 432446
## Median        427129 438502.2 440033
## Mean          426557 438075.0 440635
## 3rd Qu.       428243 441381.3 446614
## Max.          431869 450469.7 466303
## sd              2301   5709.3  10888</code></pre>
<p>Of course, we are interested in the smallest value of the objective,
because we’re trying to pinpoint the global minimum.</p>
<div class="sourceCode" id="cb739"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb739-1"><a href="optimisation-with-genetic-algorithms.html#cb739-1" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">apply</span>(res, <span class="dv">1</span>, min))</span></code></pre></div>
<pre><code>## HartiganWong        Lloyd        optim 
##       421889       425119       422989</code></pre>
<div style="margin-top: 1em">

</div>
<p>The Hartigan-Wong algorithm (the default one in <code>kmeans()</code>)
is the most reliable one of the three:</p>
<ul>
<li>it gives the best solution (low bias)</li>
<li>the solutions have the lowest degree of variability (low variance)</li>
<li>it is the fastest:</li>
</ul>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb741-1"><a href="optimisation-with-genetic-algorithms.html#cb741-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</span>
<span id="cb741-2"><a href="optimisation-with-genetic-algorithms.html#cb741-2" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb741-3"><a href="optimisation-with-genetic-algorithms.html#cb741-3" aria-hidden="true"></a>mu0 &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),]</span>
<span id="cb741-4"><a href="optimisation-with-genetic-algorithms.html#cb741-4" aria-hidden="true"></a><span class="kw">summary</span>(<span class="kw">microbenchmark</span>(</span>
<span id="cb741-5"><a href="optimisation-with-genetic-algorithms.html#cb741-5" aria-hidden="true"></a>    <span class="dt">HartiganWong=</span><span class="kw">min_HartiganWong</span>(mu0, X),</span>
<span id="cb741-6"><a href="optimisation-with-genetic-algorithms.html#cb741-6" aria-hidden="true"></a>    <span class="dt">Lloyd=</span><span class="kw">min_Lloyd</span>(mu0, X),</span>
<span id="cb741-7"><a href="optimisation-with-genetic-algorithms.html#cb741-7" aria-hidden="true"></a>    <span class="dt">optim=</span><span class="kw">min_optim</span>(mu0, X),</span>
<span id="cb741-8"><a href="optimisation-with-genetic-algorithms.html#cb741-8" aria-hidden="true"></a>    <span class="dt">times=</span><span class="dv">10</span></span>
<span id="cb741-9"><a href="optimisation-with-genetic-algorithms.html#cb741-9" aria-hidden="true"></a>), <span class="dt">unit=</span><span class="st">&quot;relative&quot;</span>)</span></code></pre></div>
<pre><code>##           expr       min        lq      mean    median        uq
## 1 HartiganWong    1.1066    1.1052    1.1774    1.1984    1.1716
## 2        Lloyd    1.0000    1.0000    1.0000    1.0000    1.0000
## 3        optim 1637.6214 1639.4981 1577.0414 1612.6708 1503.2987
##         max neval
## 1    1.3662    10
## 2    1.0000    10
## 3 1516.5368    10</code></pre>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb743"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb743-1"><a href="optimisation-with-genetic-algorithms.html#cb743-1" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">min</span>(res))</span></code></pre></div>
<pre><code>## [1] 421889</code></pre>
<p>Is it the global minimum?</p>
<blockquote>
<p>We don’t know, we just didn’t happen to find anything better (yet).</p>
</blockquote>
<p>Did we put enough effort to find it?</p>
<blockquote>
<p>Well, maybe. We can try more random restarts:</p>
</blockquote>
<div class="sourceCode" id="cb745"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb745-1"><a href="optimisation-with-genetic-algorithms.html#cb745-1" aria-hidden="true"></a>res_tried_very_hard &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, K, <span class="dt">nstart=</span><span class="dv">100000</span>, <span class="dt">iter.max=</span><span class="dv">10000</span>)<span class="op">$</span>centers</span>
<span id="cb745-2"><a href="optimisation-with-genetic-algorithms.html#cb745-2" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">get_loss</span>(res_tried_very_hard, X))</span></code></pre></div>
<pre><code>## [1] 421889</code></pre>
<p>Is it good enough?</p>
<blockquote>
<p>It depends what we’d like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem?
Is it useful anyhow?
Are you really looking for the global minimum?</p>
</blockquote>
</div>
</div>
<div id="genetic-algorithms" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Genetic Algorithms</h2>
<div id="introduction-15" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Introduction</h3>
<div style="margin-top: 1em">

</div>
<p>What if our optimisation problem cannot be solved reliably with
gradient-based methods like those in <code>optim()</code> and we don’t have
any custom solver for the task at hand?</p>
<p>There are a couple of useful <em>metaheuristics</em> in the literature
that can serve this purpose.</p>
<p>Most of them rely on clever randomised search.</p>
<p>They are slow to run and don’t guarantee anything, but yet they still might be useful – some claim that <strong>a</strong> solution is better than no solution at all.</p>
<div style="margin-top: 1em">

</div>
<p>There is a wide class of <strong>nature-inspired</strong> algorithms (that traditionally
belong to the subfield of AI called <em>computational intelligence</em> or <em>soft computing</em>);
see, e.g, <span class="citation">(Simon <a href="#ref-evolution" role="doc-biblioref">2013</a>)</span>:</p>
<ul>
<li><p>evolutionary algorithms – inspired by the principle of natural selection</p>
<blockquote>
<p>maintain a population of candidate solutions, let the “fittest” combine with each
other to generate new “offspring” solutions.</p>
</blockquote></li>
</ul>
<div style="margin-top: 1em">

</div>
<ul>
<li><p>swarm algorithms</p>
<blockquote>
<p>maintain a herd of candidate solutions, allow them to “explore” the environment,
“communicate” with each other in order to seek the best spot to “go to”.</p>
</blockquote>
<p>For example:</p>
<ul>
<li>ant colony</li>
<li>bees</li>
<li>cuckoo search</li>
<li>particle swarm</li>
<li>krill herd</li>
</ul></li>
<li><p>other metaheuristics:</p>
<ul>
<li>harmony search</li>
<li>memetic algorithm</li>
<li>firefly algorithm</li>
</ul></li>
</ul>
<p>All of these sound fancy, but the general ideas behind them are pretty simple.</p>
</div>
<div id="overview-of-the-method" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Overview of the Method</h3>
<div style="margin-top: 1em">

</div>
<p>Genetic algorithms (GAs) are amongst the most popular evolutionary approaches.</p>
<p>They are based on Charles Darwin’s work on evolution by natural selection;
first proposed by John Holland in the 1960s.</p>
<p>See (Goldberg, 1989) for a comprehensive overview
and (Simon, 2013) for extensions.</p>
<p>Here is the general idea of a GA (there might be many)
to minimise a given objective/loss function <span class="math inline">\(f\)</span>
over a given domain <span class="math inline">\(D\)</span>.</p>
<div style="margin-top: 1em">

</div>
<ol style="list-style-type: decimal">
<li><p>Generate a random initial population of individuals – <span class="math inline">\(n_\text{pop}\)</span> points in <span class="math inline">\(D\)</span>,
e.g., <span class="math inline">\(n_\text{pop}=32\)</span></p></li>
<li><p>Repeat until some convergence criterion is not met:</p>
<ol style="list-style-type: lower-alpha">
<li>evaluate the fitness of each individual
(the smaller the loss, the greater its fitness)</li>
<li>select the pairs of the individuals for reproduction, the fitter
should be selected more eagerly</li>
<li>apply crossover operations to create offspring</li>
<li>slightly mutate randomly selected individuals</li>
<li>replace the old population with the new one</li>
</ol></li>
</ol>
</div>
<div id="example-implementation---ga-for-k-means" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Example Implementation - GA for K-means</h3>
<!-- TODO: check unbalance -->
<div style="margin-top: 1em">

</div>
<p>Initial setup:</p>
<div class="sourceCode" id="cb747"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb747-1"><a href="optimisation-with-genetic-algorithms.html#cb747-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb747-2"><a href="optimisation-with-genetic-algorithms.html#cb747-2" aria-hidden="true"></a></span>
<span id="cb747-3"><a href="optimisation-with-genetic-algorithms.html#cb747-3" aria-hidden="true"></a><span class="co"># simulation parameters:</span></span>
<span id="cb747-4"><a href="optimisation-with-genetic-algorithms.html#cb747-4" aria-hidden="true"></a>npop &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb747-5"><a href="optimisation-with-genetic-algorithms.html#cb747-5" aria-hidden="true"></a>niter &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb747-6"><a href="optimisation-with-genetic-algorithms.html#cb747-6" aria-hidden="true"></a></span>
<span id="cb747-7"><a href="optimisation-with-genetic-algorithms.html#cb747-7" aria-hidden="true"></a><span class="co"># randomly generate an initial population of size `npop`:</span></span>
<span id="cb747-8"><a href="optimisation-with-genetic-algorithms.html#cb747-8" aria-hidden="true"></a>pop &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>npop, <span class="cf">function</span>(i) X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),])</span>
<span id="cb747-9"><a href="optimisation-with-genetic-algorithms.html#cb747-9" aria-hidden="true"></a></span>
<span id="cb747-10"><a href="optimisation-with-genetic-algorithms.html#cb747-10" aria-hidden="true"></a><span class="co"># evaluate loss for each individual:</span></span>
<span id="cb747-11"><a href="optimisation-with-genetic-algorithms.html#cb747-11" aria-hidden="true"></a>cur_loss &lt;-<span class="st"> </span><span class="kw">sapply</span>(pop, get_loss, X)</span>
<span id="cb747-12"><a href="optimisation-with-genetic-algorithms.html#cb747-12" aria-hidden="true"></a>cur_best_loss &lt;-<span class="st"> </span><span class="kw">min</span>(cur_loss)</span>
<span id="cb747-13"><a href="optimisation-with-genetic-algorithms.html#cb747-13" aria-hidden="true"></a>best_loss &lt;-<span class="st"> </span>cur_best_loss</span></code></pre></div>
<p>Each individual in the population is just the set of <span class="math inline">\(K\)</span>
candidate cluster centres represented as a matrix in <span class="math inline">\(\mathbb{R}^{K\times p}\)</span>.</p>
<div style="margin-top: 1em">

</div>
<p>Let’s assume that the loss for each individual should be a function of
the rank of the objective function’s value
(smallest objective/loss == highest rank/utility/fitness == best fit).</p>
<p>For the crossover, we will sample pairs of individuals with
probabilities inversely proportional to their losses.</p>
<div class="sourceCode" id="cb748"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb748-1"><a href="optimisation-with-genetic-algorithms.html#cb748-1" aria-hidden="true"></a>selection &lt;-<span class="st"> </span><span class="cf">function</span>(cur_loss) {</span>
<span id="cb748-2"><a href="optimisation-with-genetic-algorithms.html#cb748-2" aria-hidden="true"></a>    npop &lt;-<span class="st"> </span><span class="kw">length</span>(cur_loss)</span>
<span id="cb748-3"><a href="optimisation-with-genetic-algorithms.html#cb748-3" aria-hidden="true"></a>    probs &lt;-<span class="st"> </span><span class="kw">rank</span>(<span class="op">-</span>cur_loss)</span>
<span id="cb748-4"><a href="optimisation-with-genetic-algorithms.html#cb748-4" aria-hidden="true"></a>    probs &lt;-<span class="st"> </span>probs<span class="op">/</span><span class="kw">sum</span>(probs)</span>
<span id="cb748-5"><a href="optimisation-with-genetic-algorithms.html#cb748-5" aria-hidden="true"></a>    left  &lt;-<span class="st"> </span><span class="kw">sample</span>(npop, npop, <span class="dt">replace=</span><span class="ot">TRUE</span>, <span class="dt">prob=</span>probs)</span>
<span id="cb748-6"><a href="optimisation-with-genetic-algorithms.html#cb748-6" aria-hidden="true"></a>    right &lt;-<span class="st"> </span><span class="kw">sample</span>(npop, npop, <span class="dt">replace=</span><span class="ot">TRUE</span>, <span class="dt">prob=</span>probs)</span>
<span id="cb748-7"><a href="optimisation-with-genetic-algorithms.html#cb748-7" aria-hidden="true"></a>    <span class="kw">cbind</span>(left, right)</span>
<span id="cb748-8"><a href="optimisation-with-genetic-algorithms.html#cb748-8" aria-hidden="true"></a>}</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>An example crossover combines each cluster centre
in such a way that we take a few coordinates of the “left” parent
and the remaining ones from the “right” parent:</p>
<div class="figure"><span id="fig:crossover"></span>
<img src="figures/crossover.png" alt="" />
<p class="caption">Figure 8.2:  Crossover</p>
</div>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb749"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb749-1"><a href="optimisation-with-genetic-algorithms.html#cb749-1" aria-hidden="true"></a>crossover &lt;-<span class="st"> </span><span class="cf">function</span>(pop, pairs, K, p) {</span>
<span id="cb749-2"><a href="optimisation-with-genetic-algorithms.html#cb749-2" aria-hidden="true"></a>    old_pop &lt;-<span class="st"> </span>pop</span>
<span id="cb749-3"><a href="optimisation-with-genetic-algorithms.html#cb749-3" aria-hidden="true"></a>    pop &lt;-<span class="st"> </span>pop[pairs[,<span class="dv">2</span>]]</span>
<span id="cb749-4"><a href="optimisation-with-genetic-algorithms.html#cb749-4" aria-hidden="true"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(pop)) {</span>
<span id="cb749-5"><a href="optimisation-with-genetic-algorithms.html#cb749-5" aria-hidden="true"></a>        wh &lt;-<span class="st"> </span><span class="kw">sample</span>(p<span class="dv">-1</span>, K, <span class="dt">replace=</span><span class="ot">TRUE</span>)</span>
<span id="cb749-6"><a href="optimisation-with-genetic-algorithms.html#cb749-6" aria-hidden="true"></a>        <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K)</span>
<span id="cb749-7"><a href="optimisation-with-genetic-algorithms.html#cb749-7" aria-hidden="true"></a>            pop[[j]][l,<span class="dv">1</span><span class="op">:</span>wh[l]] &lt;-</span>
<span id="cb749-8"><a href="optimisation-with-genetic-algorithms.html#cb749-8" aria-hidden="true"></a><span class="st">                </span>old_pop[[pairs[j,<span class="dv">1</span>]]][l,<span class="dv">1</span><span class="op">:</span>wh[l]]</span>
<span id="cb749-9"><a href="optimisation-with-genetic-algorithms.html#cb749-9" aria-hidden="true"></a>    }</span>
<span id="cb749-10"><a href="optimisation-with-genetic-algorithms.html#cb749-10" aria-hidden="true"></a>    pop</span>
<span id="cb749-11"><a href="optimisation-with-genetic-algorithms.html#cb749-11" aria-hidden="true"></a>}</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>Mutation (occurring with a very small probability)
substitutes some cluster centre with a random vector from the input dataset.</p>
<div class="sourceCode" id="cb750"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb750-1"><a href="optimisation-with-genetic-algorithms.html#cb750-1" aria-hidden="true"></a>mutate &lt;-<span class="st"> </span><span class="cf">function</span>(pop, X, K) {</span>
<span id="cb750-2"><a href="optimisation-with-genetic-algorithms.html#cb750-2" aria-hidden="true"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(pop)) {</span>
<span id="cb750-3"><a href="optimisation-with-genetic-algorithms.html#cb750-3" aria-hidden="true"></a>       <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.025</span>) {</span>
<span id="cb750-4"><a href="optimisation-with-genetic-algorithms.html#cb750-4" aria-hidden="true"></a>          szw &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="dv">1</span>)</span>
<span id="cb750-5"><a href="optimisation-with-genetic-algorithms.html#cb750-5" aria-hidden="true"></a>          pop[[j]][szw,] &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), <span class="kw">length</span>(szw)),]</span>
<span id="cb750-6"><a href="optimisation-with-genetic-algorithms.html#cb750-6" aria-hidden="true"></a>       }</span>
<span id="cb750-7"><a href="optimisation-with-genetic-algorithms.html#cb750-7" aria-hidden="true"></a>    }</span>
<span id="cb750-8"><a href="optimisation-with-genetic-algorithms.html#cb750-8" aria-hidden="true"></a>    pop</span>
<span id="cb750-9"><a href="optimisation-with-genetic-algorithms.html#cb750-9" aria-hidden="true"></a>}</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>We also need a function that checks if
the new cluster centres aren’t too far away from the input points.</p>
<p>If it happens that we have empty clusters, our solution is degenerate and
we must correct it.</p>
<p>All “bad” cluster centres will be substituted with randomly chosen points from <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Moreover, we will recompute the cluster centres as the componentwise arithmetic mean
of the closest points, just like in Lloyd’s algorithm, to speed up convergence.</p>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb751"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb751-1"><a href="optimisation-with-genetic-algorithms.html#cb751-1" aria-hidden="true"></a>recompute_mus &lt;-<span class="st"> </span><span class="cf">function</span>(pop, X, K) {</span>
<span id="cb751-2"><a href="optimisation-with-genetic-algorithms.html#cb751-2" aria-hidden="true"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(pop)) {</span>
<span id="cb751-3"><a href="optimisation-with-genetic-algorithms.html#cb751-3" aria-hidden="true"></a>    <span class="co"># get nearest cluster centres for each point:</span></span>
<span id="cb751-4"><a href="optimisation-with-genetic-algorithms.html#cb751-4" aria-hidden="true"></a>    memb &lt;-<span class="st"> </span><span class="kw">get.knnx</span>(pop[[j]], X, <span class="dv">1</span>)<span class="op">$</span>nn.index</span>
<span id="cb751-5"><a href="optimisation-with-genetic-algorithms.html#cb751-5" aria-hidden="true"></a>    sz &lt;-<span class="st"> </span><span class="kw">tabulate</span>(memb, K) <span class="co"># number of points in each cluster</span></span>
<span id="cb751-6"><a href="optimisation-with-genetic-algorithms.html#cb751-6" aria-hidden="true"></a>    <span class="co"># if there are empty clusters, fix them:</span></span>
<span id="cb751-7"><a href="optimisation-with-genetic-algorithms.html#cb751-7" aria-hidden="true"></a>    szw &lt;-<span class="st"> </span><span class="kw">which</span>(sz<span class="op">==</span><span class="dv">0</span>)</span>
<span id="cb751-8"><a href="optimisation-with-genetic-algorithms.html#cb751-8" aria-hidden="true"></a>    <span class="cf">if</span> (<span class="kw">length</span>(szw)<span class="op">&gt;</span><span class="dv">0</span>) { <span class="co"># random points in X will be new cluster centres</span></span>
<span id="cb751-9"><a href="optimisation-with-genetic-algorithms.html#cb751-9" aria-hidden="true"></a>        pop[[j]][szw,] &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), <span class="kw">length</span>(szw)),]</span>
<span id="cb751-10"><a href="optimisation-with-genetic-algorithms.html#cb751-10" aria-hidden="true"></a>        memb &lt;-<span class="st"> </span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(pop[[j]], X, <span class="dv">1</span>)<span class="op">$</span>nn.index</span>
<span id="cb751-11"><a href="optimisation-with-genetic-algorithms.html#cb751-11" aria-hidden="true"></a>        sz &lt;-<span class="st"> </span><span class="kw">tabulate</span>(memb, K)</span>
<span id="cb751-12"><a href="optimisation-with-genetic-algorithms.html#cb751-12" aria-hidden="true"></a>    }</span>
<span id="cb751-13"><a href="optimisation-with-genetic-algorithms.html#cb751-13" aria-hidden="true"></a>    <span class="co"># recompute cluster centres - componentwise average:</span></span>
<span id="cb751-14"><a href="optimisation-with-genetic-algorithms.html#cb751-14" aria-hidden="true"></a>    pop[[j]][,] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb751-15"><a href="optimisation-with-genetic-algorithms.html#cb751-15" aria-hidden="true"></a>    <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X))</span>
<span id="cb751-16"><a href="optimisation-with-genetic-algorithms.html#cb751-16" aria-hidden="true"></a>        pop[[j]][memb[l],] &lt;-<span class="st"> </span>pop[[j]][memb[l],]<span class="op">+</span>X[l,]</span>
<span id="cb751-17"><a href="optimisation-with-genetic-algorithms.html#cb751-17" aria-hidden="true"></a>    pop[[j]] &lt;-<span class="st"> </span>pop[[j]]<span class="op">/</span>sz</span>
<span id="cb751-18"><a href="optimisation-with-genetic-algorithms.html#cb751-18" aria-hidden="true"></a>  }</span>
<span id="cb751-19"><a href="optimisation-with-genetic-algorithms.html#cb751-19" aria-hidden="true"></a>  pop</span>
<span id="cb751-20"><a href="optimisation-with-genetic-algorithms.html#cb751-20" aria-hidden="true"></a>}</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<p>We are ready to build our genetic algorithm to solve the K-means clustering problem:</p>
<div class="sourceCode" id="cb752"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb752-1"><a href="optimisation-with-genetic-algorithms.html#cb752-1" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>niter) {</span>
<span id="cb752-2"><a href="optimisation-with-genetic-algorithms.html#cb752-2" aria-hidden="true"></a>    pairs &lt;-<span class="st"> </span><span class="kw">selection</span>(cur_loss)</span>
<span id="cb752-3"><a href="optimisation-with-genetic-algorithms.html#cb752-3" aria-hidden="true"></a>    pop &lt;-<span class="st"> </span><span class="kw">crossover</span>(pop, pairs, K, p)</span>
<span id="cb752-4"><a href="optimisation-with-genetic-algorithms.html#cb752-4" aria-hidden="true"></a>    pop &lt;-<span class="st"> </span><span class="kw">mutate</span>(pop, X, K)</span>
<span id="cb752-5"><a href="optimisation-with-genetic-algorithms.html#cb752-5" aria-hidden="true"></a>    pop &lt;-<span class="st"> </span><span class="kw">recompute_mus</span>(pop, X, K)</span>
<span id="cb752-6"><a href="optimisation-with-genetic-algorithms.html#cb752-6" aria-hidden="true"></a>    <span class="co"># re-evaluate losses:</span></span>
<span id="cb752-7"><a href="optimisation-with-genetic-algorithms.html#cb752-7" aria-hidden="true"></a>    cur_loss &lt;-<span class="st"> </span><span class="kw">sapply</span>(pop, get_loss, X)</span>
<span id="cb752-8"><a href="optimisation-with-genetic-algorithms.html#cb752-8" aria-hidden="true"></a>    cur_best_loss &lt;-<span class="st"> </span><span class="kw">min</span>(cur_loss)</span>
<span id="cb752-9"><a href="optimisation-with-genetic-algorithms.html#cb752-9" aria-hidden="true"></a>    <span class="co"># give feedback on what&#39;s going on:</span></span>
<span id="cb752-10"><a href="optimisation-with-genetic-algorithms.html#cb752-10" aria-hidden="true"></a>    <span class="cf">if</span> (cur_best_loss <span class="op">&lt;</span><span class="st"> </span>best_loss) {</span>
<span id="cb752-11"><a href="optimisation-with-genetic-algorithms.html#cb752-11" aria-hidden="true"></a>        best_loss &lt;-<span class="st"> </span>cur_best_loss</span>
<span id="cb752-12"><a href="optimisation-with-genetic-algorithms.html#cb752-12" aria-hidden="true"></a>        best_mu &lt;-<span class="st"> </span>pop[[<span class="kw">which.min</span>(cur_loss)]]</span>
<span id="cb752-13"><a href="optimisation-with-genetic-algorithms.html#cb752-13" aria-hidden="true"></a>        <span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%5d: f_best=%10.5f</span><span class="ch">\n</span><span class="st">&quot;</span>, i, best_loss))</span>
<span id="cb752-14"><a href="optimisation-with-genetic-algorithms.html#cb752-14" aria-hidden="true"></a>    }</span>
<span id="cb752-15"><a href="optimisation-with-genetic-algorithms.html#cb752-15" aria-hidden="true"></a>}</span></code></pre></div>
<div style="margin-top: 1em">

</div>
<pre><code>##     1: f_best=435638.52165
##     2: f_best=428808.89706
##     4: f_best=428438.45125
##     6: f_best=422277.99136
##     8: f_best=421889.46265</code></pre>
<div class="sourceCode" id="cb754"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb754-1"><a href="optimisation-with-genetic-algorithms.html#cb754-1" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">get_loss</span>(best_mu, X))</span></code></pre></div>
<pre><code>## [1] 421889</code></pre>
<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb756-1"><a href="optimisation-with-genetic-algorithms.html#cb756-1" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">get_loss</span>(res_tried_very_hard, X))</span></code></pre></div>
<pre><code>## [1] 421889</code></pre>
<p>It works! :)</p>
<!--

(\*) Interestingly, the above can be rewritten as:
\[
\min_{ (c_1,\dots,c_n)\in\{1,\dots,k\}^n }
\sum_{i=1}^n \left(
\sum_{\ell=1}^p \left(x_\ell^{(i)}-\mu_\ell^{(c_i)}\right)^2
\right)
\]
with $c_i$ denoting the cluster number (between $1$ and $k$) of the $i$-th input point
and the centre of the $j$-th cluster:
\[
\mu_\ell^{(j)} =
\frac{1}{|\{i: c_i=j\}|}
 \sum_{i: c_i=j} x_\ell^{(i)}
\]
being defined as the arithmetic mean of the coordinates
of all the input points belonging to that cluster.

> Here $|\{\cdots\}|$ denotes the number of elements in a given set.

-->
</div>
</div>
<div id="outro-7" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Outro</h2>
<div id="remarks-7" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Remarks</h3>
<div style="margin-top: 1em">

</div>
<p>For any <span class="math inline">\(p\ge 1\)</span>, the search space type determines the problem class:</p>
<ul>
<li><p><span class="math inline">\(\mathbb{D}\subseteq\mathbb{R}^p\)</span> – <strong>continuous optimisation</strong></p>
<p>In particular:</p>
<ul>
<li><p><span class="math inline">\(\mathbb{D}=\mathbb{R}^p\)</span> – continuous unconstrained</p></li>
<li><p><span class="math inline">\(\mathbb{D}=[a_1,b_1]\times\dots\times[a_n,b_n]\)</span> – continuous with box constraints</p></li>
<li><p>constrained with <span class="math inline">\(k\)</span> linear inequality constraints</p>
<p><span class="math display">\[
  \left\{
  \begin{array}{lll}
  a_{1,1} x_1 + \dots + a_{1,p} x_p &amp;\le&amp; b_1 \\
  &amp;\vdots&amp;\\
  a_{k,1} x_1 + \dots + a_{k,p} x_p &amp;\le&amp; b_k \\
  \end{array}
  \right.
  \]</span></p></li>
</ul></li>
</ul>
<div style="margin-top: 1em">

</div>
<p>However, there are other possibilities as well:</p>
<ul>
<li><p><span class="math inline">\(\mathbb{D}\subseteq\mathbb{Z}^p\)</span> (<span class="math inline">\(\mathbb{Z}\)</span> – the set of integers) – <strong>discrete optimisation</strong></p>
<p>In particular:</p>
<ul>
<li><span class="math inline">\(\mathbb{D}=\{0,1\}^p\)</span> – 0–1 optimisation (hard!)</li>
</ul></li>
<li><p><span class="math inline">\(\mathbb{D}\)</span> is finite (but perhaps large, its objects can be enumerated) – <strong>combination optimisation</strong></p>
<p>For example:</p>
<ul>
<li><span class="math inline">\(\mathbb{D}=\)</span> all possible routes between two points on a map.</li>
</ul></li>
</ul>
<blockquote>
<p>These optimisation tasks tend to be much harder than the continuous ones.</p>
</blockquote>
<p>Genetic algorithms might come in handy in such cases.</p>
<div style="margin-top: 1em">

</div>
<p>Specialised methods, customised to solve a specific problem (like Lloyd’s algorithm)
will often outperform generic ones (like SGD, genetic algorithms)
in terms of speed and reliability.</p>
<p>All in all, we prefer a suboptimal solution obtained by means of heuristics
to no solution at all.</p>
<p>Problems that you could try solving with GAs include variable selection
in multiple regression – finding the subset of features optimising the AIC
(this is a hard problem to and forward selection was just a simple greed heuristic).</p>
<div style="margin-top: 1em">

</div>
<p>Other interesting algorithms:</p>
<ul>
<li>Hill Climbing (a simple variation of GD with no gradient use)</li>
<li>Simulated annealing</li>
<li>CMA-ES</li>
<li>Tabu search</li>
<li>Particle swarm optimisation (e.g, <code>hydroPSO</code> package)</li>
<li>Artificial bee/ant colony optimisation</li>
<li>Cuckoo Search</li>
<li>Differential Evolution (e.g., <code>DEoptim</code> package)</li>
</ul>
</div>
<div id="further-reading-7" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Further Reading</h3>
<div id="section" class="section level4 allowframebreaks unnumbered" number="">
<h4 class="allowframebreaks unnumbered" number=""></h4>
<p>Recommended further reading:</p>
<ul>
<li><span class="citation">(Goldberg <a href="#ref-genetic" role="doc-biblioref">1989</a>)</span></li>
</ul>
<p>Other:</p>
<ul>
<li><span class="citation">(Simon <a href="#ref-evolution" role="doc-biblioref">2013</a>)</span></li>
</ul>
<p>See also package <code>GA</code>.</p>

<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-genetic">
<p>Goldberg DE (1989) <em>Genetic algorithms in search, optimization and machine learning</em>. Addison-Wesley.</p>
</div>
<div id="ref-evolution">
<p>Simon D (2013) <em>Evolutionary optimization algorithms: Biologically-inspired and population-based approaches to computer intelligence</em>. Wiley.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="recommender-systems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
