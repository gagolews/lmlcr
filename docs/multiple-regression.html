<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Multiple Regression | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="pandoc, pandoc-citeproc, knitr, bookdown (custom), GitBook, and many more" />

  <meta property="og:title" content="2 Multiple Regression | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="classification-with-k-nearest-neighbours.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<!-- Use KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<!-- /Use KaTeX -->

<style type="text/css">
/* Marek's custom CSS */

@import url("https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic,700italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans:400,500,600,700,800,900,400italic,500italic,600italic,700italic,800italic,900italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:400,600,700,400italic,600italic,700italic");

span.katex {
    font-size: 100%;
}

</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2.1 2021-05-10 09:49 (f981ede)</a></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#exercises-in-r-2"><i class="fa fa-check"></i><b>4.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-preparing-data"><i class="fa fa-check"></i><b>4.4.1</b> EdStats – Preparing Data</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.4.2</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.4.3</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>4.4.4</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>4.4.5</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.5</b> Outro</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks (*)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="5.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>5.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Continuous Optimisation with Iterative Algorithms (*)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="6.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>6.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="6.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.5</b> Outro</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>7.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3</b> Agglomerative Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>7.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#exercises-in-r-3"><i class="fa fa-check"></i><b>7.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>7.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>7.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>7.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.5</b> Outro</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms (*)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.2</b> Genetic Algorithms</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.2.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.3</b> Outro</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.3.1</b> Remarks</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems (*)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-16"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> Exercise: The MovieLens Dataset (*)</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a>
<ul>
<li class="chapter" data-level="B.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a>
<ul>
<li class="chapter" data-level="C.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a>
<ul>
<li class="chapter" data-level="C.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a>
<ul>
<li class="chapter" data-level="C.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>C.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a>
<ul>
<li class="chapter" data-level="D.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>D.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a>
<ul>
<li class="chapter" data-level="E.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a>
<ul>
<li class="chapter" data-level="E.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>E.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-regression" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Multiple Regression</h1>
<blockquote>
<p>This is a slightly older (distributed in the hope that it will be useful)
version of the forthcoming textbook (ETA 2022) preliminarily entitled
<em>Machine Learning in R from Scratch</em> by
<a href="https://www.gagolewski.com">Marek Gagolewski</a>, which is now undergoing
a major revision (when I am not busy with other projects). There will be
not much work on-going in this repository anymore, as its sources have
moved elsewhere; however, if you happen to find any bugs or typos,
please drop me an
<a href="https://github.com/gagolews/lmlcr/blob/master/CODE_OF_CONDUCT.md">email</a>.
I will share a new draft once it’s ripe. Stay tuned.</p>
</blockquote>
<!-- TODO

standardisation of variables and interpretability

show how to "derive" the original model from the "standardised" one

-->
<div id="introduction-2" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<div id="formalism-1" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Formalism</h3>
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>In other words, we have a database on <span class="math inline">\(n\)</span> objects, each of which
being described by means of <span class="math inline">\(p\)</span> numerical features.</p>
<p><span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \\
\end{array}
\right]
\]</span></p>
<p>Recall that in supervised learning,
apart from <span class="math inline">\(\mathbf{X}\)</span>, we are also given the corresponding <span class="math inline">\(\mathbf{y}\)</span>;
with each input point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output <span class="math inline">\(y_i\)</span>.</p>
<p>In this chapter we are still interested in <strong>regression</strong> tasks;
hence, we assume that each <span class="math inline">\(y_i\)</span>
it is a real number, i.e., <span class="math inline">\(y_i\in\mathbb{R}\)</span>.</p>
<p>Hence, our dataset is <span class="math inline">\([\mathbf{X}\ \mathbf{y}]\)</span> –
where each object is represented as a row vector
<span class="math inline">\([\mathbf{x}_{i,\cdot}\ y_i]\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>:</p>
<p><span class="math display">\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} &amp; y_1\\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} &amp; y_2\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots    &amp; \vdots\\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} &amp; y_n\\
\end{array}
\right].
\]</span></p>
</div>
<div id="simple-linear-regression---recap" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Simple Linear Regression - Recap</h3>
<p>In a simple regression task, we have assumed that <span class="math inline">\(p=1\)</span> – there is only
one independent variable,
denoted <span class="math inline">\(x_i=x_{i,1}\)</span>.</p>
<p>We restricted ourselves to linear models of the form <span class="math inline">\(Y=f(X)=a+bX\)</span>
that minimised the sum of squared residuals (SSR), i.e.,</p>
<p><span class="math display">\[
\min_{a,b\in\mathbb{R}} \sum_{i=1}^n \left(
a+bx_i-y_i
\right)^2.
\]</span></p>
<p>The solution is:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rl}
b  = &amp; \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
a = &amp; \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - b  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]</span></p>
<p>Fitting in R can be performed by calling the <code>lm()</code> function:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="multiple-regression.html#cb87-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;ISLR&quot;</span>) <span class="co"># Credit dataset</span></span>
<span id="cb87-2"><a href="multiple-regression.html#cb87-2" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb87-3"><a href="multiple-regression.html#cb87-3" aria-hidden="true"></a>Y &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb87-4"><a href="multiple-regression.html#cb87-4" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X) <span class="co"># Y~X is a formula, read: Y is a function of X</span></span>
<span id="cb87-5"><a href="multiple-regression.html#cb87-5" aria-hidden="true"></a><span class="kw">print</span>(f)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     226.471        0.266</code></pre>
<p>Figure <a href="multiple-regression.html#fig:simple-recap2">2.1</a> gives the scatter plot
of Y vs. X together with the fitted simple linear model.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="multiple-regression.html#cb89-1" aria-hidden="true"></a><span class="kw">plot</span>(X, Y, <span class="dt">xlab=</span><span class="st">&quot;X (Balance)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Y (Credit)&quot;</span>)</span>
<span id="cb89-2"><a href="multiple-regression.html#cb89-2" aria-hidden="true"></a><span class="kw">abline</span>(f, <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:simple-recap2"></span>
<img src="02-regression-multiple-figures/simple-recap2-1.png" alt="" />
<p class="caption">Figure 2.1:  Fitted regression line for the Credit dataset</p>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Multiple Linear Regression</h2>
<div id="problem-formulation" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Problem Formulation</h3>
<p>Let’s now generalise the above to the case of many variables
<span class="math inline">\(X_1, \dots, X_p\)</span>.</p>
<p>We wish to model the dependent variable as a function of <span class="math inline">\(p\)</span> independent variables.
<span class="math display">\[
Y = f(X_1,\dots,X_p)   \qquad (+\varepsilon)
\]</span></p>
<p>Restricting ourselves to the class of <strong>linear models</strong>, we have
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
\]</span></p>
<p>Above we studied the case where <span class="math inline">\(p=1\)</span>, i.e., <span class="math inline">\(Y=a+bX_1\)</span> with <span class="math inline">\(\beta_0=a\)</span> and <span class="math inline">\(\beta_1=b\)</span>.</p>
<p>The above equation defines:</p>
<ul>
<li><span class="math inline">\(p=1\)</span> — a line (see Figure <a href="multiple-regression.html#fig:simple-recap2">2.1</a>),</li>
<li><span class="math inline">\(p=2\)</span> — a plane (see Figure <a href="multiple-regression.html#fig:scatterplot3dexample">2.2</a>),</li>
<li><span class="math inline">\(p\ge 3\)</span> — a hyperplane (well, most people find it difficult
to imagine objects in high dimensions,
but we are lucky to have this thing called maths).</li>
</ul>
<div class="figure"><span id="fig:scatterplot3dexample"></span>
<img src="02-regression-multiple-figures/scatterplot3dexample-1.png" alt="" />
<p class="caption">Figure 2.2:  Fitted regression plane for the Credit dataset</p>
</div>
</div>
<div id="fitting-a-linear-model-in-r" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Fitting a Linear Model in R</h3>
<p><code>lm()</code> accepts a formula of the form <code>Y~X1+X2+...+Xp</code>.</p>
<p>It finds the least squares fit, i.e., solves
<span class="math display">\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_i \right) ^2
\]</span></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="multiple-regression.html#cb90-1" aria-hidden="true"></a>X1 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb90-2"><a href="multiple-regression.html#cb90-2" aria-hidden="true"></a>X2 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Income[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb90-3"><a href="multiple-regression.html#cb90-3" aria-hidden="true"></a>Y  &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb90-4"><a href="multiple-regression.html#cb90-4" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span>X2)</span>
<span id="cb90-5"><a href="multiple-regression.html#cb90-5" aria-hidden="true"></a>f<span class="op">$</span>coefficients <span class="co"># ß0, ß1, ß2</span></span></code></pre></div>
<pre><code>## (Intercept)          X1          X2 
##    172.5587      0.1828      2.1976</code></pre>
<p>By the way, the 3D scatter plot in Figure <a href="multiple-regression.html#fig:scatterplot3dexample">2.2</a>
was generated by calling:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="multiple-regression.html#cb92-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;scatterplot3d&quot;</span>)</span>
<span id="cb92-2"><a href="multiple-regression.html#cb92-2" aria-hidden="true"></a>s3d &lt;-<span class="st"> </span><span class="kw">scatterplot3d</span>(X1, X2, Y,</span>
<span id="cb92-3"><a href="multiple-regression.html#cb92-3" aria-hidden="true"></a>    <span class="dt">angle=</span><span class="dv">60</span>, <span class="co"># change angle to reveal more</span></span>
<span id="cb92-4"><a href="multiple-regression.html#cb92-4" aria-hidden="true"></a>    <span class="dt">highlight.3d=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;Balance&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Income&quot;</span>,</span>
<span id="cb92-5"><a href="multiple-regression.html#cb92-5" aria-hidden="true"></a>    <span class="dt">zlab=</span><span class="st">&quot;Rating&quot;</span>)</span>
<span id="cb92-6"><a href="multiple-regression.html#cb92-6" aria-hidden="true"></a>s3d<span class="op">$</span><span class="kw">plane3d</span>(f, <span class="dt">lty.box=</span><span class="st">&quot;solid&quot;</span>)</span></code></pre></div>
<p>(<code>s3d</code> is an R list, one of its elements named <code>plane3d</code> is a function object – this is legal)</p>
</div>
</div>
<div id="finding-the-best-model" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Finding the Best Model</h2>
<div id="model-diagnostics" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Model Diagnostics</h3>
<!-- more metrics:
https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics
-->
<p>Here is Rating (<span class="math inline">\(Y\)</span>) as function of Balance (<span class="math inline">\(X_1\)</span>, lefthand side of Figure <a href="multiple-regression.html#fig:x12-y">2.3</a>)
and Income (<span class="math inline">\(X_2\)</span>, righthand side of Figure <a href="multiple-regression.html#fig:x12-y">2.3</a>).</p>
<div class="figure"><span id="fig:x12-y"></span>
<img src="02-regression-multiple-figures/x12-y-1.png" alt="" />
<p class="caption">Figure 2.3:  Scatter plots of <span class="math inline">\(Y\)</span> vs. <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span></p>
</div>
<p>Moreover, Figure <a href="multiple-regression.html#fig:x12-ycolmap">2.4</a> depicts
(in a hopefully readable manner) both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with Rating <span class="math inline">\(Y\)</span>
encoded with a colour (low ratings are green, high ratings are red;
some rating values are explicitly printed out within the plot).</p>
<div class="figure"><span id="fig:x12-ycolmap"></span>
<img src="02-regression-multiple-figures/x12-ycolmap-1.png" alt="" />
<p class="caption">Figure 2.4:  A heatmap for Rating as a function of Balance and Income; greens represent low credit ratings, whereas reds – high ones</p>
</div>
<p>Consider the three following models.</p>
<table>
<thead>
<tr class="header">
<th>Formula</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rating ~ Balance + Income</td>
<td><span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span></td>
</tr>
<tr class="even">
<td>Rating ~ Balance</td>
<td><span class="math inline">\(Y=a + b X_1\)</span> (<span class="math inline">\(\beta_0=a, \beta_1=b, \beta_2=0\)</span>)</td>
</tr>
<tr class="odd">
<td>Rating ~ Income</td>
<td><span class="math inline">\(Y=a + b X_2\)</span> (<span class="math inline">\(\beta_0=a, \beta_1=0, \beta_2=b\)</span>)</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="multiple-regression.html#cb93-1" aria-hidden="true"></a>f12 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span>X2) <span class="co"># Rating ~ Balance + Income</span></span>
<span id="cb93-2"><a href="multiple-regression.html#cb93-2" aria-hidden="true"></a>f12<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          X1          X2 
##    172.5587      0.1828      2.1976</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="multiple-regression.html#cb95-1" aria-hidden="true"></a>f1  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1)    <span class="co"># Rating ~ Balance</span></span>
<span id="cb95-2"><a href="multiple-regression.html#cb95-2" aria-hidden="true"></a>f1<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          X1 
##   226.47114     0.26615</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="multiple-regression.html#cb97-1" aria-hidden="true"></a>f2  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X2)    <span class="co"># Rating ~ Income</span></span>
<span id="cb97-2"><a href="multiple-regression.html#cb97-2" aria-hidden="true"></a>f2<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          X2 
##    253.8514      3.0253</code></pre>
<p>Which of the three models is the best?
Of course, by using the word “best”,
we need to answer the question “best?… but with respect to what kind of measure?”</p>
<p>So far we were fitting w.r.t. SSR,
as the multiple regression model generalises the two simple ones,
the former must yield a not-worse SSR.
This is because in the case of <span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>,
setting <span class="math inline">\(\beta_1\)</span> to 0 (just one of uncountably many possible <span class="math inline">\(\beta_1\)</span>s,
if it happens to be the <em>best</em> one, good for us)
gives <span class="math inline">\(Y=a + b X_2\)</span>
whereas by setting <span class="math inline">\(\beta_2\)</span> to 0 we obtain <span class="math inline">\(Y=a + b X_1\)</span>.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="multiple-regression.html#cb99-1" aria-hidden="true"></a><span class="kw">sum</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 358261</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="multiple-regression.html#cb101-1" aria-hidden="true"></a><span class="kw">sum</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 2132108</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="multiple-regression.html#cb103-1" aria-hidden="true"></a><span class="kw">sum</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1823473</code></pre>
<p>We get that, in terms of SSRs, <span class="math inline">\(f_{12}\)</span> is better than <span class="math inline">\(f_{2}\)</span>,
which in turn is better than <span class="math inline">\(f_{1}\)</span>.
However, these error values per se (sheer numbers)
are meaningless (not meaningful).</p>
<dl>
<dt>Remark.</dt>
<dd><p>Interpretability in ML has always been an important issue, think the EU
General Data Protection Regulation (GDPR), amongst others.</p>
</dd>
</dl>
<div id="ssr-mse-rmse-and-mae" class="section level4" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> SSR, MSE, RMSE and MAE</h4>
<p>The quality of fit can be assessed by performing some <em>descriptive
statistical analysis of the residuals</em>, <span class="math inline">\(\hat{y}_i-y_i\)</span>,
for <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>I know how to summarise data on the residuals!
Of course I should compute their arithmetic mean and I’m done with that shtask!
Interestingly, the mean of residuals (this can be shown analytically)
in the least squared fit is always equal to <span class="math inline">\(0\)</span>:
<span class="math display">\[
 \frac{1}{n} \sum_{i=1}^n (\hat{y}_i-y_i)=0.
\]</span>
Therefore, we need a different metric.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>(*) A proof of this fact is left as an exercise to the curious;
assume <span class="math inline">\(p=1\)</span> just as in the previous chapter and note that <span class="math inline">\(\hat{y}_i=a x_i+b\)</span>.</p>
</div>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="multiple-regression.html#cb105-1" aria-hidden="true"></a><span class="kw">mean</span>(f12<span class="op">$</span>residuals) <span class="co"># almost zero numerically</span></span></code></pre></div>
<pre><code>## [1] -2.0867e-16</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="multiple-regression.html#cb107-1" aria-hidden="true"></a><span class="kw">all.equal</span>(<span class="kw">mean</span>(f12<span class="op">$</span>residuals), <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>We noted that sum of squared residuals (SSR) is not interpretable,
but the mean squared residuals
(MSR) – also called mean squared error (MSE) regression loss – is a little better.
Recall that mean is defined as the sum divided by number of samples.</p>
<p><span class="math display">\[
 \mathrm{MSE}(f) = \frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2.
\]</span></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="multiple-regression.html#cb109-1" aria-hidden="true"></a><span class="kw">mean</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1155.7</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multiple-regression.html#cb111-1" aria-hidden="true"></a><span class="kw">mean</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 6877.8</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="multiple-regression.html#cb113-1" aria-hidden="true"></a><span class="kw">mean</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 5882.2</code></pre>
<p>This gives an information of how much do we err <em>per sample</em>,
so at least this measure does not depend on <span class="math inline">\(n\)</span> anymore.
However, if the original <span class="math inline">\(Y\)</span>s are, say, in metres <span class="math inline">\([\mathrm{m}]\)</span>,
MSE is expressed in metres squared <span class="math inline">\([\mathrm{m}^2]\)</span>.</p>
<p>To account for that, we may consider the root mean squared error (RMSE):
<span class="math display">\[
 \mathrm{RMSE}(f) = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2}.
\]</span>
This is just like with the sample variance vs. standard deviation –
recall the latter is defined as the square root of the former.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multiple-regression.html#cb115-1" aria-hidden="true"></a><span class="kw">sqrt</span>(<span class="kw">mean</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 33.995</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="multiple-regression.html#cb117-1" aria-hidden="true"></a><span class="kw">sqrt</span>(<span class="kw">mean</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 82.932</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="multiple-regression.html#cb119-1" aria-hidden="true"></a><span class="kw">sqrt</span>(<span class="kw">mean</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 76.695</code></pre>
<p>The interpretation of the RMSE is rather quirky;
it is some-sort-of-averaged <em>deviance</em> from the true rating
(which is on the scale 0–1000, hence we see that the first model is not
that bad). Recall that the square function is sensitive to large observations,
hence, it penalises notable deviations more heavily.</p>
<p>As still we have a problem with finding something easily interpretable
(your non-technical boss or client may ask you: but what do these numbers mean??),
we suggest here that the mean absolute error (MAE;
also called mean absolute deviations, MAD)
might be a better idea than the above:
<span class="math display">\[
 \mathrm{MAE}(f) = \frac{1}{n} \sum_{i=1}^n |f(\mathbf{x}_{i,\cdot})-y_i|.
\]</span></p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="multiple-regression.html#cb121-1" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">abs</span>(f12<span class="op">$</span>residuals))</span></code></pre></div>
<pre><code>## [1] 22.863</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="multiple-regression.html#cb123-1" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">abs</span>(f1<span class="op">$</span>residuals))</span></code></pre></div>
<pre><code>## [1] 61.489</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="multiple-regression.html#cb125-1" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">abs</span>(f2<span class="op">$</span>residuals))</span></code></pre></div>
<pre><code>## [1] 64.151</code></pre>
<p>With the above we may say “On average, the predicted rating differs from the
observed one by…”. That is good enough.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) You may ask why don’t we fit models so as to minimise the MAE
and we minimise the RMSE instead (note that minimising RMSE is the same as
minimising the SSR, one is a strictly monotone transformation of the other
and do not affect the solution). Well, it is possible.
It turns out that, however, minimising MAE is more computationally expensive
and the solution may be numerically unstable.
So it’s rarely an analyst’s first choice (assuming they are well-educated
enough to know about the MAD regression task). However, it may be worth
trying it out sometimes.</p>
<p>Sometimes we might prefer MAD regression to the classic one
if our data is heavily contaminated by outliers. But
in such cases it is worth checking if proper data cleansing does
the trick.</p>
</dd>
</dl>
</div>
<div id="graphical-summaries-of-residuals" class="section level4" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Graphical Summaries of Residuals</h4>
<p>If we are not happy with single numerical aggregated of the residuals
or their absolute values, we can (and should) always compute a whole
bunch of descriptive statistics:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="multiple-regression.html#cb127-1" aria-hidden="true"></a><span class="kw">summary</span>(f12<span class="op">$</span>residuals)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -108.10   -1.94    7.81    0.00   20.25   50.62</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="multiple-regression.html#cb129-1" aria-hidden="true"></a><span class="kw">summary</span>(f1<span class="op">$</span>residuals)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -226.8   -48.3   -10.1     0.0    42.6   268.7</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="multiple-regression.html#cb131-1" aria-hidden="true"></a><span class="kw">summary</span>(f2<span class="op">$</span>residuals)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -195.16  -57.34   -1.28    0.00   64.01  175.34</code></pre>
<p>The outputs generated by <code>summary()</code> include:</p>
<ul>
<li><code>Min.</code> – sample minimum</li>
<li><code>1st Qu.</code> – 1st quartile == 25th percentile == quantile of order 0.25</li>
<li><code>Median</code> – median == 50th percentile == quantile of order 0.5</li>
<li><code>3rd Qu.</code> – 3rd quartile = 75th percentile == quantile of order 0.75</li>
<li><code>Max.</code> – sample maximum</li>
</ul>
<p>For example, 1st quartile is the observation <span class="math inline">\(q\)</span> such that
25% values are <span class="math inline">\(\le q\)</span> and 75% values are <span class="math inline">\(\ge q\)</span>,
see <code>?quantile</code> in R.</p>
<p>Graphically, it is nice to summarise the empirical distribution
of the residuals on a <strong>box and whisker plot</strong>.
Here is the key to decipher Figure <a href="multiple-regression.html#fig:boxplot-explained">2.5</a>:</p>
<ul>
<li>IQR == Interquartile range == Q3<span class="math inline">\(-\)</span>Q1 (box width)</li>
<li>The box contains 50% of the “most typical” observations</li>
<li>Box and whiskers altogether have width <span class="math inline">\(\le\)</span> 4 IQR</li>
<li>Outliers == observations potentially worth inspecting (is it a bug or a feature?)</li>
</ul>
<div class="figure"><span id="fig:boxplot-explained"></span>
<img src="02-regression-multiple-figures/boxplot-explained-1.png" alt="" />
<p class="caption">Figure 2.5:  An example boxplot</p>
</div>
<p>Figure <a href="multiple-regression.html#fig:boxplot-residuals">2.6</a> is worth a thousand words:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="multiple-regression.html#cb133-1" aria-hidden="true"></a><span class="kw">boxplot</span>(<span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;residuals&quot;</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>,</span>
<span id="cb133-2"><a href="multiple-regression.html#cb133-2" aria-hidden="true"></a>  <span class="kw">list</span>(<span class="dt">f12=</span>f12<span class="op">$</span>residuals, <span class="dt">f1=</span>f1<span class="op">$</span>residuals, <span class="dt">f2=</span>f2<span class="op">$</span>residuals))</span>
<span id="cb133-3"><a href="multiple-regression.html#cb133-3" aria-hidden="true"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:boxplot-residuals"></span>
<img src="02-regression-multiple-figures/boxplot-residuals-1.png" alt="" />
<p class="caption">Figure 2.6:  Box plots of the residuals for the three models studied</p>
</div>
<p>Figure <a href="multiple-regression.html#fig:violinplot-residuals">2.7</a> gives a <em>violin plot</em> – a blend of a box plot and a (kernel) density estimator (histogram-like):</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="multiple-regression.html#cb134-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;vioplot&quot;</span>)</span>
<span id="cb134-2"><a href="multiple-regression.html#cb134-2" aria-hidden="true"></a><span class="kw">vioplot</span>(<span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;residuals&quot;</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>,</span>
<span id="cb134-3"><a href="multiple-regression.html#cb134-3" aria-hidden="true"></a>  <span class="kw">list</span>(<span class="dt">f12=</span>f12<span class="op">$</span>residuals, <span class="dt">f1=</span>f1<span class="op">$</span>residuals, <span class="dt">f2=</span>f2<span class="op">$</span>residuals))</span>
<span id="cb134-4"><a href="multiple-regression.html#cb134-4" aria-hidden="true"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:violinplot-residuals"></span>
<img src="02-regression-multiple-figures/violinplot-residuals-1.png" alt="" />
<p class="caption">Figure 2.7:  Violin plots of the residuals for the three models studied</p>
</div>
<p>We can also take a look at the absolute values of the residuals.
Here are some descriptive statistics:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="multiple-regression.html#cb135-1" aria-hidden="true"></a><span class="kw">summary</span>(<span class="kw">abs</span>(f12<span class="op">$</span>residuals))</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.065   6.464  14.071  22.863  26.418 108.100</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="multiple-regression.html#cb137-1" aria-hidden="true"></a><span class="kw">summary</span>(<span class="kw">abs</span>(f1<span class="op">$</span>residuals))</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.506  19.664  45.072  61.489  80.124 268.738</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="multiple-regression.html#cb139-1" aria-hidden="true"></a><span class="kw">summary</span>(<span class="kw">abs</span>(f2<span class="op">$</span>residuals))</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.655  29.854  59.676  64.151  95.738 195.156</code></pre>
<p>Figure <a href="multiple-regression.html#fig:absresiduals-boxplot">2.8</a> is worth $1000:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="multiple-regression.html#cb141-1" aria-hidden="true"></a><span class="kw">boxplot</span>(<span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;abs(residuals)&quot;</span>,</span>
<span id="cb141-2"><a href="multiple-regression.html#cb141-2" aria-hidden="true"></a>  <span class="kw">list</span>(<span class="dt">f12=</span><span class="kw">abs</span>(f12<span class="op">$</span>residuals), <span class="dt">f1=</span><span class="kw">abs</span>(f1<span class="op">$</span>residuals),</span>
<span id="cb141-3"><a href="multiple-regression.html#cb141-3" aria-hidden="true"></a>       <span class="dt">f2=</span><span class="kw">abs</span>(f2<span class="op">$</span>residuals)))</span>
<span id="cb141-4"><a href="multiple-regression.html#cb141-4" aria-hidden="true"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:absresiduals-boxplot"></span>
<img src="02-regression-multiple-figures/absresiduals-boxplot-1.png" alt="" />
<p class="caption">Figure 2.8:  Box plots of the modules of the residuals for the three models studied</p>
</div>
</div>
<div id="coefficient-of-determination-r-squared" class="section level4" number="2.3.1.3">
<h4><span class="header-section-number">2.3.1.3</span> Coefficient of Determination (R-squared)</h4>
<p>If we didn’t know the range of the dependent variable
(in our case we do know that the credit rating is on the scale 0–1000),
the RMSE or MAE would be hard to interpret.</p>
<p>It turns out that there is a popular <em>normalised</em> (unit-less) measure
that is somehow easy to interpret with no domain-specific knowledge
of the modelled problem.
Namely, the (unadjusted) <strong><span class="math inline">\(R^2\)</span> score</strong> (the coefficient of determination)
is given by:</p>
<p><span class="math display">\[
R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2},
\]</span>
where <span class="math inline">\(\bar{y}\)</span> is the arithmetic mean <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n y_i\)</span>.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="multiple-regression.html#cb142-1" aria-hidden="true"></a>(r12 &lt;-<span class="st"> </span><span class="kw">summary</span>(f12)<span class="op">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.93909</code></pre>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="multiple-regression.html#cb144-1" aria-hidden="true"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>) <span class="co"># the same</span></span></code></pre></div>
<pre><code>## [1] 0.93909</code></pre>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="multiple-regression.html#cb146-1" aria-hidden="true"></a>(r1 &lt;-<span class="st"> </span><span class="kw">summary</span>(f1)<span class="op">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.63751</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="multiple-regression.html#cb148-1" aria-hidden="true"></a>(r2 &lt;-<span class="st"> </span><span class="kw">summary</span>(f2)<span class="op">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.68998</code></pre>
<p>The coefficient of determination gives the proportion of variance of the
dependent variable explained by independent variables in the model;
<span class="math inline">\(R^2(f)\simeq 1\)</span> indicates a perfect fit.
The first model is a very good one, the simple models are
“more or less okay”.</p>
<p>Unfortunately, <span class="math inline">\(R^2\)</span> tends to automatically increase as the number of independent variables
increase (recall that the more variables in the model,
the better the SSR must be).
To correct for this phenomenon, we sometimes consider the <strong>adjusted <span class="math inline">\(R^2\)</span></strong>:</p>
<p><span class="math display">\[
\bar{R}^2(f) = 1 - (1-{R}^2(f))\frac{n-1}{n-p-1}
\]</span></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="multiple-regression.html#cb150-1" aria-hidden="true"></a><span class="kw">summary</span>(f12)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.93869</code></pre>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="multiple-regression.html#cb152-1" aria-hidden="true"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(x); <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r12)<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-3</span>) <span class="co"># the same</span></span></code></pre></div>
<pre><code>## [1] 0.93869</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="multiple-regression.html#cb154-1" aria-hidden="true"></a><span class="kw">summary</span>(f1)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.63633</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="multiple-regression.html#cb156-1" aria-hidden="true"></a><span class="kw">summary</span>(f2)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.68897</code></pre>
<p>In other words, the adjusted <span class="math inline">\(R^2\)</span> penalises for more complex models.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Side note – results of some statistical tests (e.g., significance of coefficients)
are reported by calling <code>summary(f12)</code> etc. — refer to a more advanced source to obtain more information.
These, however, require the verification of some assumptions regarding the input data
and the residuals.</p>
</dd>
</dl>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="multiple-regression.html#cb158-1" aria-hidden="true"></a><span class="kw">summary</span>(f12)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -108.10   -1.94    7.81   20.25   50.62 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.73e+02   3.95e+00    43.7   &lt;2e-16 ***
## X1          1.83e-01   5.16e-03    35.4   &lt;2e-16 ***
## X2          2.20e+00   5.64e-02    39.0   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 34.2 on 307 degrees of freedom
## Multiple R-squared:  0.939,  Adjusted R-squared:  0.939 
## F-statistic: 2.37e+03 on 2 and 307 DF,  p-value: &lt;2e-16</code></pre>
</div>
<div id="residuals-vs.-fitted-plot" class="section level4" number="2.3.1.4">
<h4><span class="header-section-number">2.3.1.4</span> Residuals vs. Fitted Plot</h4>
<p>We can also create scatter plots of the residuals
(predicted <span class="math inline">\(\hat{y}_i\)</span> minus
true <span class="math inline">\(y_i\)</span>) as a function of the predicted
<span class="math inline">\(\hat{y}_i=f(\mathbf{x}_{i,\cdot})\)</span>, see Figure <a href="multiple-regression.html#fig:resid-vs-fitted">2.9</a>.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="multiple-regression.html#cb160-1" aria-hidden="true"></a>Y_pred12 &lt;-<span class="st"> </span>f12<span class="op">$</span>fitted.values <span class="co"># predict(f12, data.frame(X1, X2))</span></span>
<span id="cb160-2"><a href="multiple-regression.html#cb160-2" aria-hidden="true"></a>Y_pred1  &lt;-<span class="st"> </span>f1<span class="op">$</span>fitted.values  <span class="co"># predict(f1, data.frame(X1))</span></span>
<span id="cb160-3"><a href="multiple-regression.html#cb160-3" aria-hidden="true"></a>Y_pred2  &lt;-<span class="st"> </span>f2<span class="op">$</span>fitted.values  <span class="co"># predict(f2, data.frame(X2))</span></span>
<span id="cb160-4"><a href="multiple-regression.html#cb160-4" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb160-5"><a href="multiple-regression.html#cb160-5" aria-hidden="true"></a><span class="kw">plot</span>(Y_pred12, Y_pred12<span class="op">-</span>Y)</span>
<span id="cb160-6"><a href="multiple-regression.html#cb160-6" aria-hidden="true"></a><span class="kw">plot</span>(Y_pred1,  Y_pred1 <span class="op">-</span>Y)</span>
<span id="cb160-7"><a href="multiple-regression.html#cb160-7" aria-hidden="true"></a><span class="kw">plot</span>(Y_pred2,  Y_pred2 <span class="op">-</span>Y)</span></code></pre></div>
<div class="figure"><span id="fig:resid-vs-fitted"></span>
<img src="02-regression-multiple-figures/resid-vs-fitted-1.png" alt="" />
<p class="caption">Figure 2.9:  Residuals vs. fitted outputs for the three regression models</p>
</div>
<p>Ideally (provided that the hypothesis that the dependent variable
is indeed a linear function of the dependent variable(s) is true),
we would expect to see a point cloud that spread around <span class="math inline">\(0\)</span> in a
very much unorderly fashion.</p>
<!--
homoskedastic
-->
</div>
</div>
<div id="variable-selection" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Variable Selection</h3>
<p>Okay, up to now we’ve been considering the problem of modelling
the <code>Rating</code> variable as a function of <code>Balance</code> and/or <code>Income</code>.
However, it the <code>Credit</code> data set there are other variables
possibly worth inspecting.</p>
<p>Consider all quantitative (numeric-continuous) variables in the <code>Credit</code> data set.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="multiple-regression.html#cb161-1" aria-hidden="true"></a>C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</span>
<span id="cb161-2"><a href="multiple-regression.html#cb161-2" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb161-3"><a href="multiple-regression.html#cb161-3" aria-hidden="true"></a>      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</span>
<span id="cb161-4"><a href="multiple-regression.html#cb161-4" aria-hidden="true"></a><span class="kw">head</span>(C)</span></code></pre></div>
<pre><code>##   Rating Limit  Income Age Education Balance
## 1    283  3606  14.891  34        11     333
## 2    483  6645 106.025  82        15     903
## 3    514  7075 104.593  71        11     580
## 4    681  9504 148.924  36        11     964
## 5    357  4897  55.882  68        16     331
## 6    569  8047  80.180  77        10    1151</code></pre>
<p>Obviously there are many possible combinations of the variables
upon which regression models can be constructed
(precisely, for <span class="math inline">\(p\)</span> variables there are <span class="math inline">\(2^p\)</span> such models).
How do we choose the <em>best</em> set of inputs?</p>
<dl>
<dt>Remark.</dt>
<dd><p>We should already be suspicious at this point:
wait… <em>best</em> requires some sort of criterion, right?</p>
</dd>
</dl>
<p>First, however, let’s draw a matrix of scatter plots for
every pair of variables
– so as to get an impression of how individual variables
interact with each other, see Figure <a href="multiple-regression.html#fig:pairs">2.10</a>.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="multiple-regression.html#cb163-1" aria-hidden="true"></a><span class="kw">pairs</span>(C)</span></code></pre></div>
<div class="figure"><span id="fig:pairs"></span>
<img src="02-regression-multiple-figures/pairs-1.png" alt="" />
<p class="caption">Figure 2.10:  Scatter plot matrix for the Credit dataset</p>
</div>
<p>It seems like <code>Rating</code> depends on <code>Limit</code> almost linearly…
We have a tool to actually quantify the degree of linear dependence
between a pair of variables –
Pearson’s <span class="math inline">\(r\)</span> – the linear correlation coefficient:</p>
<p><span class="math display">\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}.
\]</span></p>
<p>It holds <span class="math inline">\(r\in[-1,1]\)</span>, where:</p>
<ul>
<li><span class="math inline">\(r=1\)</span> – positive linear dependence (<span class="math inline">\(y\)</span> increases as <span class="math inline">\(x\)</span> increases)</li>
<li><span class="math inline">\(r=-1\)</span> – negative linear dependence (<span class="math inline">\(y\)</span> decreases as <span class="math inline">\(x\)</span> increases)</li>
<li><span class="math inline">\(r\simeq 0\)</span> – uncorrelated or non-linearly dependent</li>
</ul>
<p>Figure <a href="multiple-regression.html#fig:pearson-interpret">2.11</a> gives an illustration of the above.</p>
<div class="figure"><span id="fig:pearson-interpret"></span>
<img src="02-regression-multiple-figures/pearson-interpret-1.png" alt="" />
<p class="caption">Figure 2.11:  Different datasets and the corresponding Pearson’s <span class="math inline">\(r\)</span> coefficients</p>
</div>
<p>To compute Pearson’s <span class="math inline">\(r\)</span> between all pairs of variables, we call:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="multiple-regression.html#cb164-1" aria-hidden="true"></a><span class="kw">round</span>(<span class="kw">cor</span>(C), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##           Rating  Limit Income   Age Education Balance
## Rating     1.000  0.996  0.831 0.167    -0.040   0.798
## Limit      0.996  1.000  0.834 0.164    -0.032   0.796
## Income     0.831  0.834  1.000 0.227    -0.033   0.414
## Age        0.167  0.164  0.227 1.000     0.024   0.008
## Education -0.040 -0.032 -0.033 0.024     1.000   0.001
## Balance    0.798  0.796  0.414 0.008     0.001   1.000</code></pre>
<p><code>Rating</code> and <code>Limit</code> are almost perfectly linearly correlated,
and both seem to describe the same thing.</p>
<p>For practical purposes, we’d rather model <code>Rating</code> as a function of the other variables.
For simple linear regression models, we’d choose either <code>Income</code> or <code>Balance</code>.
How about multiple regression though?</p>
<p>The best model:</p>
<ul>
<li>has high predictive power,</li>
<li>is simple.</li>
</ul>
<p>These two criteria are often mutually exclusive.</p>
<!--
Tension between prediction and description, between technology and science

"what companies want" -- more money (models that increase revenue, even slightly, at all cost)


-->
<p>Which variables should be included in the optimal model?</p>
<p>Again, the definition of the “best” object needs a <em>fitness</em> function.</p>
<p>For fitting a single model to data, we use the SSR.</p>
<p>We need a metric that takes the number of dependent variables into account.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Unfortunately, the adjusted <span class="math inline">\(R^2\)</span>, despite its interpretability,
is not really suitable for this task. It does not penalise complex models
heavily enough to be really useful.</p>
</dd>
</dl>
<p>Here we’ll be using <strong>the Akaike Information Criterion</strong> (AIC).</p>
<p>For a model <span class="math inline">\(f\)</span> with <span class="math inline">\(p&#39;\)</span> independent variables:
<span class="math display">\[
\mathrm{AIC}(f) = 2(p&#39;+1)+n\log(\mathrm{SSR}(f))-n\log n
\]</span></p>
<p>Our task is to find the combination of independent variables
that minimises the AIC.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(**) Note that this is a bi-level optimisation problem – for every
considered combination of variables (which we look for),
we must solve another problem of finding the best model
involving these variables – the one that minimises the SSR.
<span class="math display">\[
\min_{s_1,s_2,\dots,s_p\in\{0, 1\}}
\left(
\begin{array}{l}
2\left(\displaystyle\sum_{j=1}^p s_j +1\right)+\\
n\log\left(
\displaystyle\min_{\beta_0,\beta_1,\dots,\beta_p\in\mathbb{R}}
\sum_{i=1}^n \left(
\beta_0 + s_1\beta_1 x_{i,1} + \dots + s_p\beta_p x_{i,p}
-y_i
\right)^2
\right)
\end{array}
\right)
\]</span>
We dropped the <span class="math inline">\(n\log n\)</span> term, because it is always constant
and hence doesn’t affect the solution.
If <span class="math inline">\(s_j=0\)</span>, then the <span class="math inline">\(s_j\beta_j x_{i,j}\)</span> term is equal to
<span class="math inline">\(0\)</span>, and hence is not considered in the model.
This plays the role of including <span class="math inline">\(s_j=1\)</span> or omitting <span class="math inline">\(s_j=0\)</span> the <span class="math inline">\(j\)</span>-th
variable in the model building exercise.</p>
</dd>
</dl>
<p>For <span class="math inline">\(p\)</span> variables, the number of their possible
combinations is equal to <span class="math inline">\(2^p\)</span>
(grows exponentially with <span class="math inline">\(p\)</span>).
For large <span class="math inline">\(p\)</span> (think big data), an extensive search is impractical
(in our case we could get away with this though – left as an exercise
to a slightly more advanced reader).
Therefore, to find the variable combination minimising the AIC,
we often rely on one of the two following greedy heuristics:</p>
<ul>
<li><p>forward selection:</p>
<ol style="list-style-type: decimal">
<li>start with an empty model</li>
<li>find an independent variable
whose addition to the current model would yield the highest decrease in the AIC and add it to the model</li>
<li>go to step 2 until AIC decreases</li>
</ol></li>
<li><p>backward elimination:</p>
<ol style="list-style-type: decimal">
<li>start with the full model</li>
<li>find an independent variable
whose removal from the current model would decrease the AIC the most and eliminate it from the model</li>
<li>go to step 2 until AIC decreases</li>
</ol></li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p>(**) The above bi-level optimisation problem
can be solved by implementing a genetic algorithm – see further chapter for more details.</p>
</dd>
<dt>Remark.</dt>
<dd><p>(*) There are of course many other methods which also perform
some form of variable selection, e.g., lasso regression.
But these minimise a different objective.</p>
</dd>
</dl>
<p>First, a forward selection example.
We need a data sample to work with:</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="multiple-regression.html#cb166-1" aria-hidden="true"></a>C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</span>
<span id="cb166-2"><a href="multiple-regression.html#cb166-2" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb166-3"><a href="multiple-regression.html#cb166-3" aria-hidden="true"></a>      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</span></code></pre></div>
<p>Then, a formula that represents a model with no variables
(model from which we’ll start our search):</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="multiple-regression.html#cb167-1" aria-hidden="true"></a>(model_empty &lt;-<span class="st"> </span>Rating<span class="op">~</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Rating ~ 1</code></pre>
<p>Last, we need a model that includes all the variables.
We’re too lazy to list all of them manually, therefore,
we can use the <code>model.frame()</code> function to generate
a corresponding formula:</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="multiple-regression.html#cb169-1" aria-hidden="true"></a>(model_full &lt;-<span class="st"> </span><span class="kw">formula</span>(<span class="kw">model.frame</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C))) <span class="co"># all variables</span></span></code></pre></div>
<pre><code>## Rating ~ Income + Age + Education + Balance</code></pre>
<p>Now we are ready.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="multiple-regression.html#cb171-1" aria-hidden="true"></a><span class="kw">step</span>(<span class="kw">lm</span>(model_empty, <span class="dt">data=</span>C), <span class="co"># starting model</span></span>
<span id="cb171-2"><a href="multiple-regression.html#cb171-2" aria-hidden="true"></a>    <span class="dt">scope=</span>model_full,         <span class="co"># gives variables to consider</span></span>
<span id="cb171-3"><a href="multiple-regression.html#cb171-3" aria-hidden="true"></a>    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=3055.8
## Rating ~ 1
## 
##             Df Sum of Sq     RSS  AIC
## + Income     1   4058342 1823473 2695
## + Balance    1   3749707 2132108 2743
## + Age        1    164567 5717248 3049
## &lt;none&gt;                   5881815 3056
## + Education  1      9631 5872184 3057
## 
## Step:  AIC=2694.7
## Rating ~ Income
## 
##             Df Sum of Sq     RSS  AIC
## + Balance    1   1465212  358261 2192
## &lt;none&gt;                   1823473 2695
## + Age        1      2836 1820637 2696
## + Education  1      1063 1822410 2697
## 
## Step:  AIC=2192.3
## Rating ~ Income + Balance
## 
##             Df Sum of Sq    RSS  AIC
## + Age        1      4119 354141 2191
## + Education  1      2692 355568 2192
## &lt;none&gt;                   358261 2192
## 
## Step:  AIC=2190.7
## Rating ~ Income + Balance + Age
## 
##             Df Sum of Sq    RSS  AIC
## + Education  1      2926 351216 2190
## &lt;none&gt;                   354141 2191
## 
## Step:  AIC=2190.1
## Rating ~ Income + Balance + Age + Education</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Balance + Age + Education, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance          Age    Education  
##     173.830        2.167        0.184        0.223       -0.960</code></pre>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="multiple-regression.html#cb174-1" aria-hidden="true"></a><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C))</span></code></pre></div>
<pre><code>## Rating ~ Income + Age + Education + Balance</code></pre>
<p>The full model has been selected.</p>
<!-- TODO: detailed explanation what's happening here -->
<div style="margin-top: 1em">

</div>
<p>And now for something completely different –
a backward elimination example:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="multiple-regression.html#cb176-1" aria-hidden="true"></a><span class="kw">step</span>(<span class="kw">lm</span>(model_full, <span class="dt">data=</span>C), <span class="co"># from</span></span>
<span id="cb176-2"><a href="multiple-regression.html#cb176-2" aria-hidden="true"></a>     <span class="dt">scope=</span>model_empty,      <span class="co"># to</span></span>
<span id="cb176-3"><a href="multiple-regression.html#cb176-3" aria-hidden="true"></a>     <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=2190.1
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS  AIC
## &lt;none&gt;                    351216 2190
## - Education  1      2926  354141 2191
## - Age        1      4353  355568 2192
## - Balance    1   1468466 1819682 2698
## - Income     1   1617191 1968406 2722</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Age + Education + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income          Age    Education      Balance  
##     173.830        2.167        0.223       -0.960        0.184</code></pre>
<p>The full model is considered the best again.</p>
<!-- TODO: detailed explanation what's happening here -->
<div style="margin-top: 1em">

</div>
<p>Forward selection example – full dataset:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="multiple-regression.html#cb179-1" aria-hidden="true"></a>C &lt;-<span class="st"> </span>Credit[,  <span class="co"># do not restrict to Credit$Balance&gt;0</span></span>
<span id="cb179-2"><a href="multiple-regression.html#cb179-2" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb179-3"><a href="multiple-regression.html#cb179-3" aria-hidden="true"></a>      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</span>
<span id="cb179-4"><a href="multiple-regression.html#cb179-4" aria-hidden="true"></a><span class="kw">step</span>(<span class="kw">lm</span>(model_empty, <span class="dt">data=</span>C),</span>
<span id="cb179-5"><a href="multiple-regression.html#cb179-5" aria-hidden="true"></a>    <span class="dt">scope=</span>model_full,</span>
<span id="cb179-6"><a href="multiple-regression.html#cb179-6" aria-hidden="true"></a>    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=4034.3
## Rating ~ 1
## 
##             Df Sum of Sq     RSS  AIC
## + Balance    1   7124258 2427627 3488
## + Income     1   5982140 3569744 3643
## + Age        1    101661 9450224 4032
## &lt;none&gt;                   9551885 4034
## + Education  1      8675 9543210 4036
## 
## Step:  AIC=3488.4
## Rating ~ Balance
## 
##             Df Sum of Sq     RSS  AIC
## + Income     1   1859749  567878 2909
## + Age        1     98562 2329065 3474
## &lt;none&gt;                   2427627 3488
## + Education  1      5130 2422497 3490
## 
## Step:  AIC=2909.3
## Rating ~ Balance + Income
## 
##             Df Sum of Sq    RSS  AIC
## &lt;none&gt;                   567878 2909
## + Age        1      2142 565735 2910
## + Education  1      1209 566669 2910</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Balance + Income, data = C)
## 
## Coefficients:
## (Intercept)      Balance       Income  
##     145.351        0.213        2.186</code></pre>
<p>This procedure suggests including only the <code>Balance</code> and <code>Income</code>
variables.</p>
<div style="margin-top: 1em">

</div>
<p>Backward elimination example – full dataset:</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="multiple-regression.html#cb182-1" aria-hidden="true"></a><span class="kw">step</span>(<span class="kw">lm</span>(model_full, <span class="dt">data=</span>C), <span class="co"># full model</span></span>
<span id="cb182-2"><a href="multiple-regression.html#cb182-2" aria-hidden="true"></a>     <span class="dt">scope=</span>model_empty, <span class="co"># empty model</span></span>
<span id="cb182-3"><a href="multiple-regression.html#cb182-3" aria-hidden="true"></a>     <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=2910.9
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS  AIC
## - Education  1      1238  565735 2910
## - Age        1      2172  566669 2910
## &lt;none&gt;                    564497 2911
## - Income     1   1759273 2323770 3475
## - Balance    1   2992164 3556661 3645
## 
## Step:  AIC=2909.8
## Rating ~ Income + Age + Balance
## 
##           Df Sum of Sq     RSS  AIC
## - Age      1      2142  567878 2909
## &lt;none&gt;                  565735 2910
## - Income   1   1763329 2329065 3474
## - Balance  1   2991523 3557259 3643
## 
## Step:  AIC=2909.3
## Rating ~ Income + Balance
## 
##           Df Sum of Sq     RSS  AIC
## &lt;none&gt;                  567878 2909
## - Income   1   1859749 2427627 3488
## - Balance  1   3001866 3569744 3643</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance  
##     145.351        2.186        0.213</code></pre>
<p>This procedure gives the same results as forward selection
(however, for other data sets this might not necessarily be the case).</p>
</div>
<div id="variable-transformation" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Variable Transformation</h3>
<p>So far we have been fitting linear models of the form:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p.
\]</span></p>
<p>What about some non-linear models such as polynomials etc.? For example:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_1^3 + \beta_4 X_2.
\]</span></p>
<p>Solution: pre-process inputs by setting
<span class="math inline">\(X_1&#39; := X_1\)</span>, <span class="math inline">\(X_2&#39; := X_1^2\)</span>, <span class="math inline">\(X_3&#39; := X_1^3\)</span>, <span class="math inline">\(X_4&#39; := X_2\)</span>
and fit a linear model:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1&#39; + \beta_2 X_2&#39; + \beta_3 X_3&#39; + \beta_4 X_4&#39;.
\]</span></p>
<p>This trick works for every model of the form
<span class="math inline">\(Y=\sum_{i=1}^k \sum_{j=1}^p \varphi_{i,j}(X_j)\)</span> for any <span class="math inline">\(k\)</span>
and any univariate functions <span class="math inline">\(\varphi_{i,j}\)</span>.</p>
<p>Also, with a little creativity (and maths), we might be able to transform
a few other models to a linear one, e.g.,</p>
<p><span class="math display">\[
Y = b e^{aX} \qquad \to \qquad \log Y = \log b + aX \qquad\to\qquad Y&#39;=aX+b&#39;
\]</span></p>
<p>This is an example of a model’s <strong>linearisation</strong>.
However, not every model can be linearised.
In particular, one that involves functions that are not invertible.</p>
<p>For example, here’s a series of simple (<span class="math inline">\(p=1\)</span>) degree-<span class="math inline">\(d\)</span>
polynomial regression models
of the form:
<span class="math display">\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_d X^d.
\]</span></p>
<p>Such models can be fitted with the <code>lm()</code> function based on the formula
of the form <code>Y~poly(X, d, raw=TRUE)</code> or <code>Y~X+I(X^2)+I(X^3)+...</code></p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="multiple-regression.html#cb185-1" aria-hidden="true"></a>f1_<span class="dv">1</span>  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1)</span>
<span id="cb185-2"><a href="multiple-regression.html#cb185-2" aria-hidden="true"></a>f1_<span class="dv">3</span>  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span><span class="kw">I</span>(X1<span class="op">^</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">I</span>(X1<span class="op">^</span><span class="dv">3</span>)) <span class="co"># also: Y~poly(X1, 3, raw=TRUE)</span></span>
<span id="cb185-3"><a href="multiple-regression.html#cb185-3" aria-hidden="true"></a>f1_<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span><span class="kw">poly</span>(X1, <span class="dv">10</span>, <span class="dt">raw=</span><span class="ot">TRUE</span>))</span></code></pre></div>
<p>Above we have fitted the polynomials of degrees 1, 3 and 10.
Note that a polynomial of degree 1 is just a line.</p>
<p>Let us depict the three models:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="multiple-regression.html#cb186-1" aria-hidden="true"></a><span class="kw">plot</span>(X1, Y, <span class="dt">col=</span><span class="st">&quot;#000000aa&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1100</span>))</span>
<span id="cb186-2"><a href="multiple-regression.html#cb186-2" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(X1), <span class="kw">max</span>(X1), <span class="dt">length.out=</span><span class="dv">101</span>)</span>
<span id="cb186-3"><a href="multiple-regression.html#cb186-3" aria-hidden="true"></a><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">1</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb186-4"><a href="multiple-regression.html#cb186-4" aria-hidden="true"></a><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">3</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb186-5"><a href="multiple-regression.html#cb186-5" aria-hidden="true"></a><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">10</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:poly2"></span>
<img src="02-regression-multiple-figures/poly2-1.png" alt="" />
<p class="caption">Figure 2.12:  Polynomials of different degrees fitted to the Credit dataset</p>
</div>
<p>From Figure <a href="multiple-regression.html#fig:poly2">2.12</a> we see that there’s clearly a problem
with the degree-10 polynomial.</p>
</div>
<div id="predictive-vs.-descriptive-power" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Predictive vs. Descriptive Power</h3>
<p>The above high-degree polynomial model (<code>f1_10</code>) is a typical instance
of a phenomenon called an <strong>overfit</strong>.</p>
<p>Clearly (based on our expert knowledge), the <code>Rating</code> shouldn’t
decrease as <code>Balance</code> increases.</p>
<p>In other words, <code>f1_10</code> gives a better fit to data actually observed,
but fails to produce good results for the points that are yet to come.</p>
<p>We say that it <strong>generalises</strong> poorly to unseen data.</p>
<p>Assume our true model is of the form:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="multiple-regression.html#cb187-1" aria-hidden="true"></a>true_model &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">3</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span><span class="op">+</span><span class="dv">5</span></span></code></pre></div>
<p>Let’s generate the following random sample from this model (with <span class="math inline">\(Y\)</span> subject
to error), see Figure <a href="multiple-regression.html#fig:figBIASVARIANCE1">2.13</a>:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="multiple-regression.html#cb188-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>) <span class="co"># to assure reproducibility</span></span>
<span id="cb188-2"><a href="multiple-regression.html#cb188-2" aria-hidden="true"></a>n &lt;-<span class="st"> </span><span class="dv">25</span></span>
<span id="cb188-3"><a href="multiple-regression.html#cb188-3" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</span>
<span id="cb188-4"><a href="multiple-regression.html#cb188-4" aria-hidden="true"></a>Y &lt;-<span class="st"> </span><span class="kw">true_model</span>(X)<span class="op">+</span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span><span class="fl">0.2</span>) <span class="co"># add normally-distributed noise</span></span></code></pre></div>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="multiple-regression.html#cb189-1" aria-hidden="true"></a><span class="kw">plot</span>(X, Y)</span>
<span id="cb189-2"><a href="multiple-regression.html#cb189-2" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">101</span>)</span>
<span id="cb189-3"><a href="multiple-regression.html#cb189-3" aria-hidden="true"></a><span class="kw">lines</span>(x, <span class="kw">true_model</span>(x), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figBIASVARIANCE1"></span>
<img src="02-regression-multiple-figures/figBIASVARIANCE1-1.png" alt="" />
<p class="caption">Figure 2.13:  Synthetic data generated by means of the formula <span class="math inline">\(Y=3x^3+5\)</span> (<span class="math inline">\(+\)</span> noise)</p>
</div>
<p>Let’s fit polynomials of different degrees, see Figure <a href="multiple-regression.html#fig:figBIASVARIANCE2">2.14</a>.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="multiple-regression.html#cb190-1" aria-hidden="true"></a><span class="kw">plot</span>(X, Y)</span>
<span id="cb190-2"><a href="multiple-regression.html#cb190-2" aria-hidden="true"></a><span class="kw">lines</span>(x, <span class="kw">true_model</span>(x), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb190-3"><a href="multiple-regression.html#cb190-3" aria-hidden="true"></a></span>
<span id="cb190-4"><a href="multiple-regression.html#cb190-4" aria-hidden="true"></a>dmax &lt;-<span class="st"> </span><span class="dv">11</span> <span class="co"># maximal polynomial degree</span></span>
<span id="cb190-5"><a href="multiple-regression.html#cb190-5" aria-hidden="true"></a>MSE_train &lt;-<span class="st"> </span><span class="kw">numeric</span>(dmax)</span>
<span id="cb190-6"><a href="multiple-regression.html#cb190-6" aria-hidden="true"></a>MSE_test  &lt;-<span class="st"> </span><span class="kw">numeric</span>(dmax)</span>
<span id="cb190-7"><a href="multiple-regression.html#cb190-7" aria-hidden="true"></a><span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>dmax) { <span class="co"># for every polynomial degree</span></span>
<span id="cb190-8"><a href="multiple-regression.html#cb190-8" aria-hidden="true"></a>    f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span><span class="kw">poly</span>(X, d, <span class="dt">raw=</span><span class="ot">TRUE</span>)) <span class="co"># fit a d-degree polynomial</span></span>
<span id="cb190-9"><a href="multiple-regression.html#cb190-9" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">predict</span>(f, <span class="kw">data.frame</span>(<span class="dt">X=</span>x))</span>
<span id="cb190-10"><a href="multiple-regression.html#cb190-10" aria-hidden="true"></a>    <span class="kw">lines</span>(x, y, <span class="dt">col=</span>d)</span>
<span id="cb190-11"><a href="multiple-regression.html#cb190-11" aria-hidden="true"></a>    <span class="co"># MSE on given random X,Y:</span></span>
<span id="cb190-12"><a href="multiple-regression.html#cb190-12" aria-hidden="true"></a>    MSE_train[d] &lt;-<span class="st"> </span><span class="kw">mean</span>(f<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb190-13"><a href="multiple-regression.html#cb190-13" aria-hidden="true"></a>    <span class="co"># MSE on many more points:</span></span>
<span id="cb190-14"><a href="multiple-regression.html#cb190-14" aria-hidden="true"></a>    MSE_test[d]  &lt;-<span class="st"> </span><span class="kw">mean</span>((y<span class="op">-</span><span class="kw">true_model</span>(x))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb190-15"><a href="multiple-regression.html#cb190-15" aria-hidden="true"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:figBIASVARIANCE2"></span>
<img src="02-regression-multiple-figures/figBIASVARIANCE2-1.png" alt="" />
<p class="caption">Figure 2.14:  Polynomials fitted to our synthetic dataset</p>
</div>
<p>Some of the polynomials are fitted too well!</p>
<dl>
<dt>Remark</dt>
<dd><p>(*) The oscillation of the high-degree polynomials at the domain
boundaries is known as the Runge phenomenon.</p>
</dd>
</dl>
<p>Compare the mean squared error (MSE) for the observed vs. future data points,
see Figure <a href="multiple-regression.html#fig:figBIASVARIANCE3">2.15</a>.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="multiple-regression.html#cb191-1" aria-hidden="true"></a><span class="kw">matplot</span>(<span class="dv">1</span><span class="op">:</span>dmax, <span class="kw">cbind</span>(MSE_train, MSE_test), <span class="dt">type=</span><span class="st">&quot;b&quot;</span>,</span>
<span id="cb191-2"><a href="multiple-regression.html#cb191-2" aria-hidden="true"></a>    <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">1e-3</span>, <span class="fl">2e3</span>), <span class="dt">log=</span><span class="st">&quot;y&quot;</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,</span>
<span id="cb191-3"><a href="multiple-regression.html#cb191-3" aria-hidden="true"></a>    <span class="dt">xlab=</span><span class="st">&quot;Model complexity (polynomial degree)&quot;</span>,</span>
<span id="cb191-4"><a href="multiple-regression.html#cb191-4" aria-hidden="true"></a>    <span class="dt">ylab=</span><span class="st">&quot;MSE&quot;</span>)</span>
<span id="cb191-5"><a href="multiple-regression.html#cb191-5" aria-hidden="true"></a><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;MSE on original data&quot;</span>, <span class="st">&quot;MSE on the whole range&quot;</span>),</span>
<span id="cb191-6"><a href="multiple-regression.html#cb191-6" aria-hidden="true"></a>    <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:figBIASVARIANCE3"></span>
<img src="02-regression-multiple-figures/figBIASVARIANCE3-1.png" alt="" />
<p class="caption">Figure 2.15:  MSE on the dataset used to construct the model vs. MSE on a whole range of points as function of the polynomial degree</p>
</div>
<p>Note the logarithmic scale on the <span class="math inline">\(y\)</span> axis.</p>
<p>This is a very typical behaviour!</p>
<ul>
<li><p>A model’s fit to observed data improves as the model’s complexity increases.</p></li>
<li><p>A model’s generalisation to unseen data initially improves, but then becomes worse.</p></li>
<li><p>In the above example, the sweet spot is at a polynomial of degree 3, which is exactly
our true underlying model.</p></li>
</ul>
<p>Hence, most often <strong>we should be interested in the accuracy of the predictions
made in the case of unobserved data.</strong></p>
<p>If we have a data set of a considerable size,
we can divide it (randomly) into two parts:</p>
<ul>
<li><em>training sample</em> (say, 60% or 80%) – used to fit a model</li>
<li><em>test sample</em> (the remaining 40% or 20%) – used to assess its quality
(e.g., using MSE)</li>
</ul>
<p>More on this issue in the chapter on Classification.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) We shall see that sometimes a train-test-validate
split will be necessary, e.g., 60-20-20%.</p>
</dd>
</dl>
</div>
</div>
<div id="exercises-in-r-1" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Exercises in R</h2>
<!--

TODO

wines dataset: alcohol~density ????

example of unsuccessful modelling

-->
<div id="anscombes-quartet-revisited" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Anscombe’s Quartet Revisited</h3>
<p>Consider the <code>anscombe</code> database once again:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="multiple-regression.html#cb192-1" aria-hidden="true"></a><span class="kw">print</span>(anscombe) <span class="co"># `anscombe` is a built-in object</span></span></code></pre></div>
<pre><code>##    x1 x2 x3 x4    y1   y2    y3    y4
## 1  10 10 10  8  8.04 9.14  7.46  6.58
## 2   8  8  8  8  6.95 8.14  6.77  5.76
## 3  13 13 13  8  7.58 8.74 12.74  7.71
## 4   9  9  9  8  8.81 8.77  7.11  8.84
## 5  11 11 11  8  8.33 9.26  7.81  8.47
## 6  14 14 14  8  9.96 8.10  8.84  7.04
## 7   6  6  6  8  7.24 6.13  6.08  5.25
## 8   4  4  4 19  4.26 3.10  5.39 12.50
## 9  12 12 12  8 10.84 9.13  8.15  5.56
## 10  7  7  7  8  4.82 7.26  6.42  7.91
## 11  5  5  5  8  5.68 4.74  5.73  6.89</code></pre>
<p>Recall that in the previous Chapter we have
split the above data into four data frames
<code>ans1</code>, …, <code>ans4</code> with columns <code>x</code> and <code>y</code>.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>In <code>ans1</code>, fit a regression line to the data set as-is.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>We’ve done that already, see Figure <a href="multiple-regression.html#fig:anscombe3">2.16</a>.
What a wonderful exercise, thank you – effective
learning is often done by repeating stuff.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="multiple-regression.html#cb194-1" aria-hidden="true"></a>ans1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x1, <span class="dt">y=</span>anscombe<span class="op">$</span>y1)</span>
<span id="cb194-2"><a href="multiple-regression.html#cb194-2" aria-hidden="true"></a>f1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data=</span>ans1)</span>
<span id="cb194-3"><a href="multiple-regression.html#cb194-3" aria-hidden="true"></a><span class="kw">plot</span>(ans1<span class="op">$</span>x, ans1<span class="op">$</span>y)</span>
<span id="cb194-4"><a href="multiple-regression.html#cb194-4" aria-hidden="true"></a><span class="kw">abline</span>(f1, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:anscombe3"></span>
<img src="02-regression-multiple-figures/anscombe3-1.png" alt="" />
<p class="caption">Figure 2.16:  Fitted regression line for <code>ans1</code></p>
</div>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>In <code>ans2</code>, fit a quadratic model (<span class="math inline">\(y=a + bx + cx^2\)</span>).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>How to fit a polynomial model is explained above.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="multiple-regression.html#cb195-1" aria-hidden="true"></a>ans2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x2, <span class="dt">y=</span>anscombe<span class="op">$</span>y2)</span>
<span id="cb195-2"><a href="multiple-regression.html#cb195-2" aria-hidden="true"></a>f2 &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x<span class="op">+</span><span class="kw">I</span>(x<span class="op">^</span><span class="dv">2</span>), <span class="dt">data=</span>ans2)</span>
<span id="cb195-3"><a href="multiple-regression.html#cb195-3" aria-hidden="true"></a><span class="kw">plot</span>(ans2<span class="op">$</span>x, ans2<span class="op">$</span>y)</span>
<span id="cb195-4"><a href="multiple-regression.html#cb195-4" aria-hidden="true"></a>x_plot &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">4</span>, <span class="dv">14</span>, <span class="dt">by=</span><span class="fl">0.1</span>)</span>
<span id="cb195-5"><a href="multiple-regression.html#cb195-5" aria-hidden="true"></a>y_plot &lt;-<span class="st"> </span><span class="kw">predict</span>(f2, <span class="kw">data.frame</span>(<span class="dt">x=</span>x_plot))</span>
<span id="cb195-6"><a href="multiple-regression.html#cb195-6" aria-hidden="true"></a><span class="kw">lines</span>(x_plot, y_plot, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:anscombe4"></span>
<img src="02-regression-multiple-figures/anscombe4-1.png" alt="" />
<p class="caption">Figure 2.17:  Fitted quadratic model for <code>ans2</code></p>
</div>
<p><em>Comment: From Figure <a href="multiple-regression.html#fig:anscombe4">2.17</a> we see that it’s
an almost-perfect fit! Clearly,
the second Anscombe dataset isn’t a case of linearly
dependent variables.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>In <code>ans3</code>, remove the obvious outlier from data
and fit a regression line.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Let’s plot the data set first, see Figure <a href="multiple-regression.html#fig:anscombe5">2.18</a>.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="multiple-regression.html#cb196-1" aria-hidden="true"></a>ans3 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x3, <span class="dt">y=</span>anscombe<span class="op">$</span>y3)</span>
<span id="cb196-2"><a href="multiple-regression.html#cb196-2" aria-hidden="true"></a><span class="kw">plot</span>(ans3<span class="op">$</span>x, ans3<span class="op">$</span>y)</span></code></pre></div>
<div class="figure"><span id="fig:anscombe5"></span>
<img src="02-regression-multiple-figures/anscombe5-1.png" alt="" />
<p class="caption">Figure 2.18:  Scatter plot for <code>ans3</code></p>
</div>
<p>Indeed, the observation at <span class="math inline">\(x\simeq 13\)</span> is an obvious outlier.
Perhaps the easiest way
to remove it is to call:</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="multiple-regression.html#cb197-1" aria-hidden="true"></a>ans3b &lt;-<span class="st"> </span>ans3[ans3<span class="op">$</span>y<span class="op">&lt;=</span><span class="dv">12</span>,] <span class="co"># the outlier is definitely at y&gt;12</span></span></code></pre></div>
<p>We could also use the condition <code>y &lt; max(y)</code>, amongst others.</p>
<p>Now let’s fit the linear model:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="multiple-regression.html#cb198-1" aria-hidden="true"></a>f3b &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data=</span>ans3b)</span>
<span id="cb198-2"><a href="multiple-regression.html#cb198-2" aria-hidden="true"></a><span class="kw">plot</span>(ans3b<span class="op">$</span>x, ans3b<span class="op">$</span>y)</span>
<span id="cb198-3"><a href="multiple-regression.html#cb198-3" aria-hidden="true"></a><span class="kw">abline</span>(f3b, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:anscombe7"></span>
<img src="02-regression-multiple-figures/anscombe7-1.png" alt="" />
<p class="caption">Figure 2.19:  Scatter plot for <code>ans3</code> with the outlier removed and the fitted linear model</p>
</div>
<p><em>Comment: Now Figure <a href="multiple-regression.html#fig:anscombe7">2.19</a> is what we call linearly correlated data.
By the way, Pearson’s coefficient now equals <code>1</code>.</em></p>
</details>
</div>
<div id="countries-of-the-world-simple-models-involving-the-gdp-per-capita" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Countries of the World – Simple models involving the GDP per capita</h3>
<p>Let’s consider the World Factbook 2020 dataset
(see this book’s <code>datasets</code> folder).
It consists of country names, their population,
area, GDP, mortality rates etc. We have scraped it from the CIA website
at <a href="https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html" class="uri">https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html</a>
and compiled into a single file on 3 April 2020.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="multiple-regression.html#cb199-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</span>
<span id="cb199-2"><a href="multiple-regression.html#cb199-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<p>Here is a preview of a few features for 3 selected countries (see <code>help("%in%")</code>):</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="multiple-regression.html#cb200-1" aria-hidden="true"></a>factbook[factbook<span class="op">$</span>country <span class="op">%in%</span></span>
<span id="cb200-2"><a href="multiple-regression.html#cb200-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;Australia&quot;</span>, <span class="st">&quot;New Zealand&quot;</span>, <span class="st">&quot;United States&quot;</span>),</span>
<span id="cb200-3"><a href="multiple-regression.html#cb200-3" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="st">&quot;country&quot;</span>, <span class="st">&quot;area&quot;</span>, <span class="st">&quot;population&quot;</span>, <span class="st">&quot;gdp_per_capita_ppp&quot;</span>)]</span></code></pre></div>
<pre><code>##           country    area population gdp_per_capita_ppp
## 15      Australia 7741220   25466459              50400
## 169   New Zealand  268838    4925477              39000
## 247 United States 9833517  332639102              59800</code></pre>
<!--
The dataset consists of the following columns:
"country", "area", "population", "median_age", "population_growth_rate", "birth_rate", "death_rate", "net_migration_rate", "maternal_mortality_rate", "infant_mortality_rate", "life_expectancy_at_birth", "total_fertility_rate", "hiv_aids_adult_prevalence_rate", "hiv_aids_people_living_with", "hiv_aids_deaths", "obesity_adult_prevalence_rate", "children_under_age_5_underweight", "education_expenditures", "unemployment_youth_ages_15_to_24", "gdp_purchasing_power_parity", "gdp_real_growth_rate", "gdp_per_capita_ppp", "gross_national_saving", "industrial_production_growth_rate", "labor_force", "unemployment_rate", "taxes_and_other_revenues", "budget_surplus_or_deficit", "public_debt", "inflation_rate_consumer_prices", "current_account_balance", "exports", "imports", "reserves_of_foreign_exchange_and_gold", "debt_external", "electricity_production", "electricity_consumption", "electricity_exports", "electricity_imports", "electricity_installed_generating_capacity", "electricity_from_fossil_fuels", "electricity_from_nuclear_fuels", "electricity_from_hydroelectric_plants", "electricity_from_other_renewable_sources", "crude_oil_production", "crude_oil_exports", "crude_oil_imports", "crude_oil_proved_reserves", "refined_petroleum_products_production", "refined_petroleum_products_consumption", "refined_petroleum_products_exports", "refined_petroleum_products_imports", "natural_gas_production", "natural_gas_consumption", "natural_gas_exports", "natural_gas_imports", "natural_gas_proved_reserves", "carbon_dioxide_emissions_from_consumption_of_energy", "telephones_fixed_lines", "telephones_mobile_cellular", "internet_users", "broadband_fixed_subscriptions", "military_expenditures", "airports", "railways", "roadways", "waterways", "merchant_marine"
-->
<!--
> Please note that some of the columns contain characters
that cannot be used in R's variable names (e.g., spaces, brackets, plus, minus).
Therefore, accessing them using the `$` operator requires, e.g., the use of
double quotes.
We should use `factbook$gdp_per_capita_ppp`
or `factbook[,gdp_per_capita_ppp]` instead.
-->
<!--

> Convert `area` and `pop_density` from square miles to square kilometres.




We can easily find an appropriate conversion formula on the internet.
We will convert one column at a time using the well-known
vectorised vector division and replace the old column with new data.


```r
#countries$area <- countries$area*0.3861
#countries$pop_density <- countries$pop_density*0.3861
#countries[countries$country == "Australia", ] # preview
```

By the way, did you know that only few countries are still reluctant
to switch to the commonly-agreed-upon
[metric system](https://en.wikipedia.org/wiki/Metric_system)?
We are still (patiently...) waiting for the (inevitable)
[metrication](https://en.wikipedia.org/wiki/Metrication)
of North Korea, the UK, the US, Liberia and Myanmar.



-->
<!--
 Omitting the aforementioned  columns can be done in a few ways. Here is one:


```r
#wines <- wines[, is.na(match(names(wines), c("color", "response")))]
```

To recall, `names(data.frame)` gives the vector of column names.
On the other hand, the `match()` function, matches all the values
in the first argument against all the values in the second argument.
If there is no match (in our case, if a column name is not amongst
the two names we wish to remove), `NA` is generated.

-->
<div class="exercise"><strong>Exercise.</strong>
<p>List the 10 countries with the highest GDP per capita.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To recall, to generate a list of indexes that produce an ordered
version of a numeric vector, we need to call the <code>order()</code> function.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="multiple-regression.html#cb202-1" aria-hidden="true"></a>which_top &lt;-<span class="st"> </span><span class="kw">tail</span>(<span class="kw">order</span>(factbook<span class="op">$</span>gdp_per_capita_ppp, <span class="dt">na.last=</span><span class="ot">FALSE</span>), <span class="dv">10</span>)</span>
<span id="cb202-2"><a href="multiple-regression.html#cb202-2" aria-hidden="true"></a>factbook[which_top, <span class="kw">c</span>(<span class="st">&quot;country&quot;</span>, <span class="st">&quot;gdp_per_capita_ppp&quot;</span>)]</span></code></pre></div>
<pre><code>##           country gdp_per_capita_ppp
## 113       Ireland              73200
## 35         Brunei              78900
## 114   Isle of Man              84600
## 211     Singapore              94100
## 26        Bermuda              99400
## 141    Luxembourg             105100
## 157        Monaco             115700
## 142         Macau             122000
## 192         Qatar             124100
## 139 Liechtenstein             139100</code></pre>
<p>By the way, the reported values are in USD.</p>
<p><em>Question: Which of these countries are tax havens?</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Find the 5 most positively and the 5 most negatively
correlated variables with the <code>gdp_per_capita_ppp</code> feature
(of course, with respect to the Pearson coefficient).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>This can be solved via a call to <code>cor()</code>.
Note that we need to make sure that missing vales are omitted
from computations.
A quick glimpse at the manual page
(<code>?cor</code>) reveals that computing the correlation between a column
and all the other ones (of course, except <code>country</code>, which
is non-numeric) can be performed as follows.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="multiple-regression.html#cb204-1" aria-hidden="true"></a>r &lt;-<span class="st"> </span><span class="kw">cor</span>(factbook<span class="op">$</span>gdp_per_capita_ppp,</span>
<span id="cb204-2"><a href="multiple-regression.html#cb204-2" aria-hidden="true"></a>    factbook[,<span class="op">!</span>(<span class="kw">names</span>(factbook) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;country&quot;</span>, <span class="st">&quot;gdp_per_capita_ppp&quot;</span>))],</span>
<span id="cb204-3"><a href="multiple-regression.html#cb204-3" aria-hidden="true"></a>    <span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)[<span class="dv">1</span>,]</span>
<span id="cb204-4"><a href="multiple-regression.html#cb204-4" aria-hidden="true"></a>or &lt;-<span class="st"> </span><span class="kw">order</span>(r) <span class="co"># ordering permutation (indexes)</span></span>
<span id="cb204-5"><a href="multiple-regression.html#cb204-5" aria-hidden="true"></a>r[<span class="kw">head</span>(or, <span class="dv">5</span>)] <span class="co"># first 5 ordered indexes</span></span></code></pre></div>
<pre><code>##   infant_mortality_rate maternal_mortality_rate              birth_rate 
##                -0.74658                -0.67005                -0.60822 
##              death_rate    total_fertility_rate 
##                -0.57216                -0.56725</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="multiple-regression.html#cb206-1" aria-hidden="true"></a>r[<span class="kw">tail</span>(or, <span class="dv">5</span>)] <span class="co"># last 5 ordered indexes</span></span></code></pre></div>
<pre><code>##        natural_gas_production         gross_national_saving 
##                       0.56898                       0.61133 
##                    median_age obesity_adult_prevalence_rate 
##                       0.62090                       0.63681 
##      life_expectancy_at_birth 
##                       0.75461</code></pre>
<p><em>Comment: “Live long and prosper” just gained a new meaning.
Richer countries have lower infant and maternal mortality rates,
lower birth rates, but higher life expectancy and obesity prevalence.
Note, however, that correlation is not causation:
we are unlikely to increase the GDP by asking people to put on weight.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit simple regression models where the per capita GDP explains
its four most correlated variables (four individual models).
Draw them on a scatter plot. Compute the root mean squared errors (RMSE),
mean absolute errors (MAE) and the coefficients of determination (<span class="math inline">\(R^2\)</span>).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>The four most correlated variables (we should look at the absolute
value of the correlation coefficient now – recall that it
is the correlation of 0 that means no linear dependence; 1 and -1
show a strong association between a pair of variables) are:</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="multiple-regression.html#cb208-1" aria-hidden="true"></a>(most_correlated &lt;-<span class="st"> </span><span class="kw">names</span>(r)[<span class="kw">tail</span>(<span class="kw">order</span>(<span class="kw">abs</span>(r)), <span class="dv">4</span>)])</span></code></pre></div>
<pre><code>## [1] &quot;obesity_adult_prevalence_rate&quot; &quot;maternal_mortality_rate&quot;      
## [3] &quot;infant_mortality_rate&quot;         &quot;life_expectancy_at_birth&quot;</code></pre>
<p>We could take the above column names and construct four
formulas manually, e.g., by writing
<code>gdp_per_capita_ppp~life_expectancy_at_birth</code>,
but we are lazy. Being lazy when it comes to computer
programming is often a virtue, not a flaw in one’s character.</p>
<p>Instead, we will run a <code>for</code> loop that extracts the pairs of
interesting columns and constructs a formula based on two vectors
(<code>lm(Y~X)</code>), see Figure <a href="multiple-regression.html#fig:factbookA6">2.20</a>.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="multiple-regression.html#cb210-1" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="co"># 4 plots on a 2x2 grid</span></span>
<span id="cb210-2"><a href="multiple-regression.html#cb210-2" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>) {</span>
<span id="cb210-3"><a href="multiple-regression.html#cb210-3" aria-hidden="true"></a>    <span class="kw">print</span>(most_correlated[i])</span>
<span id="cb210-4"><a href="multiple-regression.html#cb210-4" aria-hidden="true"></a>    X &lt;-<span class="st"> </span>factbook[,<span class="st">&quot;gdp_per_capita_ppp&quot;</span>]</span>
<span id="cb210-5"><a href="multiple-regression.html#cb210-5" aria-hidden="true"></a>    Y &lt;-<span class="st"> </span>factbook[,most_correlated[i]]</span>
<span id="cb210-6"><a href="multiple-regression.html#cb210-6" aria-hidden="true"></a>    f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X)</span>
<span id="cb210-7"><a href="multiple-regression.html#cb210-7" aria-hidden="true"></a>    <span class="kw">print</span>(<span class="kw">cbind</span>(<span class="dt">RMSE=</span><span class="kw">sqrt</span>(<span class="kw">mean</span>(f<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)),</span>
<span id="cb210-8"><a href="multiple-regression.html#cb210-8" aria-hidden="true"></a>                <span class="dt">MAE=</span><span class="kw">mean</span>(<span class="kw">abs</span>(f<span class="op">$</span>residuals)),</span>
<span id="cb210-9"><a href="multiple-regression.html#cb210-9" aria-hidden="true"></a>                <span class="dt">R2=</span><span class="kw">summary</span>(f)<span class="op">$</span>r.squared))</span>
<span id="cb210-10"><a href="multiple-regression.html#cb210-10" aria-hidden="true"></a>    <span class="kw">plot</span>(X, Y, <span class="dt">xlab=</span><span class="st">&quot;gdp_per_capita_ppp&quot;</span>,</span>
<span id="cb210-11"><a href="multiple-regression.html#cb210-11" aria-hidden="true"></a>               <span class="dt">ylab=</span>most_correlated[i])</span>
<span id="cb210-12"><a href="multiple-regression.html#cb210-12" aria-hidden="true"></a>    <span class="kw">abline</span>(f, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb210-13"><a href="multiple-regression.html#cb210-13" aria-hidden="true"></a>}</span></code></pre></div>
<pre><code>## [1] &quot;obesity_adult_prevalence_rate&quot;
##        RMSE    MAE       R2
## [1,] 11.041 8.1589 0.062196</code></pre>
<pre><code>## [1] &quot;maternal_mortality_rate&quot;
##        RMSE    MAE      R2
## [1,] 204.93 146.53 0.21481</code></pre>
<pre><code>## [1] &quot;infant_mortality_rate&quot;
##        RMSE    MAE     R2
## [1,] 15.746 12.166 0.3005</code></pre>
<pre><code>## [1] &quot;life_expectancy_at_birth&quot;
##        RMSE    MAE      R2
## [1,] 5.4292 4.3727 0.43096</code></pre>
<div class="figure"><span id="fig:factbookA6"></span>
<img src="02-regression-multiple-figures/factbookA6-1.png" alt="" />
<p class="caption">Figure 2.20:  A scatter plot matrix and regression lines for the 4 variables most correlated with the per capita GDP</p>
</div>
<p>Recall that the root mean squared error is the square root of
the arithmetic mean of the squared residuals.
Mean absolute error is the average of the absolute values of the residuals.
The coefficient of determination
is given by: <span class="math inline">\(R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2}\)</span>.</p>
<p><em>Comment: Unfortunately, we were misled by the high correlation coefficients
between the <span class="math inline">\(X\)</span>s and <span class="math inline">\(Y\)</span>s:
the low actual <span class="math inline">\(R^2\)</span> scores indicate that these models should not
be deemed trustworthy. Note that 3 of the plots are evidently L-shaped.</em></p>
<p><em>Fun fact: (*) Interestingly, it can be shown that <span class="math inline">\(R^2\)</span>
(in the case of the linear models fitted by minimising
the SSR) is the square of the correlation
between the true <span class="math inline">\(Y\)</span>s and the predicted <span class="math inline">\(Y\)</span>s:</em></p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="multiple-regression.html#cb215-1" aria-hidden="true"></a>X &lt;-<span class="st"> </span>factbook[,<span class="st">&quot;gdp_per_capita_ppp&quot;</span>]</span>
<span id="cb215-2"><a href="multiple-regression.html#cb215-2" aria-hidden="true"></a>Y &lt;-<span class="st"> </span>factbook[,most_correlated[i]]</span>
<span id="cb215-3"><a href="multiple-regression.html#cb215-3" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X, <span class="dt">y=</span><span class="ot">TRUE</span>)</span>
<span id="cb215-4"><a href="multiple-regression.html#cb215-4" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">summary</span>(f)<span class="op">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.43096</code></pre>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="multiple-regression.html#cb217-1" aria-hidden="true"></a><span class="kw">print</span>(<span class="kw">cor</span>(f<span class="op">$</span>fitted.values, f<span class="op">$</span>y)<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.43096</code></pre>
<p><em>Side note: Do note that RMSE and MAE are interpretable: for instance,
average error of life expectancy prediction based on the GDP is
4-5 years. Recall that you can find the information on the variables’ units
of measure at
<a href="https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html" class="uri">https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html</a>.</em></p>
</details>
</div>
<div id="countries-of-the-world-most-correlated-variables" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Countries of the World – Most correlated variables (*)</h3>
<p>Let’s get back to the World Factbook 2020 dataset (<code>world_factbook_2020.csv</code>).</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="multiple-regression.html#cb219-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</span>
<span id="cb219-2"><a href="multiple-regression.html#cb219-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Create a data frame <code>C</code> with three columns named <code>col1</code>, <code>col2</code>
and <code>r</code> and <span class="math inline">\(p(p-1)/2\)</span> rows,
where <span class="math inline">\(p\)</span> is the number of numeric features in <code>factbook</code>.
Every row should represent a unique pair of column names in <code>factbook</code>
(we do not distinguish between <code>a,b</code> and <code>b,a</code>)
of correlation coefficients between them.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>First we will solve this exercise considering only
4 numeric features in our dataset, so that we can keep
track of how the R expressions we evaluate actually work.</p>
<p>Let us compute the Pearson coefficients between chosen pairs of variables.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="multiple-regression.html#cb220-1" aria-hidden="true"></a>R &lt;-<span class="st"> </span><span class="kw">cor</span>(factbook[,<span class="kw">c</span>(<span class="st">&quot;area&quot;</span>, <span class="st">&quot;median_age&quot;</span>, <span class="st">&quot;birth_rate&quot;</span>, <span class="st">&quot;exports&quot;</span>)],</span>
<span id="cb220-2"><a href="multiple-regression.html#cb220-2" aria-hidden="true"></a>    <span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>) <span class="co"># 4 selected columns</span></span>
<span id="cb220-3"><a href="multiple-regression.html#cb220-3" aria-hidden="true"></a><span class="kw">print</span>(R)</span></code></pre></div>
<pre><code>##                 area median_age birth_rate  exports
## area        1.000000   0.044524  -0.031995  0.49259
## median_age  0.044524   1.000000  -0.921592  0.29973
## birth_rate -0.031995  -0.921592   1.000000 -0.24296
## exports     0.492586   0.299727  -0.242955  1.00000</code></pre>
<p>Note that the <code>R</code> matrix has <code>1.0</code> on the diagonal (where each entry
represents a correlation between a variable and itself).
Moreover, it is symmetric around the diagonal – <code>R[i,j] == R[j,i]</code>,
because it is the correlation between the same pair of variables.
Hence, from now on we may be interested in the elements
below the diagonal. We can get access to them by using <code>lower.tri()</code>
(“lower triangle”).</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="multiple-regression.html#cb222-1" aria-hidden="true"></a>R[<span class="kw">lower.tri</span>(R)]</span></code></pre></div>
<pre><code>## [1]  0.044524 -0.031995  0.492586 -0.921592  0.299727 -0.242955</code></pre>
<p>This is already the 3rd column of the data frame we are asked to generate,
which should look like:</p>
<pre><code>##         col1       col2         r
## 1 median_age       area  0.044524
## 2 birth_rate       area -0.031995
## 3    exports       area  0.492586
## 4 birth_rate median_age -0.921592
## 5    exports median_age  0.299727
## 6    exports birth_rate -0.242955</code></pre>
<p>How the generate <code>col1</code> and <code>col2</code>?
One idea is to take the “lower triangles” of the following matrices:</p>
<pre><code>##      [,1]         [,2]         [,3]         [,4]        
## [1,] &quot;area&quot;       &quot;area&quot;       &quot;area&quot;       &quot;area&quot;      
## [2,] &quot;median_age&quot; &quot;median_age&quot; &quot;median_age&quot; &quot;median_age&quot;
## [3,] &quot;birth_rate&quot; &quot;birth_rate&quot; &quot;birth_rate&quot; &quot;birth_rate&quot;
## [4,] &quot;exports&quot;    &quot;exports&quot;    &quot;exports&quot;    &quot;exports&quot;</code></pre>
<p>and:</p>
<pre><code>##      [,1]   [,2]         [,3]         [,4]     
## [1,] &quot;area&quot; &quot;median_age&quot; &quot;birth_rate&quot; &quot;exports&quot;
## [2,] &quot;area&quot; &quot;median_age&quot; &quot;birth_rate&quot; &quot;exports&quot;
## [3,] &quot;area&quot; &quot;median_age&quot; &quot;birth_rate&quot; &quot;exports&quot;
## [4,] &quot;area&quot; &quot;median_age&quot; &quot;birth_rate&quot; &quot;exports&quot;</code></pre>
<p>Here is a complete solution for all the features is <code>factbook</code>:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="multiple-regression.html#cb227-1" aria-hidden="true"></a>R &lt;-<span class="st"> </span><span class="kw">cor</span>(factbook[,<span class="op">-</span><span class="dv">1</span>], <span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>) <span class="co"># skip the `country` column</span></span>
<span id="cb227-2"><a href="multiple-regression.html#cb227-2" aria-hidden="true"></a>rrr &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">dimnames</span>(R)[[<span class="dv">1</span>]], <span class="dt">nrow=</span><span class="kw">nrow</span>(R), <span class="dt">ncol=</span><span class="kw">ncol</span>(R))</span>
<span id="cb227-3"><a href="multiple-regression.html#cb227-3" aria-hidden="true"></a>ccc &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">dimnames</span>(R)[[<span class="dv">2</span>]], <span class="dt">byrow=</span><span class="ot">TRUE</span>, <span class="dt">nrow=</span><span class="kw">nrow</span>(R), <span class="dt">ncol=</span><span class="kw">ncol</span>(R))</span>
<span id="cb227-4"><a href="multiple-regression.html#cb227-4" aria-hidden="true"></a>C &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">col1=</span>rrr[<span class="kw">lower.tri</span>(rrr)],</span>
<span id="cb227-5"><a href="multiple-regression.html#cb227-5" aria-hidden="true"></a>                <span class="dt">col2=</span>ccc[<span class="kw">lower.tri</span>(ccc)],</span>
<span id="cb227-6"><a href="multiple-regression.html#cb227-6" aria-hidden="true"></a>                <span class="dt">r=</span>R[<span class="kw">lower.tri</span>(R)])</span></code></pre></div>
<p><em>Comment: In “classical” programming languages we would perhaps
have used of a double (nested) <code>for</code> loop here (a less readable solution).</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Find the 5 most correlated pairs of variables.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>This can be done by ordering the rows of <code>C</code> in decreasing
order of absolute values of <code>C$r</code>, and then choosing the first 5 rows.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="multiple-regression.html#cb228-1" aria-hidden="true"></a>C_top &lt;-<span class="st"> </span><span class="kw">head</span>(C[<span class="kw">order</span>(<span class="kw">abs</span>(C<span class="op">$</span>r), <span class="dt">decreasing=</span><span class="ot">TRUE</span>),], <span class="dv">5</span>)</span>
<span id="cb228-2"><a href="multiple-regression.html#cb228-2" aria-hidden="true"></a>knitr<span class="op">::</span><span class="kw">kable</span>(C_top)</span></code></pre></div>
<table style="width:100%;">
<colgroup>
<col width="6%" />
<col width="53%" />
<col width="30%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">col1</th>
<th align="left">col2</th>
<th align="right">r</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1687</td>
<td align="left">electricity_installed_generating_capacity</td>
<td align="left">electricity_production</td>
<td align="right">0.99942</td>
</tr>
<tr class="even">
<td align="left">1684</td>
<td align="left">electricity_consumption</td>
<td align="left">electricity_production</td>
<td align="right">0.99921</td>
</tr>
<tr class="odd">
<td align="left">88</td>
<td align="left">labor_force</td>
<td align="left">population</td>
<td align="right">0.99862</td>
</tr>
<tr class="even">
<td align="left">1718</td>
<td align="left">electricity_installed_generating_capacity</td>
<td align="left">electricity_consumption</td>
<td align="right">0.99815</td>
</tr>
<tr class="odd">
<td align="left">1300</td>
<td align="left">telephones_mobile_cellular</td>
<td align="left">labor_force</td>
<td align="right">0.99793</td>
</tr>
</tbody>
</table>
<p><em>Comment: The most correlated pairs of features are not really
“mind-blowing”…</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit simple regression models for the most correlated pair of variables.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>There is a degree of ambiguity here: should <code>col1</code> or rather <code>col2</code>
be treated as the dependent variable in our model?
Let’s do it either way.</p>
<p>To learn something new, which is exactly why we are all here,
we will create the formulas programmatically, by first
concatenating (joining) appropriate strings
(note that in order to input a double quotes character,
we need to proceed in with a backslash), and then
calling the <code>formula()</code> function.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="multiple-regression.html#cb229-1" aria-hidden="true"></a>form &lt;-<span class="st"> </span><span class="kw">formula</span>(<span class="kw">paste</span>(C_top[<span class="dv">1</span>,<span class="dv">2</span>], <span class="st">&quot;~&quot;</span>, C_top[<span class="dv">1</span>,<span class="dv">1</span>]))</span>
<span id="cb229-2"><a href="multiple-regression.html#cb229-2" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">lm</span>(form, <span class="dt">data=</span>factbook)</span>
<span id="cb229-3"><a href="multiple-regression.html#cb229-3" aria-hidden="true"></a><span class="kw">print</span>(f)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = form, data = factbook)
## 
## Coefficients:
##                               (Intercept)  
##                                  7.95e+08  
## electricity_installed_generating_capacity  
##                                  3.63e+03</code></pre>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="multiple-regression.html#cb231-1" aria-hidden="true"></a><span class="kw">plot</span>(factbook[,C_top[<span class="dv">1</span>,<span class="dv">1</span>]], factbook[,C_top[<span class="dv">1</span>,<span class="dv">2</span>]],</span>
<span id="cb231-2"><a href="multiple-regression.html#cb231-2" aria-hidden="true"></a>    <span class="dt">xlab=</span>C_top[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">ylab=</span>C_top[<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb231-3"><a href="multiple-regression.html#cb231-3" aria-hidden="true"></a><span class="kw">abline</span>(f, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:factbookB9"></span>
<img src="02-regression-multiple-figures/factbookB9-1.png" alt="" />
<p class="caption">Figure 2.21:  Most correlated pair of variables and the invisible regression line</p>
</div>
<p>Figure <a href="multiple-regression.html#fig:factbookB9">2.21</a> depicts the fitted model.</p>
</details>
</div>
<div id="countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita" class="section level3" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Countries of the World – A non-linear model based on the GDP per capita</h3>
<p>Let’s revisit the World Factbook 2020 dataset (<code>world_factbook_2020.csv</code>).</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="multiple-regression.html#cb232-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</span>
<span id="cb232-2"><a href="multiple-regression.html#cb232-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Draw a histogram of the empirical distribution
of the GDP per capita. Moreover, draw a histogram of the logarithm
of the GDP/person.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="multiple-regression.html#cb233-1" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb233-2"><a href="multiple-regression.html#cb233-2" aria-hidden="true"></a><span class="kw">hist</span>(factbook<span class="op">$</span>gdp_per_capita_ppp, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>, <span class="dt">main=</span><span class="ot">NA</span>)</span>
<span id="cb233-3"><a href="multiple-regression.html#cb233-3" aria-hidden="true"></a><span class="kw">hist</span>(<span class="kw">log</span>(factbook<span class="op">$</span>gdp_per_capita_ppp), <span class="dt">col=</span><span class="st">&quot;white&quot;</span>, <span class="dt">main=</span><span class="ot">NA</span>)</span></code></pre></div>
<div class="figure"><span id="fig:factbookC2"></span>
<img src="02-regression-multiple-figures/factbookC2-1.png" alt="" />
<p class="caption">Figure 2.22:  Histograms of the empirical distribution of the GDP per capita with linear (left) and log (right) scale on the X axis</p>
</div>
<p><em>Comment: In Figure <a href="multiple-regression.html#fig:factbookC2">2.22</a> we see that
distribution of the GDP is right-skewed: most countries
have small GDP. However, few of them
(those in the “right tail” of the distribution)
are very very rich (hey, how about taxing the richest countries?!).
There is the famous observation made by V. Pareto
stating that most assets are in the hands of the “wealthy minority”
(compare: power law, rich-get-richer rule, preferential attachment in complex networks).
Interestingly, many real-world-phenomena are distributed similarly
(e.g., the popularity of web pages, the number of followers of Instagram
profiles). It is frequently the case that the logarithm of the aforementioned
variable looks more “normal” (is bell-shaped).</em></p>
<p><em>Side note: “The” logarithm most often refers to the logarithm base
<span class="math inline">\(e\)</span>, <span class="math inline">\(\log x = \log_e x\)</span>,
where <span class="math inline">\(e\simeq 2.72\)</span> is the Euler constant, see <code>exp(1)</code> in R.
Note that you can only compute logarithms of positive real numbers.</em></p>
<p>Non-technical audience might be confused when asked to contemplate
the distribution of the logarithm of a variable. Let’s make it
more user-friendly (on the other hand, we could’ve asked them
to harden up…)
by nicely re-labelling the X axis,
see Figure <a href="multiple-regression.html#fig:factbookC3">2.23</a>.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="multiple-regression.html#cb234-1" aria-hidden="true"></a><span class="kw">hist</span>(<span class="kw">log</span>(factbook<span class="op">$</span>gdp_per_capita_ppp), <span class="dt">axes=</span><span class="ot">FALSE</span>,</span>
<span id="cb234-2"><a href="multiple-regression.html#cb234-2" aria-hidden="true"></a>    <span class="dt">xlab=</span><span class="st">&quot;GDP per capita (thousands USD)&quot;</span>, <span class="dt">main=</span><span class="ot">NA</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb234-3"><a href="multiple-regression.html#cb234-3" aria-hidden="true"></a><span class="kw">box</span>()</span>
<span id="cb234-4"><a href="multiple-regression.html#cb234-4" aria-hidden="true"></a><span class="kw">axis</span>(<span class="dv">2</span>) <span class="co"># Y axis</span></span>
<span id="cb234-5"><a href="multiple-regression.html#cb234-5" aria-hidden="true"></a>at &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">5000</span>, <span class="dv">10000</span>, <span class="dv">20000</span>, <span class="dv">50000</span>, <span class="dv">100000</span>, <span class="dv">200000</span>)</span>
<span id="cb234-6"><a href="multiple-regression.html#cb234-6" aria-hidden="true"></a><span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at=</span><span class="kw">log</span>(at), <span class="dt">labels=</span>at<span class="op">/</span><span class="dv">1000</span>)</span></code></pre></div>
<div class="figure"><span id="fig:factbookC3"></span>
<img src="02-regression-multiple-figures/factbookC3-1.png" alt="" />
<p class="caption">Figure 2.23:  Histogram of the empirical distribution of the GDP per capita now with human-readable X axis labels (not the logarithmic scale)</p>
</div>
<p><em>Comment: This is still a plot of the logarithm of the
distribution of the per capita GDP, but it’s somehow “hidden” behind
the human-readable axis labels. Nice.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit a simple linear model of <code>life_expectancy_at_birth</code>
as a function of <code>gdp_per_capita_ppp</code>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Easy. We have already done than in one of the previous exercises.
Yet, to learn something new, let’s note that the <code>plot()</code> function
accepts formulas as well.</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="multiple-regression.html#cb235-1" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">lm</span>(life_expectancy_at_birth<span class="op">~</span>gdp_per_capita_ppp, <span class="dt">data=</span>factbook)</span>
<span id="cb235-2"><a href="multiple-regression.html#cb235-2" aria-hidden="true"></a><span class="kw">plot</span>(life_expectancy_at_birth<span class="op">~</span>gdp_per_capita_ppp, <span class="dt">data=</span>factbook)</span>
<span id="cb235-3"><a href="multiple-regression.html#cb235-3" aria-hidden="true"></a><span class="kw">abline</span>(f, <span class="dt">col=</span><span class="st">&quot;purple&quot;</span>)</span>
<span id="cb235-4"><a href="multiple-regression.html#cb235-4" aria-hidden="true"></a><span class="kw">summary</span>(f)<span class="op">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.43096</code></pre>
<div class="figure"><span id="fig:factbookC4"></span>
<img src="02-regression-multiple-figures/factbookC4-1.png" alt="" />
<p class="caption">Figure 2.24:  Linear model fitted for life expectancy vs. GDP/person</p>
</div>
<p><em>Comment: From Figure <a href="multiple-regression.html#fig:factbookC4">2.24</a> we see that
this is not a good model.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Draw a scatter plot of <code>life_expectancy_at_birth</code> as a function
<code>gdp_per_capita_ppp</code>, with the X axis being logarithmic.
Compute the correlation coefficient between
<code>log(gdp_per_capita_ppp)</code> and <code>life_expectancy_at_birth</code>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>We could apply the <code>log()</code>-transformation manually
and generate fancy X axis labels ourselves. However,
the <code>plot()</code> function has the <code>log</code> argument (see <code>?plot.default</code>)
which provides us with all we need, see Figure <a href="multiple-regression.html#fig:factbookC5">2.25</a>.</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="multiple-regression.html#cb237-1" aria-hidden="true"></a><span class="kw">plot</span>(factbook<span class="op">$</span>gdp_per_capita_ppp,</span>
<span id="cb237-2"><a href="multiple-regression.html#cb237-2" aria-hidden="true"></a>    factbook<span class="op">$</span>life_expectancy_at_birth,</span>
<span id="cb237-3"><a href="multiple-regression.html#cb237-3" aria-hidden="true"></a>    <span class="dt">log=</span><span class="st">&quot;x&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:factbookC5"></span>
<img src="02-regression-multiple-figures/factbookC5-1.png" alt="" />
<p class="caption">Figure 2.25:  Scatter plot of life expectancy vs. GDP/person with log scale on the X axis</p>
</div>
<p>Here is the <em>linear</em> correlation coefficient between the logarithm
of the GDP/person and the life expectancy.</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="multiple-regression.html#cb238-1" aria-hidden="true"></a><span class="kw">cor</span>(<span class="kw">log</span>(factbook<span class="op">$</span>gdp_per_capita_ppp), factbook<span class="op">$</span>life_expectancy_at_birth,</span>
<span id="cb238-2"><a href="multiple-regression.html#cb238-2" aria-hidden="true"></a>    <span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.80665</code></pre>
<p>The correlation is quite high, hence the following task.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit a model predicting <code>life_expectancy_at_birth</code>
by means of <code>log(gdp_per_capita_ppp)</code>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>We would like to fit a model of the form <span class="math inline">\(Y=a\log X+b\)</span>.
The formula <code>life_expectancy_at_birth~log(gdp_per_capita_ppp)</code>
should do the trick here.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="multiple-regression.html#cb240-1" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">lm</span>(life_expectancy_at_birth<span class="op">~</span><span class="kw">log</span>(gdp_per_capita_ppp), <span class="dt">data=</span>factbook)</span>
<span id="cb240-2"><a href="multiple-regression.html#cb240-2" aria-hidden="true"></a><span class="kw">plot</span>(life_expectancy_at_birth<span class="op">~</span><span class="kw">log</span>(gdp_per_capita_ppp), <span class="dt">data=</span>factbook)</span>
<span id="cb240-3"><a href="multiple-regression.html#cb240-3" aria-hidden="true"></a><span class="kw">abline</span>(f, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span>
<span id="cb240-4"><a href="multiple-regression.html#cb240-4" aria-hidden="true"></a>f<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>##             (Intercept) log(gdp_per_capita_ppp) 
##                 28.3064                  4.8178</code></pre>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="multiple-regression.html#cb242-1" aria-hidden="true"></a><span class="kw">summary</span>(f)<span class="op">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.65069</code></pre>
<div class="figure"><span id="fig:factbookC7"></span>
<img src="02-regression-multiple-figures/factbookC7-1.png" alt="" />
<p class="caption">Figure 2.26:  Linear model fitted for life expectancy vs. the logarithm of GDP/person</p>
</div>
<p><em>Comment: That is an okay model (in terms of the coefficient of determination), see Figure <a href="multiple-regression.html#fig:factbookC7">2.26</a>.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Draw the fitted logarithmic model on a scatter plot
with a standard, non-logarithmic X axis.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>The model fitted above is of the form
<span class="math inline">\(Y\simeq4.82 \log X+28.31\)</span>.
To depict it on a plot with linear (non-logarithmic) axes,
we can compute this formula on multiple points by hand,
see Figure <a href="multiple-regression.html#fig:factbookC8">2.27</a>.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="multiple-regression.html#cb244-1" aria-hidden="true"></a><span class="kw">plot</span>(factbook<span class="op">$</span>gdp_per_capita_ppp, factbook<span class="op">$</span>life_expectancy_at_birth)</span>
<span id="cb244-2"><a href="multiple-regression.html#cb244-2" aria-hidden="true"></a></span>
<span id="cb244-3"><a href="multiple-regression.html#cb244-3" aria-hidden="true"></a><span class="co"># many points on the X axis:</span></span>
<span id="cb244-4"><a href="multiple-regression.html#cb244-4" aria-hidden="true"></a>xxx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(factbook<span class="op">$</span>gdp_per_capita_ppp, <span class="dt">na.rm=</span><span class="ot">TRUE</span>),</span>
<span id="cb244-5"><a href="multiple-regression.html#cb244-5" aria-hidden="true"></a>            <span class="kw">max</span>(factbook<span class="op">$</span>gdp_per_capita_ppp, <span class="dt">na.rm=</span><span class="ot">TRUE</span>),</span>
<span id="cb244-6"><a href="multiple-regression.html#cb244-6" aria-hidden="true"></a>            <span class="dt">length.out=</span><span class="dv">101</span>)</span>
<span id="cb244-7"><a href="multiple-regression.html#cb244-7" aria-hidden="true"></a>yyy &lt;-<span class="st"> </span>f<span class="op">$</span>coefficients[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>f<span class="op">$</span>coefficients[<span class="dv">2</span>]<span class="op">*</span><span class="kw">log</span>(xxx)</span>
<span id="cb244-8"><a href="multiple-regression.html#cb244-8" aria-hidden="true"></a><span class="kw">lines</span>(xxx, yyy, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:factbookC8"></span>
<img src="02-regression-multiple-figures/factbookC8-1.png" alt="" />
<p class="caption">Figure 2.27:  Logarithmic model fitted for life expectancy vs. GDP/person</p>
</div>
<p><em>Comment: Well, people are not immortal…
The original (linear) model didn’t really take that into account.
Also, recall that correlation is not causation.
Moreover, there is a lot of variability at an individual level.
Being born in a less-wealthy country (e.g., not in a tax haven),
doesn’t mean you don’t have the whole life ahead of you.
Do the cool staff, do something for the others. Life’s not about money.</em></p>
</details>
</div>
<div id="countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp" class="section level3" number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> Countries of the World – A multiple regression model for the per capita GDP</h3>
<p>Let’s play with World Factbook 2020
(<code>world_factbook_2020.csv</code>) once again.
World is an interesting place, so we’re far from being bored with this dataset.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="multiple-regression.html#cb245-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</span>
<span id="cb245-2"><a href="multiple-regression.html#cb245-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<p>Let’s restrict ourselves to the following columns, mostly
related to imports and exports:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="multiple-regression.html#cb246-1" aria-hidden="true"></a>factbookn &lt;-<span class="st"> </span>factbook[<span class="kw">c</span>(<span class="st">&quot;gdp_purchasing_power_parity&quot;</span>,</span>
<span id="cb246-2"><a href="multiple-regression.html#cb246-2" aria-hidden="true"></a>    <span class="st">&quot;imports&quot;</span>, <span class="st">&quot;exports&quot;</span>, <span class="st">&quot;electricity_exports&quot;</span>,</span>
<span id="cb246-3"><a href="multiple-regression.html#cb246-3" aria-hidden="true"></a>    <span class="st">&quot;electricity_imports&quot;</span>, <span class="st">&quot;military_expenditures&quot;</span>,</span>
<span id="cb246-4"><a href="multiple-regression.html#cb246-4" aria-hidden="true"></a>    <span class="st">&quot;crude_oil_exports&quot;</span>, <span class="st">&quot;crude_oil_imports&quot;</span>,</span>
<span id="cb246-5"><a href="multiple-regression.html#cb246-5" aria-hidden="true"></a>    <span class="st">&quot;natural_gas_exports&quot;</span>, <span class="st">&quot;natural_gas_imports&quot;</span>,</span>
<span id="cb246-6"><a href="multiple-regression.html#cb246-6" aria-hidden="true"></a>    <span class="st">&quot;reserves_of_foreign_exchange_and_gold&quot;</span>)]</span></code></pre></div>
<p>Let’s compute the per capita versions of the above, by dividing
all values by each country’s population:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="multiple-regression.html#cb247-1" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(factbookn))</span>
<span id="cb247-2"><a href="multiple-regression.html#cb247-2" aria-hidden="true"></a>    factbookn[[i]] &lt;-<span class="st"> </span>factbookn[[i]]<span class="op">/</span>factbook<span class="op">$</span>population</span></code></pre></div>
<p>We are going to build a few multiple regression models using the
<code>step()</code> function, which is not too fond of missing values, therefore
they should be removed first:</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="multiple-regression.html#cb248-1" aria-hidden="true"></a>factbookn &lt;-<span class="st"> </span><span class="kw">na.omit</span>(factbookn)</span>
<span id="cb248-2"><a href="multiple-regression.html#cb248-2" aria-hidden="true"></a><span class="kw">c</span>(<span class="kw">nrow</span>(factbook), <span class="kw">nrow</span>(factbookn)) <span class="co"># how many countries were omitted?</span></span></code></pre></div>
<pre><code>## [1] 261 157</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Build a model for <code>gdp_purchasing_power_parity</code> as a function
of <code>imports</code> and <code>exports</code> (all per capita).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Let’s first take a look at how the aforementioned variables
are related to each other, see Figure <a href="multiple-regression.html#fig:factbookD5">2.28</a>.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="multiple-regression.html#cb250-1" aria-hidden="true"></a><span class="kw">pairs</span>(factbookn[<span class="kw">c</span>(<span class="st">&quot;gdp_purchasing_power_parity&quot;</span>, <span class="st">&quot;imports&quot;</span>, <span class="st">&quot;exports&quot;</span>)])</span>
<span id="cb250-2"><a href="multiple-regression.html#cb250-2" aria-hidden="true"></a><span class="kw">cor</span>(factbookn[<span class="kw">c</span>(<span class="st">&quot;gdp_purchasing_power_parity&quot;</span>, <span class="st">&quot;imports&quot;</span>, <span class="st">&quot;exports&quot;</span>)])</span></code></pre></div>
<pre><code>##                             gdp_purchasing_power_parity imports exports
## gdp_purchasing_power_parity                     1.00000 0.82891 0.81899
## imports                                         0.82891 1.00000 0.94241
## exports                                         0.81899 0.94241 1.00000</code></pre>
<div class="figure"><span id="fig:factbookD5"></span>
<img src="02-regression-multiple-figures/factbookD5-1.png" alt="" />
<p class="caption">Figure 2.28:  Scatter plot matrix for GDP, imports and exports</p>
</div>
<p>They are nicely correlated. Moreover, they are on a similar scale
(“tens of thousands of USD per capita”).</p>
<p>Fitting the requested model yields:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="multiple-regression.html#cb252-1" aria-hidden="true"></a><span class="kw">options</span>(<span class="dt">scipen=</span><span class="dv">10</span>) <span class="co"># prefer &quot;decimal&quot; over &quot;scientific&quot; notation</span></span>
<span id="cb252-2"><a href="multiple-regression.html#cb252-2" aria-hidden="true"></a>f1 &lt;-<span class="st"> </span><span class="kw">lm</span>(gdp_purchasing_power_parity<span class="op">~</span>imports<span class="op">+</span>exports, <span class="dt">data=</span>factbookn)</span>
<span id="cb252-3"><a href="multiple-regression.html#cb252-3" aria-hidden="true"></a>f1<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)     imports     exports 
##  9852.53813     1.44194     0.78067</code></pre>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="multiple-regression.html#cb254-1" aria-hidden="true"></a><span class="kw">summary</span>(f1)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.69598</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Use forward selection to come up with
a model for <code>gdp_purchasing_power_parity</code> per capita.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="multiple-regression.html#cb256-1" aria-hidden="true"></a>(model_empty &lt;-<span class="st"> </span>gdp_purchasing_power_parity<span class="op">~</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## gdp_purchasing_power_parity ~ 1</code></pre>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="multiple-regression.html#cb258-1" aria-hidden="true"></a>(model_full &lt;-<span class="st"> </span><span class="kw">formula</span>(<span class="kw">model.frame</span>(gdp_purchasing_power_parity<span class="op">~</span>., <span class="dt">data=</span>factbookn)))</span></code></pre></div>
<pre><code>## gdp_purchasing_power_parity ~ imports + exports + electricity_exports + 
##     electricity_imports + military_expenditures + crude_oil_exports + 
##     crude_oil_imports + natural_gas_exports + natural_gas_imports + 
##     reserves_of_foreign_exchange_and_gold</code></pre>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="multiple-regression.html#cb260-1" aria-hidden="true"></a>f2 &lt;-<span class="st"> </span><span class="kw">step</span>(<span class="kw">lm</span>(model_empty, <span class="dt">data=</span>factbookn),</span>
<span id="cb260-2"><a href="multiple-regression.html#cb260-2" aria-hidden="true"></a>    <span class="dt">scope=</span>model_full,</span>
<span id="cb260-3"><a href="multiple-regression.html#cb260-3" aria-hidden="true"></a>    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>, <span class="dt">trace=</span><span class="dv">0</span>)</span>
<span id="cb260-4"><a href="multiple-regression.html#cb260-4" aria-hidden="true"></a>f2</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp_purchasing_power_parity ~ imports + crude_oil_exports + 
##     crude_oil_imports + electricity_imports + natural_gas_imports, 
##     data = factbookn)
## 
## Coefficients:
##         (Intercept)              imports    crude_oil_exports  
##             7603.24                 1.77            128472.22  
##   crude_oil_imports  electricity_imports  natural_gas_imports  
##           100781.64                 1.62                 3.13</code></pre>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="multiple-regression.html#cb262-1" aria-hidden="true"></a><span class="kw">summary</span>(f2)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.7865</code></pre>
<p><em>Comment: Interestingly, it’s mostly the import-related variables
that contribute to the GDP per capita. However, the model
is not perfect, so we should refrain ourselves from building a brand new
economic theory around this “discovery”. On the other hand,
you know what they say: all models are wrong, but some might be useful.
Note that we used the adjusted <span class="math inline">\(R^2\)</span> coefficient to correct
for the number of variables in the model
so as to make it more comparable with the coefficient corresponding
to the <code>f1</code> model.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Use backward elimination to construct a model
for <code>gdp_purchasing_power_parity</code> per capita.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="multiple-regression.html#cb264-1" aria-hidden="true"></a>f3 &lt;-<span class="st"> </span><span class="kw">step</span>(<span class="kw">lm</span>(model_full, <span class="dt">data=</span>factbookn),</span>
<span id="cb264-2"><a href="multiple-regression.html#cb264-2" aria-hidden="true"></a>    <span class="dt">scope=</span>model_empty,</span>
<span id="cb264-3"><a href="multiple-regression.html#cb264-3" aria-hidden="true"></a>    <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>, <span class="dt">trace=</span><span class="dv">0</span>)</span>
<span id="cb264-4"><a href="multiple-regression.html#cb264-4" aria-hidden="true"></a>f3</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gdp_purchasing_power_parity ~ imports + electricity_imports + 
##     crude_oil_exports + crude_oil_imports + natural_gas_imports, 
##     data = factbookn)
## 
## Coefficients:
##         (Intercept)              imports  electricity_imports  
##             7603.24                 1.77                 1.62  
##   crude_oil_exports    crude_oil_imports  natural_gas_imports  
##           128472.22            100781.64                 3.13</code></pre>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="multiple-regression.html#cb266-1" aria-hidden="true"></a><span class="kw">summary</span>(f3)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.7865</code></pre>
<p><em>Comment: This is the same model as the one
found by forward selection, i.e., <code>f2</code>.</em></p>
</details>
<!--



a binary variable 0/1
fit two models and compare
fit a single model involving the 0/1 variable and compare




multiple regression

draw scatterplot matrix (pair plot)

compute the matrix of Pearson's correlations

fit the full model

fit some other models

draw the fitted vs residuals plot

provide an interpretation of each coefficient in the model

standardise the variables??? and give the interpretation

forward selection/backward elimination



linearisation example


highly correlated input variables



-->
</div>
</div>
<div id="outro-1" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Outro</h2>
<div id="remarks-1" class="section level3" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Remarks</h3>
<p>Multiple regression is simple, fast to apply and interpretable.</p>
<p>Linear models go beyond fitting of straight lines and other hyperplanes!</p>
<p>A complex model may overfit and hence generalise poorly to unobserved inputs.</p>
<p>Note that the SSR criterion makes the models sensitive to outliers.</p>
<p><strong>Remember:</strong></p>
<p>good models
<span class="math display">\[=\]</span>
better understanding of the modelled reality <span class="math inline">\(+\)</span> better predictions
<span class="math display">\[=\]</span>
more revenue, your boss’ happiness, your startup’s growth etc.</p>
</div>
<div id="other-methods-for-regression" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Other Methods for Regression</h3>
<p>Other example approaches to regression:</p>
<ul>
<li>ridge regression,</li>
<li>lasso regression,</li>
<li>least absolute deviations (LAD) regression,</li>
<li>multiadaptive regression splines (MARS),</li>
<li>K-nearest neighbour (K-NN) regression, see <code>FNN::knn.reg()</code> in R,</li>
<li>regression trees,</li>
<li>support-vector regression (SVR),</li>
<li>neural networks (also deep) for regression.</li>
</ul>
</div>
<div id="derivation-of-the-solution-1" class="section level3" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Derivation of the Solution (**)</h3>
<p>We would like to find an analytical solution
to the problem of minimising of the sum of squared residuals:</p>
<p><span class="math display">\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}} E(\beta_0, \beta_1, \dots, \beta_p)=
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)^2
\]</span></p>
<p>This requires computing the <span class="math inline">\(p+1\)</span> partial derivatives
<span class="math inline">\({\partial E}/{\partial \beta_j}\)</span> for <span class="math inline">\(j=0,\dots,p\)</span>.</p>
<p>The partial derivatives are very similar to each other;
<span class="math inline">\(\frac{\partial E}{\partial \beta_0}\)</span> is given by:
<span class="math display">\[
\frac{\partial E}{\partial \beta_0}(\beta_0,\beta_1,\dots,\beta_p)=
2 \sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)
\]</span>
and <span class="math inline">\(\frac{\partial E}{\partial \beta_j}\)</span> for <span class="math inline">\(j&gt;0\)</span> is equal to:
<span class="math display">\[
\frac{\partial E}{\partial \beta_j}(\beta_0,\beta_1,\dots,\beta_p)=
2 \sum_{i=1}^n x_{i,j} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)
\]</span></p>
<p>Then all we need to do is to solve the system of linear equations:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\frac{\partial E}{\partial \beta_0}(\beta_0,\beta_1,\dots,\beta_p)&amp;=&amp;0 \\
\frac{\partial E}{\partial \beta_1}(\beta_0,\beta_1,\dots,\beta_p)&amp;=&amp;0 \\
\vdots\\
\frac{\partial E}{\partial \beta_p}(\beta_0,\beta_1,\dots,\beta_p)&amp;=&amp;0 \\
\end{array}
\right.
\]</span></p>
<p>The above system of <span class="math inline">\(p+1\)</span> linear equations, which we are supposed to solve
for <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_p\)</span>:
<span class="math display">\[
\left\{
\begin{array}{rcl}
2 \sum_{i=1}^n \phantom{x_{i,0}}\left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&amp;=&amp;0 \\
2 \sum_{i=1}^n x_{i,1} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&amp;=&amp;0 \\
\vdots\\
2 \sum_{i=1}^n x_{i,p} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&amp;=&amp;0 \\
\end{array}
\right.
\]</span>
can be rewritten as:
<span class="math display">\[
\left\{
\begin{array}{rcl}
\sum_{i=1}^n \phantom{x_{i,0}}\left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&amp;=&amp; \sum_{i=1}^n \phantom{x_{i,0}} y_i \\
\sum_{i=1}^n x_{i,1} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&amp;=&amp;\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\sum_{i=1}^n x_{i,p} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&amp;=&amp;\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]</span></p>
<p>and further as:
<span class="math display">\[
\left\{
\begin{array}{rcl}
\beta_0\ n\phantom{\sum_{i=1}^n x} + \beta_1\sum_{i=1}^n \phantom{x_{i,0}}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n \phantom{x_{i,0}}  x_{i,p} &amp;=&amp;\sum_{i=1}^n\phantom{x_{i,0}} y_i \\
\beta_0 \sum_{i=1}^n x_{i,1} + \beta_1\sum_{i=1}^n x_{i,1}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,1}  x_{i,p} &amp;=&amp;\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\beta_0 \sum_{i=1}^n x_{i,p} + \beta_1\sum_{i=1}^n x_{i,p}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,p}  x_{i,p} &amp;=&amp;\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]</span>
Note that the terms involving <span class="math inline">\(x_{i,j}\)</span> and <span class="math inline">\(y_i\)</span> (the sums) are all constant
– these are some fixed real numbers. We have learned how to solve such
problems in high school.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Try deriving the analytical solution and implementing it for <span class="math inline">\(p=2\)</span>.
Recall that in the previous chapter we solved the special case of <span class="math inline">\(p=1\)</span>.</p>
</div>
</div>
<div id="solution-in-matrix-form" class="section level3" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Solution in Matrix Form (***)</h3>
<p>Assume that <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> (a matrix with inputs),
<span class="math inline">\(\mathbf{y}\in\mathbb{R}^{n\times 1}\)</span> (a column vector of reference outputs)
and
<span class="math inline">\(\boldsymbol{\beta}\in\mathbb{R}^{(p+1)\times 1}\)</span> (a column vector of parameters).</p>
<p>Firstly, note that a linear model of the form:
<span class="math display">\[
f_{\boldsymbol\beta}(\mathbf{x})=\beta_0+\beta_1 x_1+\dots+\beta_p x_p
\]</span>
can be rewritten as:
<span class="math display">\[
f_{\boldsymbol\beta}(\mathbf{x})=\beta_0 1+\beta_1 x_1+\dots+\beta_p x_p
=\mathbf{\dot{x}}\boldsymbol\beta,
\]</span>
where <span class="math inline">\(\mathbf{\dot{x}}=[1\ x_1\ x_2\ \cdots\ x_p]\)</span>.</p>
<p>Similarly, if we assume that <span class="math inline">\(\mathbf{\dot{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}\)</span>
is the input matrix with a prepended column of <span class="math inline">\(1\)</span>s, i.e.,
<span class="math inline">\(\boldsymbol{1}=[1\ 1\ \cdots\ 1]^T\)</span> and <span class="math inline">\(\dot{x}_{i,0}=1\)</span> (for brevity of notation
the columns added will have index <span class="math inline">\(0\)</span>),
<span class="math inline">\(\dot{x}_{i,j}=x_{i,j}\)</span> for all <span class="math inline">\(j\ge 1\)</span> and all <span class="math inline">\(i\)</span>,
then:
<span class="math display">\[
\mathbf{\hat{y}} = \mathbf{\dot{X}} \boldsymbol\beta
\]</span>
gives the vector of predicted outputs for every input point.</p>
<p>This way, the sum of squared residuals
<span class="math display">\[
E(\beta_0, \beta_1, \dots, \beta_p)=
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)^2
\]</span>
can be rewritten as:
<span class="math display">\[
E(\boldsymbol\beta)=\| \mathbf{\dot{X}} \boldsymbol\beta - \mathbf{y} \|^2,
\]</span>
where as usual <span class="math inline">\(\|\cdot\|^2\)</span> denotes the squared Euclidean norm.</p>
<p>Recall that this can be re-expressed as:
<span class="math display">\[
E(\boldsymbol\beta)= (\mathbf{\dot{X}} \boldsymbol\beta - \mathbf{y})^T (\mathbf{\dot{X}} \boldsymbol\beta - \mathbf{y}).
\]</span></p>
<p>In order to find the minimum of <span class="math inline">\(E\)</span> w.r.t. <span class="math inline">\(\boldsymbol\beta\)</span>,
we need to find the parameters that make the partial derivatives vanish, i.e.:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
\frac{\partial E}{\partial \beta_0}(\boldsymbol\beta)&amp;=&amp;0 \\
\frac{\partial E}{\partial \beta_1}(\boldsymbol\beta)&amp;=&amp;0 \\
\vdots\\
\frac{\partial E}{\partial \beta_p}(\boldsymbol\beta)&amp;=&amp;0 \\
\end{array}
\right.
\]</span></p>
<dl>
<dt>Remark.</dt>
<dd><p>(***) Interestingly, the above can also be expressed in matrix form,
using the special notation:
<span class="math display">\[
\nabla E(\boldsymbol\beta) = \boldsymbol{0}
\]</span>
Here, <span class="math inline">\(\nabla E\)</span> (nabla symbol = differential operator)
denotes the function gradient, i.e., the vector of all partial derivatives.
This is nothing more than syntactic sugar for this quite commonly applied operator.</p>
</dd>
</dl>
<p>Anyway, the system of linear equations we have derived above:
<span class="math display">\[
\left\{
\begin{array}{rcl}
\beta_0\ n\phantom{\sum_{i=1}^n x} + \beta_1\sum_{i=1}^n \phantom{x_{i,0}}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n \phantom{x_{i,0}}  x_{i,p} &amp;=&amp;\sum_{i=1}^n\phantom{x_{i,0}} y_i \\
\beta_0 \sum_{i=1}^n x_{i,1} + \beta_1\sum_{i=1}^n x_{i,1}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,1}  x_{i,p} &amp;=&amp;\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\beta_0 \sum_{i=1}^n x_{i,p} + \beta_1\sum_{i=1}^n x_{i,p}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,p}  x_{i,p} &amp;=&amp;\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]</span>
can be rewritten in matrix terms as:
<span class="math display">\[
\left\{
\begin{array}{rcl}
\beta_0 \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{x}}_{\cdot,0} + \beta_1 \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{x}}_{\cdot,1}+\dots+\beta_p \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{x}}_{\cdot,p} &amp;=&amp; \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{y} \\
\beta_0 \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{x}}_{\cdot,0} + \beta_1 \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{x}}_{\cdot,1}+\dots+\beta_p \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{x}}_{\cdot,p} &amp;=&amp; \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{y} \\
\vdots\\
\beta_0 \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{x}}_{\cdot,0} + \beta_1 \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{x}}_{\cdot,1}+\dots+\beta_p \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{x}}_{\cdot,p} &amp;=&amp; \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{y}\\
\end{array}
\right.
\]</span></p>
<p>This can be restated as:
<span class="math display">\[
\left\{
\begin{array}{rcl}
\left(\mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{X}}\right)\, \boldsymbol\beta &amp;=&amp; \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{y} \\
\left(\mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{X}}\right)\, \boldsymbol\beta  &amp;=&amp; \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{y} \\
\vdots\\
\left(\mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{X}}\right)\, \boldsymbol\beta  &amp;=&amp; \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{y}\\
\end{array}
\right.
\]</span>
which in turn is equivalent to:
<span class="math display">\[
\left(\mathbf{\dot{X}}^T\mathbf{X}\right)\,\boldsymbol\beta = \mathbf{\dot{X}}^T\mathbf{y}.
\]</span></p>
<p>Such a system of linear equations in matrix form can be solved numerically using,
amongst others, the <code>solve()</code> function.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(***) In practice, we’d rather rely on QR or SVD decompositions
of matrices for efficiency and numerical accuracy reasons.</p>
</dd>
</dl>
<p>Numeric example – solution via <code>lm()</code>:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="multiple-regression.html#cb268-1" aria-hidden="true"></a>X1 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb268-2"><a href="multiple-regression.html#cb268-2" aria-hidden="true"></a>X2 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Income[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb268-3"><a href="multiple-regression.html#cb268-3" aria-hidden="true"></a>Y  &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>])</span>
<span id="cb268-4"><a href="multiple-regression.html#cb268-4" aria-hidden="true"></a><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span>X2)<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          X1          X2 
##    172.5587      0.1828      2.1976</code></pre>
<p>Recalling that <span class="math inline">\(\mathbf{A}^T \mathbf{B}\)</span> can be computed
by calling <code>t(A) %*% B</code> or – even faster – by calling <code>crossprod(A, B)</code>,
we can also use <code>solve()</code> to obtain the same result:</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="multiple-regression.html#cb270-1" aria-hidden="true"></a>X_dot &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, X1, X2)</span>
<span id="cb270-2"><a href="multiple-regression.html#cb270-2" aria-hidden="true"></a><span class="kw">solve</span>( <span class="kw">crossprod</span>(X_dot, X_dot), <span class="kw">crossprod</span>(X_dot, Y) )</span></code></pre></div>
<pre><code>##        [,1]
##    172.5587
## X1   0.1828
## X2   2.1976</code></pre>
</div>
<div id="pearsons-r-in-matrix-form" class="section level3" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Pearson’s r in Matrix Form (**)</h3>
<p>Recall the Pearson linear correlation coefficient:
<span class="math display">\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\ \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}
\]</span></p>
<p>Denote with <span class="math inline">\(\boldsymbol{x}^\circ\)</span> and <span class="math inline">\(\boldsymbol{y}^\circ\)</span> the centred versions
of <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(\boldsymbol{y}\)</span>, respectively,
i.e., <span class="math inline">\(x_i^\circ=x_i-\bar{x}\)</span> and <span class="math inline">\(y_i^\circ=y_i-\bar{y}\)</span>.</p>
<p>Rewriting the above yields:
<span class="math display">\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n x_i^\circ y_i^\circ
}{
    \sqrt{\sum_{i=1}^n ({x_i^\circ})^2}\  \sqrt{\sum_{i=1}^n ({y_i^\circ})^2}
}
\]</span>
which is exactly:
<span class="math display">\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \boldsymbol{x}^\circ\cdot \boldsymbol{y}^\circ
}{
    \| \boldsymbol{x}^\circ \|\    \| \boldsymbol{y}^\circ \|
}
\]</span>
i.e., the normalised dot product of the centred versions of the two vectors.</p>
<p>This is the cosine of the angle between the two vectors
(in <span class="math inline">\(n\)</span>-dimensional spaces)!</p>
<p>(**) Recalling from the previous chapter that <span class="math inline">\(\mathbf{A}^T \mathbf{A}\)</span>
gives the dot product between all the pairs of columns in a matrix <span class="math inline">\(\mathbf{A}\)</span>,
we can implement an equivalent version of <code>cor(C)</code> as follows:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="multiple-regression.html#cb272-1" aria-hidden="true"></a>C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</span>
<span id="cb272-2"><a href="multiple-regression.html#cb272-2" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</span>
<span id="cb272-3"><a href="multiple-regression.html#cb272-3" aria-hidden="true"></a>    <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</span>
<span id="cb272-4"><a href="multiple-regression.html#cb272-4" aria-hidden="true"></a>C_centred &lt;-<span class="st"> </span><span class="kw">apply</span>(C, <span class="dv">2</span>, <span class="cf">function</span>(c) c<span class="op">-</span><span class="kw">mean</span>(c))</span>
<span id="cb272-5"><a href="multiple-regression.html#cb272-5" aria-hidden="true"></a>C_normalised &lt;-<span class="st"> </span><span class="kw">apply</span>(C_centred, <span class="dv">2</span>, <span class="cf">function</span>(c)</span>
<span id="cb272-6"><a href="multiple-regression.html#cb272-6" aria-hidden="true"></a>    c<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(c<span class="op">^</span><span class="dv">2</span>)))</span>
<span id="cb272-7"><a href="multiple-regression.html#cb272-7" aria-hidden="true"></a><span class="kw">round</span>(<span class="kw">t</span>(C_normalised) <span class="op">%*%</span><span class="st"> </span>C_normalised, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##           Rating  Limit Income   Age Education Balance
## Rating     1.000  0.996  0.831 0.167    -0.040   0.798
## Limit      0.996  1.000  0.834 0.164    -0.032   0.796
## Income     0.831  0.834  1.000 0.227    -0.033   0.414
## Age        0.167  0.164  0.227 1.000     0.024   0.008
## Education -0.040 -0.032 -0.033 0.024     1.000   0.001
## Balance    0.798  0.796  0.414 0.008     0.001   1.000</code></pre>
</div>
<div id="further-reading-1" class="section level3" number="2.5.6">
<h3><span class="header-section-number">2.5.6</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(James et al. <a href="#ref-islr" role="doc-biblioref">2017</a>: Chapters 1, 2 and 3)</span></p>
<p>Other: <span class="citation">(Hastie et al. <a href="#ref-esl" role="doc-biblioref">2017</a>: Chapter 1, Sections 3.2 and 3.3)</span></p>

<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-esl">
<p>Hastie T, Tibshirani R, Friedman J (2017) <em>The elements of statistical learning</em>. Springer-Verlag <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-islr">
<p>James G, Witten D, Hastie T, Tibshirani R (2017) <em>An introduction to statistical learning with applications in R</em>. Springer-Verlag <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-with-k-nearest-neighbours.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
