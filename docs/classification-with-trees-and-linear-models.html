<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Classification with Trees and Linear Models | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="pandoc, pandoc-citeproc, knitr, bookdown (custom), GitBook, and many more" />

  <meta property="og:title" content="4 Classification with Trees and Linear Models | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-with-k-nearest-neighbours.html"/>
<link rel="next" href="shallow-and-deep-neural-networks.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<!-- Use KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<!-- /Use KaTeX -->

<style type="text/css">
/* Marek's custom CSS */

@import url("https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic,700italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans:400,500,600,700,800,900,400italic,500italic,600italic,700italic,800italic,900italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:400,600,700,400italic,600italic,700italic");

span.katex {
    font-size: 100%;
}

</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2.1 2021-05-10 09:49 (f981ede)</a></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#exercises-in-r-2"><i class="fa fa-check"></i><b>4.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-preparing-data"><i class="fa fa-check"></i><b>4.4.1</b> EdStats – Preparing Data</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.4.2</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.4.3</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>4.4.4</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>4.4.5</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.5</b> Outro</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks (*)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="5.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>5.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Continuous Optimisation with Iterative Algorithms (*)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="6.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>6.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="6.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.5</b> Outro</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>7.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3</b> Agglomerative Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>7.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#exercises-in-r-3"><i class="fa fa-check"></i><b>7.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>7.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>7.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>7.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.5</b> Outro</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms (*)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.2</b> Genetic Algorithms</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.2.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.3</b> Outro</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.3.1</b> Remarks</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems (*)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-16"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> Exercise: The MovieLens Dataset (*)</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a>
<ul>
<li class="chapter" data-level="B.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a>
<ul>
<li class="chapter" data-level="C.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a>
<ul>
<li class="chapter" data-level="C.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a>
<ul>
<li class="chapter" data-level="C.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>C.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a>
<ul>
<li class="chapter" data-level="D.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>D.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a>
<ul>
<li class="chapter" data-level="E.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a>
<ul>
<li class="chapter" data-level="E.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>E.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-trees-and-linear-models" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Classification with Trees and Linear Models</h1>
<blockquote>
<p>This is a slightly older (distributed in the hope that it will be useful)
version of the forthcoming textbook (ETA 2022) preliminarily entitled
<em>Machine Learning in R from Scratch</em> by
<a href="https://www.gagolewski.com">Marek Gagolewski</a>, which is now undergoing
a major revision (when I am not busy with other projects). There will be
not much work on-going in this repository anymore, as its sources have
moved elsewhere; however, if you happen to find any bugs or typos,
please drop me an
<a href="https://github.com/gagolews/lmlcr/blob/master/CODE_OF_CONDUCT.md">email</a>.
I will share a new draft once it’s ripe. Stay tuned.</p>
</blockquote>
<div id="introduction-5" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<div id="classification-task-1" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Classification Task</h3>
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space (each of the <span class="math inline">\(n\)</span> objects
is described by means of <span class="math inline">\(p\)</span> numerical features)</p>
<p>Recall that in supervised learning, with each
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output <span class="math inline">\(y_i\)</span>.</p>
<p>Hence, our dataset is <span class="math inline">\([\mathbf{X}\ \mathbf{y}]\)</span> –
where each object is represented as a row vector
<span class="math inline">\([\mathbf{x}_{i,\cdot}\ y_i]\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>:</p>
<p><span class="math display">\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} &amp; y_1\\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} &amp; y_2\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots    &amp; \vdots\\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} &amp; y_n\\
\end{array}
\right].
\]</span></p>
<div style="margin-top: 1em">

</div>
<p>In this chapter we are still interested in <strong>classification</strong> tasks;
we assume that each <span class="math inline">\(y_i\)</span> is a descriptive label.</p>
<p>Let’s assume that we are faced with <strong>binary classification</strong> tasks.</p>
<p>Hence, there are only two possible labels that we traditionally denote with <span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s.</p>
<p>For example:</p>
<table>
<thead>
<tr class="header">
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>false</td>
<td>true</td>
</tr>
<tr class="odd">
<td>failure</td>
<td>success</td>
</tr>
<tr class="even">
<td>healthy</td>
<td>ill</td>
</tr>
</tbody>
</table>
<p>Let’s recall the synthetic 2D dataset from the previous chapter
(true decision boundary is at <span class="math inline">\(X_1=0\)</span>), see Figure <a href="classification-with-trees-and-linear-models.html#fig:synthetic">4.1</a>.</p>
<div class="figure"><span id="fig:synthetic"></span>
<img src="04-classification-trees_and_logistic-figures/synthetic-1.png" alt="" />
<p class="caption">Figure 4.1:  A synthetic 2D dataset with the true decision boundary at <span class="math inline">\(X_1=0\)</span></p>
</div>
</div>
<div id="data-1" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Data</h3>
<p>For illustration, we’ll be considering the Wine Quality dataset
(white wines only):</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="classification-with-trees-and-linear-models.html#cb392-1" aria-hidden="true"></a>wines &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/winequality-all.csv&quot;</span>,</span>
<span id="cb392-2"><a href="classification-with-trees-and-linear-models.html#cb392-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb392-3"><a href="classification-with-trees-and-linear-models.html#cb392-3" aria-hidden="true"></a>wines &lt;-<span class="st"> </span>wines[wines<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</span>
<span id="cb392-4"><a href="classification-with-trees-and-linear-models.html#cb392-4" aria-hidden="true"></a>(n &lt;-<span class="st"> </span><span class="kw">nrow</span>(wines)) <span class="co"># number of samples</span></span></code></pre></div>
<pre><code>## [1] 4898</code></pre>
<p>The input matrix <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>
consists of the first 10 numeric variables:</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="classification-with-trees-and-linear-models.html#cb394-1" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</span>
<span id="cb394-2"><a href="classification-with-trees-and-linear-models.html#cb394-2" aria-hidden="true"></a><span class="kw">dim</span>(X)</span></code></pre></div>
<pre><code>## [1] 4898   10</code></pre>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="classification-with-trees-and-linear-models.html#cb396-1" aria-hidden="true"></a><span class="kw">head</span>(X, <span class="dv">2</span>) <span class="co"># first two rows</span></span></code></pre></div>
<pre><code>##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density  pH
## 1600     0.045                  45                  170   1.001 3.0
## 1601     0.049                  14                  132   0.994 3.3
##      sulphates
## 1600      0.45
## 1601      0.49</code></pre>
<p>The 11th variable measures the amount of alcohol (in %).</p>
<p>We will convert this dependent variable to a binary one:</p>
<ul>
<li>0 == (<code>alcohol  &lt; 12</code>) == lower-alcohol wines</li>
<li>1 == (<code>alcohol &gt;= 12</code>) == higher-alcohol wines</li>
</ul>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="classification-with-trees-and-linear-models.html#cb398-1" aria-hidden="true"></a><span class="co"># recall that TRUE == 1</span></span>
<span id="cb398-2"><a href="classification-with-trees-and-linear-models.html#cb398-2" aria-hidden="true"></a>Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</span>
<span id="cb398-3"><a href="classification-with-trees-and-linear-models.html#cb398-3" aria-hidden="true"></a><span class="kw">table</span>(Y)</span></code></pre></div>
<pre><code>## Y
##    0    1 
## 4085  813</code></pre>
<p>60/40% train-test split:</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="classification-with-trees-and-linear-models.html#cb400-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></span>
<span id="cb400-2"><a href="classification-with-trees-and-linear-models.html#cb400-2" aria-hidden="true"></a>random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</span>
<span id="cb400-3"><a href="classification-with-trees-and-linear-models.html#cb400-3" aria-hidden="true"></a><span class="kw">head</span>(random_indices) <span class="co"># preview</span></span></code></pre></div>
<pre><code>## [1] 2463 2511 2227  526 4291 2986</code></pre>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="classification-with-trees-and-linear-models.html#cb402-1" aria-hidden="true"></a><span class="co"># first 60% of the indices (they are arranged randomly)</span></span>
<span id="cb402-2"><a href="classification-with-trees-and-linear-models.html#cb402-2" aria-hidden="true"></a><span class="co"># will constitute the train sample:</span></span>
<span id="cb402-3"><a href="classification-with-trees-and-linear-models.html#cb402-3" aria-hidden="true"></a>train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</span>
<span id="cb402-4"><a href="classification-with-trees-and-linear-models.html#cb402-4" aria-hidden="true"></a>X_train &lt;-<span class="st"> </span>X[train_indices,]</span>
<span id="cb402-5"><a href="classification-with-trees-and-linear-models.html#cb402-5" aria-hidden="true"></a>Y_train &lt;-<span class="st"> </span>Y[train_indices]</span>
<span id="cb402-6"><a href="classification-with-trees-and-linear-models.html#cb402-6" aria-hidden="true"></a><span class="co"># the remaining indices (40%) go to the test sample:</span></span>
<span id="cb402-7"><a href="classification-with-trees-and-linear-models.html#cb402-7" aria-hidden="true"></a>X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</span>
<span id="cb402-8"><a href="classification-with-trees-and-linear-models.html#cb402-8" aria-hidden="true"></a>Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</span></code></pre></div>
<p>Let’s also compute <code>Z_train</code> and <code>Z_test</code>, being the standardised versions of <code>X_train</code>
and <code>X_test</code>, respectively.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="classification-with-trees-and-linear-models.html#cb403-1" aria-hidden="true"></a>means &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, mean) <span class="co"># column means</span></span>
<span id="cb403-2"><a href="classification-with-trees-and-linear-models.html#cb403-2" aria-hidden="true"></a>sds   &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, sd)   <span class="co"># column standard deviations</span></span>
<span id="cb403-3"><a href="classification-with-trees-and-linear-models.html#cb403-3" aria-hidden="true"></a>Z_train &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</span>
<span id="cb403-4"><a href="classification-with-trees-and-linear-models.html#cb403-4" aria-hidden="true"></a>Z_test  &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_test,  <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</span></code></pre></div>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="classification-with-trees-and-linear-models.html#cb404-1" aria-hidden="true"></a>get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</span>
<span id="cb404-2"><a href="classification-with-trees-and-linear-models.html#cb404-2" aria-hidden="true"></a>{</span>
<span id="cb404-3"><a href="classification-with-trees-and-linear-models.html#cb404-3" aria-hidden="true"></a>    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></span>
<span id="cb404-4"><a href="classification-with-trees-and-linear-models.html#cb404-4" aria-hidden="true"></a>    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb404-5"><a href="classification-with-trees-and-linear-models.html#cb404-5" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></span>
<span id="cb404-6"><a href="classification-with-trees-and-linear-models.html#cb404-6" aria-hidden="true"></a>      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></span>
<span id="cb404-7"><a href="classification-with-trees-and-linear-models.html#cb404-7" aria-hidden="true"></a>      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></span>
<span id="cb404-8"><a href="classification-with-trees-and-linear-models.html#cb404-8" aria-hidden="true"></a>      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></span>
<span id="cb404-9"><a href="classification-with-trees-and-linear-models.html#cb404-9" aria-hidden="true"></a>      <span class="co"># Confusion matrix items:</span></span>
<span id="cb404-10"><a href="classification-with-trees-and-linear-models.html#cb404-10" aria-hidden="true"></a>      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</span>
<span id="cb404-11"><a href="classification-with-trees-and-linear-models.html#cb404-11" aria-hidden="true"></a>      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb404-12"><a href="classification-with-trees-and-linear-models.html#cb404-12" aria-hidden="true"></a>    ) <span class="co"># return a named vector</span></span>
<span id="cb404-13"><a href="classification-with-trees-and-linear-models.html#cb404-13" aria-hidden="true"></a>}</span></code></pre></div>
<p>Let’s go back to the K-NN algorithm.</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="classification-with-trees-and-linear-models.html#cb405-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</span>
<span id="cb405-2"><a href="classification-with-trees-and-linear-models.html#cb405-2" aria-hidden="true"></a>Y_knn5   &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</span>
<span id="cb405-3"><a href="classification-with-trees-and-linear-models.html#cb405-3" aria-hidden="true"></a>Y_knn9   &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb405-4"><a href="classification-with-trees-and-linear-models.html#cb405-4" aria-hidden="true"></a>Y_knn5s  &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</span>
<span id="cb405-5"><a href="classification-with-trees-and-linear-models.html#cb405-5" aria-hidden="true"></a>Y_knn9s  &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</span></code></pre></div>
<p>Recall the quality metrics we have obtained previously (as a point of reference):</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="classification-with-trees-and-linear-models.html#cb406-1" aria-hidden="true"></a><span class="kw">cbind</span>(</span>
<span id="cb406-2"><a href="classification-with-trees-and-linear-models.html#cb406-2" aria-hidden="true"></a>    <span class="dt">Knn5=</span><span class="kw">get_metrics</span>(Y_knn5, Y_test),</span>
<span id="cb406-3"><a href="classification-with-trees-and-linear-models.html#cb406-3" aria-hidden="true"></a>    <span class="dt">Knn9=</span><span class="kw">get_metrics</span>(Y_knn9, Y_test),</span>
<span id="cb406-4"><a href="classification-with-trees-and-linear-models.html#cb406-4" aria-hidden="true"></a>    <span class="dt">Knn5s=</span><span class="kw">get_metrics</span>(Y_knn5s, Y_test),</span>
<span id="cb406-5"><a href="classification-with-trees-and-linear-models.html#cb406-5" aria-hidden="true"></a>    <span class="dt">Knn9s=</span><span class="kw">get_metrics</span>(Y_knn9s, Y_test)</span>
<span id="cb406-6"><a href="classification-with-trees-and-linear-models.html#cb406-6" aria-hidden="true"></a>)</span></code></pre></div>
<pre><code>##            Knn5       Knn9      Knn5s      Knn9s
## Acc     0.81735    0.81939    0.91429    0.91378
## Prec    0.38674    0.34959    0.82251    0.83636
## Rec     0.22082    0.13565    0.59937    0.58044
## F       0.28112    0.19545    0.69343    0.68529
## TN   1532.00000 1563.00000 1602.00000 1607.00000
## FN    247.00000  274.00000  127.00000  133.00000
## FP    111.00000   80.00000   41.00000   36.00000
## TP     70.00000   43.00000  190.00000  184.00000</code></pre>
<p>In this chapter we discuss the following simple and educational
(yet practically useful)
classification algorithms:</p>
<ul>
<li><em>decision trees</em>,</li>
<li><em>binary logistic regression</em>.</li>
</ul>
</div>
</div>
<div id="decision-trees" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Decision Trees</h2>
<div id="introduction-6" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Introduction</h3>
<p>Note that a K-NN classifier discussed in the previous chapter
is <strong>model-free</strong>.
The whole training set must be stored and referred to at all times.</p>
<p>Therefore, it doesn’t <em>explain</em> the data we have – we may use it solely
for the purpose of <em>prediction</em>.</p>
<p>Perhaps one of the most interpretable (and hence human-friendly) models
consist of decision rules of the form:</p>
<p><strong>IF <span class="math inline">\(x_{i,j_1}\le v_1\)</span> AND … AND <span class="math inline">\(x_{i,j_r}\le v_r\)</span> THEN <span class="math inline">\(\hat{y}_i=1\)</span>.</strong></p>
<p>These can be organised into a <strong>hierarchy</strong> for greater readability.</p>
<p>This idea inspired the notion of <strong>decision trees</strong> <span class="citation">(Breiman et al. <a href="#ref-cart" role="doc-biblioref">1984</a>)</span>.</p>
<div class="figure">
<img src="04-classification-trees_and_logistic-figures/plot_rpart-1.png" alt="" />
<p class="caption">(#fig:plot_rpart) The simplest decision tree for the synthetic 2D dataset and the corresponding decision boundaries</p>
</div>
<p>Figure <a href="classification-with-trees-and-linear-models.html#fig:plot-rpart2">4.2</a> depicts a very simple decision tree
for the aforementioned synthetic dataset.
There is only one decision boundary (based on <span class="math inline">\(X_1\)</span>) that splits
data into the “left” and “right” sides.
Each tree node reports 3 pieces of information:</p>
<ul>
<li>dominating class (0 or 1)</li>
<li>(relative) proportion of 1s represented in a node</li>
<li>(absolute) proportion of all observations in a node</li>
</ul>
<p>Figures <a href="classification-with-trees-and-linear-models.html#fig:plot-rpart2">4.2</a> and <a href="classification-with-trees-and-linear-models.html#fig:plot-rpart3">4.3</a> depict
trees with more decision rules.
Take a moment to contemplate how the corresponding decision boundaries
changed with the introduction of new decision rules.</p>
<div class="figure"><span id="fig:plot-rpart2"></span>
<img src="04-classification-trees_and_logistic-figures/plot-rpart2-1.png" alt="" />
<p class="caption">Figure 4.2:  A more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries</p>
</div>
<div class="figure"><span id="fig:plot-rpart3"></span>
<img src="04-classification-trees_and_logistic-figures/plot-rpart3-1.png" alt="" />
<p class="caption">Figure 4.3:  An even more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries</p>
</div>
</div>
<div id="example-in-r-1" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Example in R</h3>
<p>We will use the <code>rpart()</code> function from the <code>rpart</code> package
to build a classification tree.</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="classification-with-trees-and-linear-models.html#cb408-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb408-2"><a href="classification-with-trees-and-linear-models.html#cb408-2" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rpart.plot&quot;</span>)</span>
<span id="cb408-3"><a href="classification-with-trees-and-linear-models.html#cb408-3" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span></code></pre></div>
<p><code>rpart()</code> uses a formula (<code>~</code>) interface, hence it will be easier
to feed it with data in a data.frame form.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="classification-with-trees-and-linear-models.html#cb409-1" aria-hidden="true"></a>XY_train &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(X_train), <span class="dt">Y=</span>Y_train)</span>
<span id="cb409-2"><a href="classification-with-trees-and-linear-models.html#cb409-2" aria-hidden="true"></a>XY_test &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(X_test), <span class="dt">Y=</span>Y_test)</span></code></pre></div>
<p>Fit and plot a decision tree, see Figure <a href="classification-with-trees-and-linear-models.html#fig:plot-rpart1">4.4</a>.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="classification-with-trees-and-linear-models.html#cb410-1" aria-hidden="true"></a>t1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>., <span class="dt">data=</span>XY_train, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb410-2"><a href="classification-with-trees-and-linear-models.html#cb410-2" aria-hidden="true"></a><span class="kw">rpart.plot</span>(t1, <span class="dt">tweak=</span><span class="fl">1.1</span>, <span class="dt">fallen.leaves=</span><span class="ot">FALSE</span>, <span class="dt">digits=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:plot-rpart1"></span>
<img src="04-classification-trees_and_logistic-figures/plot-rpart1-1.png" alt="" />
<p class="caption">Figure 4.4:  A decision tree for the <code>wines</code> dataset</p>
</div>
<p>We can build less or more complex trees by playing
with the <code>cp</code> parameter, see Figures <a href="classification-with-trees-and-linear-models.html#fig:plot-rpart222">4.5</a>
and <a href="classification-with-trees-and-linear-models.html#fig:tree333">4.6</a>.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="classification-with-trees-and-linear-models.html#cb411-1" aria-hidden="true"></a><span class="co"># cp = complexity parameter, smaller → more complex tree</span></span>
<span id="cb411-2"><a href="classification-with-trees-and-linear-models.html#cb411-2" aria-hidden="true"></a>t2 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>., <span class="dt">data=</span>XY_train, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">cp=</span><span class="fl">0.1</span>)</span>
<span id="cb411-3"><a href="classification-with-trees-and-linear-models.html#cb411-3" aria-hidden="true"></a><span class="kw">rpart.plot</span>(t2, <span class="dt">tweak=</span><span class="fl">1.1</span>, <span class="dt">fallen.leaves=</span><span class="ot">FALSE</span>, <span class="dt">digits=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:plot-rpart222"></span>
<img src="04-classification-trees_and_logistic-figures/plot-rpart222-1.png" alt="" />
<p class="caption">Figure 4.5:  A (simpler) decision tree for the <code>wines</code> dataset</p>
</div>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="classification-with-trees-and-linear-models.html#cb412-1" aria-hidden="true"></a><span class="co"># cp = complexity parameter, smaller → more complex tree</span></span>
<span id="cb412-2"><a href="classification-with-trees-and-linear-models.html#cb412-2" aria-hidden="true"></a>t3 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>., <span class="dt">data=</span>XY_train, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">cp=</span><span class="fl">0.00001</span>)</span>
<span id="cb412-3"><a href="classification-with-trees-and-linear-models.html#cb412-3" aria-hidden="true"></a><span class="kw">rpart.plot</span>(t3, <span class="dt">tweak=</span><span class="fl">1.1</span>, <span class="dt">fallen.leaves=</span><span class="ot">FALSE</span>, <span class="dt">digits=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:tree333"></span>
<img src="04-classification-trees_and_logistic-figures/tree333-1.png" alt="" />
<p class="caption">Figure 4.6:  A (more complex) decision tree for the <code>wines</code> dataset</p>
</div>
<p>Trees with few decision rules actually are very nicely interpretable.
On the other hand, plotting of the complex ones is just hopeless;
we should treat them as “black boxes” instead.</p>
<!--
The fitted model is rather... simple.

Only the `alcohol` variable is taken into account.


Well note how these two distributions are shifted:


```r
#    horizontal=TRUE)
```





-->
<p>Let’s make some predictions:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="classification-with-trees-and-linear-models.html#cb413-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t1, XY_test, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb413-2"><a href="classification-with-trees-and-linear-models.html#cb413-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.92857    0.80623    0.73502    0.76898 1587.00000   84.00000 
##         FP         TP 
##   56.00000  233.00000</code></pre>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="classification-with-trees-and-linear-models.html#cb415-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t2, XY_test, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb415-2"><a href="classification-with-trees-and-linear-models.html#cb415-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.90255    0.83871    0.49211    0.62028 1613.00000  161.00000 
##         FP         TP 
##   30.00000  156.00000</code></pre>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="classification-with-trees-and-linear-models.html#cb417-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t3, XY_test, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb417-2"><a href="classification-with-trees-and-linear-models.html#cb417-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.91837    0.73433    0.77603    0.75460 1554.00000   71.00000 
##         FP         TP 
##   89.00000  246.00000</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Interestingly, <code>rpart()</code> also provides us with information
about the importance degrees of each independent variable.</p>
</dd>
</dl>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="classification-with-trees-and-linear-models.html#cb419-1" aria-hidden="true"></a>t1<span class="op">$</span>variable.importance<span class="op">/</span><span class="kw">sum</span>(t1<span class="op">$</span>variable.importance)</span></code></pre></div>
<pre><code>##              density       residual.sugar        fixed.acidity 
##            0.6562490            0.1984221            0.0305167 
##            chlorides                   pH     volatile.acidity 
##            0.0215008            0.0209678            0.0192880 
##            sulphates total.sulfur.dioxide          citric.acid 
##            0.0184293            0.0140482            0.0119201 
##  free.sulfur.dioxide 
##            0.0086579</code></pre>
</div>
<div id="a-note-on-decision-tree-learning" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> A Note on Decision Tree Learning</h3>
<p>Learning an optimal decision tree is a computationally hard problem
– we need some heuristics.</p>
<p>Examples:</p>
<ul>
<li>ID3 (Iterative Dichotomiser 3) <span class="citation">(Quinlan <a href="#ref-id3" role="doc-biblioref">1986</a>)</span></li>
<li>C4.5 algorithm <span class="citation">(Quinlan <a href="#ref-c45" role="doc-biblioref">1993</a>)</span></li>
<li>CART by Leo Breiman et al., <span class="citation">(Breiman et al. <a href="#ref-cart" role="doc-biblioref">1984</a>)</span></li>
</ul>
<p>(**) Decision trees are most often constructed by a <em>greedy</em>, <em>top-down</em>
<em>recursive partitioning</em>, see., e.g., <span class="citation">(Therneau &amp; Atkinson <a href="#ref-rpart" role="doc-biblioref">2019</a>)</span>.</p>
<!--

## TODO Random Forests and XGBoost

## ...





just show how to use it

boosting

bagging


```r
#library("randomForest")
#rf <- randomForest(X_train, Y_train)
#Y_pred <- predict(rf, X_test)
#get_metrics(Y_pred, Y_test)
```







```r
#library("xgboost")
#xg <- xgboost(X_train, Y_train)
#Y_pred <- predict(xg, X_test)
#get_metrics(Y_pred, Y_test)
```

-->
</div>
</div>
<div id="binary-logistic-regression" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Binary Logistic Regression</h2>
<div id="motivation" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Motivation</h3>
<p>Recall that for a regression task, we fitted a very simple family of models
– the linear ones – by minimising the sum of squared residuals.</p>
<p>This approach was pretty effective.</p>
<p>(Very) theoretically, we could treat the class labels as numeric <span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s
and apply regression models in a binary classification task.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="classification-with-trees-and-linear-models.html#cb421-1" aria-hidden="true"></a>XY_train_r &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(X_train),</span>
<span id="cb421-2"><a href="classification-with-trees-and-linear-models.html#cb421-2" aria-hidden="true"></a>    <span class="dt">Y=</span><span class="kw">as.numeric</span>(Y_train)<span class="op">-</span><span class="dv">1</span> <span class="co"># 0.0 or 1.0</span></span>
<span id="cb421-3"><a href="classification-with-trees-and-linear-models.html#cb421-3" aria-hidden="true"></a>)</span>
<span id="cb421-4"><a href="classification-with-trees-and-linear-models.html#cb421-4" aria-hidden="true"></a>XY_test_r &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(X_test),</span>
<span id="cb421-5"><a href="classification-with-trees-and-linear-models.html#cb421-5" aria-hidden="true"></a>    <span class="dt">Y=</span><span class="kw">as.numeric</span>(Y_test)<span class="op">-</span><span class="dv">1</span> <span class="co"># 0.0 or 1.0</span></span>
<span id="cb421-6"><a href="classification-with-trees-and-linear-models.html#cb421-6" aria-hidden="true"></a>)</span>
<span id="cb421-7"><a href="classification-with-trees-and-linear-models.html#cb421-7" aria-hidden="true"></a>f_r &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>density<span class="op">+</span>residual.sugar<span class="op">+</span>pH, <span class="dt">data=</span>XY_train_r)</span></code></pre></div>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="classification-with-trees-and-linear-models.html#cb422-1" aria-hidden="true"></a>Y_pred_r &lt;-<span class="st"> </span><span class="kw">predict</span>(f_r, XY_test_r)</span>
<span id="cb422-2"><a href="classification-with-trees-and-linear-models.html#cb422-2" aria-hidden="true"></a><span class="kw">summary</span>(Y_pred_r)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -3.0468 -0.0211  0.1192  0.1645  0.3491  0.8892</code></pre>
<p>The predicted outputs, <span class="math inline">\(\hat{Y}\)</span>, are arbitrary real numbers,
but we can convert them to binary ones by checking if, e.g., <span class="math inline">\(\hat{Y}&gt;0.5\)</span>.</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="classification-with-trees-and-linear-models.html#cb424-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Y_pred_r<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb424-2"><a href="classification-with-trees-and-linear-models.html#cb424-2" aria-hidden="true"></a><span class="kw">round</span>(<span class="kw">get_metrics</span>(Y_pred, XY_test_r<span class="op">$</span>Y), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##    0.927    0.865    0.647    0.740 1611.000  112.000   32.000  205.000</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>(*) The threshold <span class="math inline">\(T=0.5\)</span> could even be treated as a free parameter
we optimise for (w.r.t. different metrics over the validation sample),
see Figure <a href="classification-with-trees-and-linear-models.html#fig:lm3b">4.7</a>.</p>
</dd>
</dl>
<div class="figure"><span id="fig:lm3b"></span>
<img src="04-classification-trees_and_logistic-figures/lm3b-1.png" alt="" />
<p class="caption">Figure 4.7:  Quality metrics for a binary classifier “Classify X as 1 if <span class="math inline">\(f(X)&gt;T\)</span> and as 0 if <span class="math inline">\(f(X)\le T\)</span>”</p>
</div>
<p>Despite we can, we shouldn’t use linear regression for classification.
Treating class labels “0” and “1” as ordinary real numbers just doesn’t
cut it – we intuitively feel that we are doing something <em>ugly</em>.
Luckily, there is a better, more meaningful approach that
still relies on a linear model, but has the <em>right</em> semantics.</p>
</div>
<div id="logistic-model" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Logistic Model</h3>
<p>Inspired by this idea, we could try modelling
the <strong><em>probability</em> that a given point belongs to class <span class="math inline">\(1\)</span></strong>.</p>
<p>This could also provide us with the <em>confidence</em> in our prediction.</p>
<p>Probability is a number in <span class="math inline">\([0,1]\)</span>, but the outputs of a linear model are arbitrary real numbers.</p>
<p>However, we could transform those real-valued outputs by means
of some function <span class="math inline">\(\phi:\mathbb{R}\to[0,1]\)</span> (preferably S-shaped == sigmoid),
so as to get:</p>
<p><span class="math display">\[
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=\phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p).
\]</span></p>
<dl>
<dt>Remark.</dt>
<dd><p>The above reads as “Probability that <span class="math inline">\(Y\)</span> is from class 1 given <span class="math inline">\(\mathbf{X}\)</span>
and <span class="math inline">\(\boldsymbol\beta\)</span>”.</p>
</dd>
</dl>
<p>A popular choice is the <strong>logistic sigmoid function</strong>,
see Figure <a href="classification-with-trees-and-linear-models.html#fig:sigmoid">4.8</a>:</p>
<p><span class="math display">\[
\phi(t) = \frac{1}{1+e^{-t}} = \frac{e^t}{1+e^t}.
\]</span></p>
<div class="figure"><span id="fig:sigmoid"></span>
<img src="04-classification-trees_and_logistic-figures/sigmoid-1.png" alt="" />
<p class="caption">Figure 4.8:  The logistic sigmoid function, <span class="math inline">\(\varphi\)</span></p>
</div>
<p>Hence our model becomes:</p>
<p><span class="math display">\[
Y=\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
\]</span></p>
<p>It is an instance of a <strong>generalised linear model</strong> (glm)
(there are of course many other possible generalisations).</p>
</div>
<div id="example-in-r-2" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Example in R</h3>
<p>Let us first fit a simple (i.e., <span class="math inline">\(p=1\)</span>) logistic regression model
using the <code>density</code> variable. The goodness-of-fit measure used in this
problem will be discussed a bit later.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="classification-with-trees-and-linear-models.html#cb426-1" aria-hidden="true"></a>(f &lt;-<span class="st"> </span><span class="kw">glm</span>(Y<span class="op">~</span>density, <span class="dt">data=</span>XY_train, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)))</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = Y ~ density, family = binomial(&quot;logit&quot;), data = XY_train)
## 
## Coefficients:
## (Intercept)      density  
##        1173        -1184  
## 
## Degrees of Freedom: 2937 Total (i.e. Null);  2936 Residual
## Null Deviance:       2670 
## Residual Deviance: 1420  AIC: 1420</code></pre>
<p>“logit” above denotes the inverse of the logistic sigmoid function.
The fitted coefficients are equal to:</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="classification-with-trees-and-linear-models.html#cb428-1" aria-hidden="true"></a>f<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)     density 
##      1173.2     -1184.2</code></pre>
<p>Figure <a href="classification-with-trees-and-linear-models.html#fig:glm2">4.9</a> depicts the obtained model, which can be written
as:
<span class="math display">\[
\Pr(Y=1|x)=\displaystyle\frac{1}{1+e^{-\left(
1173.21-1184.21x
\right)}
}
\]</span>
with <span class="math inline">\(x=\text{density}\)</span>.</p>
<div class="figure"><span id="fig:glm2"></span>
<img src="04-classification-trees_and_logistic-figures/glm2-1.png" alt="" />
<p class="caption">Figure 4.9:  The probability that a given wine is a high-alcohol one given its density; black and red points denote the actual observed data points from the class 0 and 1, respectively</p>
</div>
<p>Some predicted probabilities:</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="classification-with-trees-and-linear-models.html#cb430-1" aria-hidden="true"></a><span class="kw">round</span>(<span class="kw">head</span>(<span class="kw">predict</span>(f, XY_test, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>), <span class="dv">12</span>), <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## 1602 1605 1607 1608 1609 1613 1614 1615 1621 1622 1623 1627 
## 0.01 0.01 0.00 0.02 0.03 0.36 0.00 0.31 0.36 0.06 0.03 0.00</code></pre>
<p>We classify <span class="math inline">\(Y\)</span> as 1 if the corresponding membership probability
is greater than <span class="math inline">\(0.5\)</span>.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="classification-with-trees-and-linear-models.html#cb432-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(f, XY_test, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb432-2"><a href="classification-with-trees-and-linear-models.html#cb432-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.89796    0.72763    0.58991    0.65157 1573.00000  130.00000 
##         FP         TP 
##   70.00000  187.00000</code></pre>
<p>And now a fit based on some other input variables:</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="classification-with-trees-and-linear-models.html#cb434-1" aria-hidden="true"></a>(f &lt;-<span class="st"> </span><span class="kw">glm</span>(Y<span class="op">~</span>density<span class="op">+</span>residual.sugar<span class="op">+</span>total.sulfur.dioxide,</span>
<span id="cb434-2"><a href="classification-with-trees-and-linear-models.html#cb434-2" aria-hidden="true"></a>    <span class="dt">data=</span>XY_train, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)))</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = Y ~ density + residual.sugar + total.sulfur.dioxide, 
##     family = binomial(&quot;logit&quot;), data = XY_train)
## 
## Coefficients:
##          (Intercept)               density        residual.sugar  
##             2.50e+03             -2.53e+03              8.58e-01  
## total.sulfur.dioxide  
##             9.74e-03  
## 
## Degrees of Freedom: 2937 Total (i.e. Null);  2934 Residual
## Null Deviance:       2670 
## Residual Deviance: 920   AIC: 928</code></pre>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="classification-with-trees-and-linear-models.html#cb436-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(f, XY_test, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb436-2"><a href="classification-with-trees-and-linear-models.html#cb436-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.93214    0.82394    0.73817    0.77870 1593.00000   83.00000 
##         FP         TP 
##   50.00000  234.00000</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Try fitting different models based on other sets of features.</p>
</div>
</div>
<div id="loss-function-cross-entropy" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Loss Function: Cross-entropy</h3>
<p>The fitting of the model can be written as an optimisation task:</p>
<p><span class="math display">\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\frac{1}{n} \sum_{i=1}^n
\epsilon\left(\hat{y}_i, y_i \right)
\]</span></p>
<p>where <span class="math inline">\(\epsilon(\hat{y}_i, y_i)\)</span> denotes the penalty that measures the
“difference” between the true <span class="math inline">\(y_i\)</span> and its predicted version
<span class="math inline">\(\hat{y}_i=\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)\)</span>.</p>
<p>In the ordinary regression, we used the squared residual
<span class="math inline">\(\epsilon(\hat{y}_i, y_i) = (\hat{y}_i-y_i)^2\)</span>.
In <strong>logistic regression</strong> (the kind of a classifier we are
interested in right now), we use
the <strong>cross-entropy</strong> (a.k.a. <strong>log-loss</strong>, binary cross-entropy),</p>
<p><span class="math display">\[
\epsilon(\hat{y}_i,y_i) = - \left(y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i)\right)
\]</span></p>
<p>The corresponding loss function has not only
many nice statistical properties (** related to maximum likelihood
estimation etc.)
but also an intuitive interpretation.</p>
<p>Note that the predicted <span class="math inline">\(\hat{y}_i\)</span> is in <span class="math inline">\((0,1)\)</span> and the true <span class="math inline">\(y_i\)</span>
equals to either 0 or 1.
Recall also that <span class="math inline">\(\log t\in(-\infty, 0)\)</span> for <span class="math inline">\(t\in (0,1)\)</span>.
Therefore, the formula for <span class="math inline">\(\epsilon(\hat{y}_i,y_i)\)</span>
has a very intuitive behaviour:</p>
<ul>
<li><p>if true <span class="math inline">\(y_i=1\)</span>, then the penalty becomes <span class="math inline">\(\epsilon(\hat{y}_i, 1) = -\log(\hat{y}_i)\)</span></p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> is the probability that the classified input is indeed from class <span class="math inline">\(1\)</span></li>
<li>we’d be happy if the classifier outputted <span class="math inline">\(\hat{y}_i\simeq 1\)</span> in this case;
this is not penalised as <span class="math inline">\(-\log(t)\to 0\)</span> as <span class="math inline">\(t\to 1\)</span></li>
<li>however, if the classifier is totally wrong, i.e., it thinks that
<span class="math inline">\(\hat{y}_i\simeq 0\)</span>, then the penalty will be very high, as <span class="math inline">\(-\log(t)\to+\infty\)</span>
as <span class="math inline">\(t\to 0\)</span></li>
</ul></li>
<li><p>if true <span class="math inline">\(y_i=0\)</span>, then the penalty becomes <span class="math inline">\(\epsilon(\hat{y}_i, 0) = -\log(1-\hat{y}_i)\)</span></p>
<ul>
<li><span class="math inline">\(1-\hat{y}_i\)</span> is the predicted probability that the input is from class <span class="math inline">\(0\)</span></li>
<li>we penalise heavily the case where <span class="math inline">\(1-\hat{y}_i\)</span> is small (we’d be happy
if the classifier was sure that <span class="math inline">\(1-\hat{y}_i\simeq 1\)</span>, because this is the ground-truth)</li>
</ul></li>
</ul>
<div style="margin-top: 1em">

</div>
<p>(*) Having said that, let’s expand the above formulae.
The task of minimising cross-entropy in the binary logistic regression
can be written as <span class="math inline">\(\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}} E(\boldsymbol\beta)\)</span>
with:</p>
<p><span class="math display">\[
E(\boldsymbol\beta)=
-\frac{1}{n} \sum_{i=1}^n
y_i \log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
+ (1-y_i) \log(1-\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta))
\]</span></p>
<p>Taking into account that:</p>
<p><span class="math display">\[
\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)=
\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}},
\]</span></p>
<p>we get:</p>
<p><span class="math display">\[
E(\boldsymbol\beta)=
-\frac{1}{n}
\sum_{i=1}^n \left(
\begin{array}{r}
y_i \log \displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}\\
+
(1-y_i) \log \displaystyle\frac{e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}
\end{array}
\right).
\]</span></p>
<p>Logarithms are really practitioner-friendly functions,
it holds:</p>
<ul>
<li><span class="math inline">\(\log 1=0\)</span>,</li>
<li><span class="math inline">\(\log e=1\)</span> (where <span class="math inline">\(e \simeq 2.71828\)</span> is the Euler constant;
note that by writing <span class="math inline">\(\log\)</span> we mean the natural a.k.a. base-<span class="math inline">\(e\)</span> logarithm),</li>
<li><span class="math inline">\(\log xy = \log x + \log y\)</span>,</li>
<li><span class="math inline">\(\log x^p = p\log x\)</span> (this is <span class="math inline">\(\log (x^p)\)</span>, not <span class="math inline">\((\log x)^p\)</span>).</li>
</ul>
<p>These facts imply, amongst others that:</p>
<ul>
<li><span class="math inline">\(\log e^x = x \log e = x\)</span>,</li>
<li><span class="math inline">\(\log \frac{x}{y} = \log x y^{-1} = \log x+\log y^{-1} = \log x - \log y\)</span>
(of course for <span class="math inline">\(y\neq 0\)</span>),</li>
<li><span class="math inline">\(\log \frac{1}{y} = -\log y\)</span></li>
</ul>
<p>and so forth. Therefore,
based on the fact that
<span class="math inline">\(1/(1+e^{-x})=e^x/(1+e^x)\)</span>,
the above optimisation problem can be rewritten as:</p>
<p><span class="math display">\[
E(\boldsymbol\beta)=
\frac{1}{n}
\sum_{i=1}^n \left(
\begin{array}{r}
y_i \log \left(1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)\\
+
(1-y_i) \log \left(1+e^{+(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)
\end{array}
\right)
\]</span></p>
<p>or, if someone prefers:</p>
<p><span class="math display">\[
E(\boldsymbol\beta)=
\frac{1}{n}
\sum_{i=1}^n \left(
(1-y_i)\left(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)
+\log \left(1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)
\right).
\]</span></p>
<p>It turns out that there is no analytical formula
for the optimal set of parameters (<span class="math inline">\(\beta_0,\beta_1,\dots,\beta_p\)</span>
minimising the log-loss).
In the chapter on optimisation, we shall see that
the solution to the logistic regression can be solved numerically
by means of quite simple iterative algorithms.
The two expanded formulae have lost the appealing interpretation
of the original one, however, it’s more numerically well-behaving,
see, e.g., the <code>log1p()</code> function in base R or, even better,
<code>fermi_dirac_0()</code> in the <code>gsl</code> package.</p>
<!-- TODO: cite Goldberg's paper -->
</div>
</div>
<div id="exercises-in-r-2" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Exercises in R</h2>
<div id="edstats-preparing-data" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> EdStats – Preparing Data</h3>
<p>In this exercise, we will prepare the EdStats dataset
for further analysis.
The file <code>edstats_2019.csv</code> provides us with many
country-level Education Statistics extracted from the World Bank’s Databank,
see <a href="https://databank.worldbank.org/" class="uri">https://databank.worldbank.org/</a>.
Databank aggregates information from such sources as
the UNESCO Institute for Statistics,
OECD Programme for International Student Assessment (PISA)
etc. The official description reads:</p>
<blockquote>
<p>“The World Bank EdStats Query holds around 2,500 internationally comparable
education indicators for access, progression, completion, literacy, teachers,
population, and expenditures. The indicators cover the education cycle from
pre-primary to tertiary education. The query also holds learning outcome data
from international learning assessments (PISA, TIMSS, etc.), equity data
from household surveys, and projection data to 2050.”</p>
</blockquote>
<p><code>edstats_2019.csv</code> was compiled on 24 April 2020 and lists
indicators reported between 2010 and 2019.
First, let’s load the dataset:</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="classification-with-trees-and-linear-models.html#cb438-1" aria-hidden="true"></a>edstats_<span class="dv">2019</span> &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019.csv&quot;</span>,</span>
<span id="cb438-2"><a href="classification-with-trees-and-linear-models.html#cb438-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb438-3"><a href="classification-with-trees-and-linear-models.html#cb438-3" aria-hidden="true"></a><span class="kw">head</span>(edstats_<span class="dv">2019</span>)</span></code></pre></div>
<pre><code>##   CountryName CountryCode
## 1 Afghanistan         AFG
## 2 Afghanistan         AFG
## 3 Afghanistan         AFG
## 4 Afghanistan         AFG
## 5 Afghanistan         AFG
## 6 Afghanistan         AFG
##                                                    Series
## 1     Government expenditure on education as % of GDP (%)
## 2              Gross enrolment ratio, primary, female (%)
## 3                 Net enrolment rate, primary, female (%)
## 4                 Primary completion rate, both sexes (%)
## 5         PISA: Mean performance on the mathematics scale
## 6 PISA: Mean performance on the mathematics scale. Female
##                Code   Y2010  Y2011   Y2012   Y2013   Y2014   Y2015
## 1 SE.XPD.TOTL.GD.ZS  3.4794  3.462  2.6042  3.4545  3.6952  3.2558
## 2    SE.PRM.ENRR.FE 80.6355 80.937 86.3288 85.9021 86.7296 83.5044
## 3    SE.PRM.NENR.FE      NA     NA      NA      NA      NA      NA
## 4    SE.PRM.CMPT.ZS      NA     NA      NA      NA      NA      NA
## 5       LO.PISA.MAT      NA     NA      NA      NA      NA      NA
## 6    LO.PISA.MAT.FE      NA     NA      NA      NA      NA      NA
##     Y2016   Y2017  Y2018 Y2019
## 1  4.2284  4.0589     NA    NA
## 2 82.5584 82.0803 82.850    NA
## 3      NA      NA     NA    NA
## 4 79.9346 84.4150 85.625    NA
## 5      NA      NA     NA    NA
## 6      NA      NA     NA    NA</code></pre>
<p>This data frame is in a “long” format, where each indicator
for each country is given in its own row. Note that some indicators
are not surveyed/updated every year.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Convert <code>edstats_2019</code> to a “wide” format (one row per country,
each indicator in its own column) based on the most recent observed indicators.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>First we need a function that returns the last non-missing value in a
given numeric vector.
To recall, <code>na.omit()</code>, removes all missing values and <code>tail()</code> can be
used to access the last observation easily. Unfortunately, if the vector
is consists of missing values only, the removal of <code>NA</code>s leads
to an empty sequence. However, the trick we can use is that
by extracting the first element from an empty vector by using
<code>[...]</code>, we get a <code>NA</code>.</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="classification-with-trees-and-linear-models.html#cb440-1" aria-hidden="true"></a>last_non_na &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">tail</span>(<span class="kw">na.omit</span>(x), <span class="dv">1</span>)[<span class="dv">1</span>]</span>
<span id="cb440-2"><a href="classification-with-trees-and-linear-models.html#cb440-2" aria-hidden="true"></a><span class="kw">last_non_na</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="ot">NA</span>, <span class="dv">3</span>,<span class="ot">NA</span>,<span class="ot">NA</span>)) <span class="co"># example 1</span></span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="classification-with-trees-and-linear-models.html#cb442-1" aria-hidden="true"></a><span class="kw">last_non_na</span>(<span class="kw">c</span>(<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>)) <span class="co"># example 2</span></span></code></pre></div>
<pre><code>## [1] NA</code></pre>
<p>Let’s extract the most recent indicator from each row in <code>edstats_2019</code>.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="classification-with-trees-and-linear-models.html#cb444-1" aria-hidden="true"></a>values &lt;-<span class="st"> </span><span class="kw">apply</span>(edstats_<span class="dv">2019</span>[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)], <span class="dv">1</span>, last_non_na)</span>
<span id="cb444-2"><a href="classification-with-trees-and-linear-models.html#cb444-2" aria-hidden="true"></a><span class="kw">head</span>(values)</span></code></pre></div>
<pre><code>## [1]  4.0589 82.8503      NA 85.6253      NA      NA</code></pre>
<p>Now, we shall create a data frame with 3 columns:
name of the country, indicator code, indicator value.
Let’s order it with respect to the first two columns.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="classification-with-trees-and-linear-models.html#cb446-1" aria-hidden="true"></a>edstats_<span class="dv">2019</span> &lt;-<span class="st"> </span>edstats_<span class="dv">2019</span>[<span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;Code&quot;</span>)]</span>
<span id="cb446-2"><a href="classification-with-trees-and-linear-models.html#cb446-2" aria-hidden="true"></a><span class="co"># add a new column at the righthand end:</span></span>
<span id="cb446-3"><a href="classification-with-trees-and-linear-models.html#cb446-3" aria-hidden="true"></a>edstats_<span class="dv">2019</span>[<span class="st">&quot;Value&quot;</span>] &lt;-<span class="st"> </span>values</span>
<span id="cb446-4"><a href="classification-with-trees-and-linear-models.html#cb446-4" aria-hidden="true"></a>edstats_<span class="dv">2019</span> &lt;-<span class="st"> </span>edstats_<span class="dv">2019</span>[</span>
<span id="cb446-5"><a href="classification-with-trees-and-linear-models.html#cb446-5" aria-hidden="true"></a>    <span class="kw">order</span>(edstats_<span class="dv">2019</span><span class="op">$</span>CountryName, edstats_<span class="dv">2019</span><span class="op">$</span>Code), ]</span>
<span id="cb446-6"><a href="classification-with-trees-and-linear-models.html#cb446-6" aria-hidden="true"></a><span class="kw">head</span>(edstats_<span class="dv">2019</span>)</span></code></pre></div>
<pre><code>##    CountryName           Code  Value
## 59 Afghanistan    HD.HCI.AMRT 0.7797
## 57 Afghanistan HD.HCI.AMRT.FE 0.8018
## 58 Afghanistan HD.HCI.AMRT.MA 0.7597
## 53 Afghanistan    HD.HCI.EYRS 8.5800
## 51 Afghanistan HD.HCI.EYRS.FE 6.7300
## 52 Afghanistan HD.HCI.EYRS.MA 9.2100</code></pre>
<p>To convert the data frame to a “wide” format, many readers would choose
the <code>pivot_wider()</code> function from the <code>tidyr</code> package
(amongst others).</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="classification-with-trees-and-linear-models.html#cb448-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;tidyr&quot;</span>)</span>
<span id="cb448-2"><a href="classification-with-trees-and-linear-models.html#cb448-2" aria-hidden="true"></a>edstats &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(</span>
<span id="cb448-3"><a href="classification-with-trees-and-linear-models.html#cb448-3" aria-hidden="true"></a>    <span class="kw">pivot_wider</span>(edstats_<span class="dv">2019</span>, <span class="dt">names_from=</span><span class="st">&quot;Code&quot;</span>, <span class="dt">values_from=</span><span class="st">&quot;Value&quot;</span>)</span>
<span id="cb448-4"><a href="classification-with-trees-and-linear-models.html#cb448-4" aria-hidden="true"></a>)</span>
<span id="cb448-5"><a href="classification-with-trees-and-linear-models.html#cb448-5" aria-hidden="true"></a>edstats[<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>]</span></code></pre></div>
<pre><code>##   CountryName HD.HCI.AMRT HD.HCI.AMRT.FE HD.HCI.AMRT.MA HD.HCI.EYRS
## 1 Afghanistan      0.7797         0.8018         0.7597        8.58
##   HD.HCI.EYRS.FE HD.HCI.EYRS.MA
## 1           6.73           9.21</code></pre>
<div style="margin-top: 1em">

</div>
<p>On a side note (*), the above solution is of course perfectly fine
and we can now live long and prosper.
Nevertheless, we are here to learn new skills, so let’s note
that it has the drawback that it required
us to search for the answer on the internet (and go through many “answers”
that actually don’t work). If we are not converting between the long and the
wide formats on a daily basis, this might not be worth the hassle
(moreover, there’s no guarantee that this function will work the same
way in the future, that the package we relied on will provide the same API etc.).</p>
<p>Instead, by relaying on a bit deeper knowledge of R programming
(which we already have, see Appendices A-D of our book),
we could implement the relevant procedure manually. The downside is that
this requires us to get out of our comfort zone and… think.</p>
<p>First, let’s generate the list of all countries and indicators:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="classification-with-trees-and-linear-models.html#cb450-1" aria-hidden="true"></a>countries  &lt;-<span class="st"> </span><span class="kw">unique</span>(edstats_<span class="dv">2019</span><span class="op">$</span>CountryName)</span>
<span id="cb450-2"><a href="classification-with-trees-and-linear-models.html#cb450-2" aria-hidden="true"></a><span class="kw">head</span>(countries)</span></code></pre></div>
<pre><code>## [1] &quot;Afghanistan&quot;    &quot;Albania&quot;        &quot;Algeria&quot;        &quot;American Samoa&quot;
## [5] &quot;Andorra&quot;        &quot;Angola&quot;</code></pre>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="classification-with-trees-and-linear-models.html#cb452-1" aria-hidden="true"></a>indicators &lt;-<span class="st"> </span><span class="kw">unique</span>(edstats_<span class="dv">2019</span><span class="op">$</span>Code)</span>
<span id="cb452-2"><a href="classification-with-trees-and-linear-models.html#cb452-2" aria-hidden="true"></a><span class="kw">head</span>(indicators)</span></code></pre></div>
<pre><code>## [1] &quot;HD.HCI.AMRT&quot;    &quot;HD.HCI.AMRT.FE&quot; &quot;HD.HCI.AMRT.MA&quot; &quot;HD.HCI.EYRS&quot;   
## [5] &quot;HD.HCI.EYRS.FE&quot; &quot;HD.HCI.EYRS.MA&quot;</code></pre>
<p>Second, note that <code>edstats_2019</code> gives all the possible combinations (pairs)
of the indexes and countries:</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="classification-with-trees-and-linear-models.html#cb454-1" aria-hidden="true"></a><span class="kw">nrow</span>(edstats_<span class="dv">2019</span>) <span class="co"># number of rows in edstats_2019</span></span></code></pre></div>
<pre><code>## [1] 23852</code></pre>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="classification-with-trees-and-linear-models.html#cb456-1" aria-hidden="true"></a><span class="kw">length</span>(countries)<span class="op">*</span><span class="kw">length</span>(indicators) <span class="co"># number of pairs</span></span></code></pre></div>
<pre><code>## [1] 23852</code></pre>
<p>Looking at the numbers in the <code>Value</code> column of <code>edstats_2019</code>,
this will exactly provide us with our desired “wide” data matrix,
if we read it in a rowwise manner. Hence, we can use
<code>matrix(..., byrow=TRUE)</code> to generate it:</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="classification-with-trees-and-linear-models.html#cb458-1" aria-hidden="true"></a><span class="co"># edstats_2019 is already sorted w.r.t. CountryName and Code</span></span>
<span id="cb458-2"><a href="classification-with-trees-and-linear-models.html#cb458-2" aria-hidden="true"></a>edstats2 &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb458-3"><a href="classification-with-trees-and-linear-models.html#cb458-3" aria-hidden="true"></a>    <span class="dt">CountryName=</span>countries, <span class="co"># first column</span></span>
<span id="cb458-4"><a href="classification-with-trees-and-linear-models.html#cb458-4" aria-hidden="true"></a>    <span class="kw">as.data.frame</span>(</span>
<span id="cb458-5"><a href="classification-with-trees-and-linear-models.html#cb458-5" aria-hidden="true"></a>        <span class="kw">matrix</span>(edstats_<span class="dv">2019</span><span class="op">$</span>Value,</span>
<span id="cb458-6"><a href="classification-with-trees-and-linear-models.html#cb458-6" aria-hidden="true"></a>            <span class="dt">byrow=</span><span class="ot">TRUE</span>,</span>
<span id="cb458-7"><a href="classification-with-trees-and-linear-models.html#cb458-7" aria-hidden="true"></a>            <span class="dt">ncol=</span><span class="kw">length</span>(indicators),</span>
<span id="cb458-8"><a href="classification-with-trees-and-linear-models.html#cb458-8" aria-hidden="true"></a>            <span class="dt">dimnames=</span><span class="kw">list</span>(<span class="ot">NULL</span>, indicators)</span>
<span id="cb458-9"><a href="classification-with-trees-and-linear-models.html#cb458-9" aria-hidden="true"></a>    )))</span>
<span id="cb458-10"><a href="classification-with-trees-and-linear-models.html#cb458-10" aria-hidden="true"></a><span class="kw">identical</span>(edstats, edstats2)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Export <code>edstats</code> to a CSV file.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>This can be done as follows:</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="classification-with-trees-and-linear-models.html#cb460-1" aria-hidden="true"></a><span class="kw">write.csv</span>(edstats, <span class="st">&quot;edstats_2019_wide.csv&quot;</span>, <span class="dt">row.names=</span><span class="ot">FALSE</span>)</span></code></pre></div>
<p>We didn’t export the row names, because they’re useless in our case.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Explore <code>edstats_meta.csv</code> to understand the meaning of the
EdStats indicators.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>First, let’s load the dataset:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="classification-with-trees-and-linear-models.html#cb461-1" aria-hidden="true"></a>meta &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_meta.csv&quot;</span>)</span>
<span id="cb461-2"><a href="classification-with-trees-and-linear-models.html#cb461-2" aria-hidden="true"></a><span class="kw">names</span>(meta) <span class="co"># column names</span></span></code></pre></div>
<pre><code>## [1] &quot;Code&quot;       &quot;Series&quot;     &quot;Definition&quot; &quot;Source&quot;     &quot;Topic&quot;</code></pre>
<p>The <code>Series</code> column deciphers each indicator’s meaning.
For instance, <code>LO.PISA.MAT</code> gives:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="classification-with-trees-and-linear-models.html#cb463-1" aria-hidden="true"></a>meta[meta<span class="op">$</span>Code<span class="op">==</span><span class="st">&quot;LO.PISA.MAT&quot;</span>, <span class="st">&quot;Series&quot;</span>]</span></code></pre></div>
<pre><code>## [1] &quot;PISA: Mean performance on the mathematics scale&quot;</code></pre>
<p>To get more information, we can take a look at the <code>Definition</code>
column:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="classification-with-trees-and-linear-models.html#cb465-1" aria-hidden="true"></a>meta[meta<span class="op">$</span>Code<span class="op">==</span><span class="st">&quot;LO.PISA.MAT&quot;</span>, <span class="st">&quot;Definition&quot;</span>]</span></code></pre></div>
<p>which reads:
<em>Average score of 15-year-old students on the PISA mathematics scale. The metric for the overall mathematics scale is based on a mean for OECD countries of 500 points and a standard deviation of 100 points. Data reflects country performance in the stated year according to PISA reports, but may not be comparable across years or countries. Consult the PISA website for more detailed information: <a href="http://www.oecd.org/pisa/" class="uri">http://www.oecd.org/pisa/</a></em>.</p>
</details>
</div>
<div id="edstats-where-girls-are-better-at-maths-than-boys" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> EdStats – Where Girls Are Better at Maths Than Boys?</h3>
<p>In this task we will consider the “wide” version of the EdStats
dataset:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="classification-with-trees-and-linear-models.html#cb466-1" aria-hidden="true"></a>edstats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019_wide.csv&quot;</span>,</span>
<span id="cb466-2"><a href="classification-with-trees-and-linear-models.html#cb466-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb466-3"><a href="classification-with-trees-and-linear-models.html#cb466-3" aria-hidden="true"></a>edstats[<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>]</span></code></pre></div>
<pre><code>##   CountryName HD.HCI.AMRT HD.HCI.AMRT.FE HD.HCI.AMRT.MA HD.HCI.EYRS
## 1 Afghanistan      0.7797         0.8018         0.7597        8.58
##   HD.HCI.EYRS.FE
## 1           6.73</code></pre>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="classification-with-trees-and-linear-models.html#cb468-1" aria-hidden="true"></a>meta &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_meta.csv&quot;</span>)</span></code></pre></div>
<p>This dataset is small, moreover, we’ll be more interested in the description
(understanding) of data, not prediction of the response variable
to unobserved samples. Note that we have the <em>population</em> of the World
countries at hand here (new countries do not arise on a daily basis).
Therefore, a train-test split won’t be performed.</p>
<!--

```r
#pairs(edstats[c("LO.PISA.MAT", "LO.PISA.SCI", "LO.PISA.REA")])
```
-->
<div class="exercise"><strong>Exercise.</strong>
<p>Add a 0/1 factor-type variable <code>girls_rule_maths</code> that is equal to 1
if and only if a country’s average score of 15-year-old female students on the PISA
mathematics scale is greater than the corresponding indicator for the male ones.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Recall that a conversion of a logical value to a number
yields 1 for <code>TRUE</code> and <code>0</code> for <code>FALSE</code>. Hence:</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="classification-with-trees-and-linear-models.html#cb469-1" aria-hidden="true"></a>edstats<span class="op">$</span>girls_rule_maths &lt;-</span>
<span id="cb469-2"><a href="classification-with-trees-and-linear-models.html#cb469-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">factor</span>(<span class="kw">as.numeric</span>(</span>
<span id="cb469-3"><a href="classification-with-trees-and-linear-models.html#cb469-3" aria-hidden="true"></a>        edstats<span class="op">$</span>LO.PISA.MAT.FE<span class="op">&gt;</span>edstats<span class="op">$</span>LO.PISA.MAT.MA</span>
<span id="cb469-4"><a href="classification-with-trees-and-linear-models.html#cb469-4" aria-hidden="true"></a>    ))</span>
<span id="cb469-5"><a href="classification-with-trees-and-linear-models.html#cb469-5" aria-hidden="true"></a><span class="kw">head</span>(edstats<span class="op">$</span>girls_rule_maths, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  [1] &lt;NA&gt; 1    1    &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0    &lt;NA&gt;
## Levels: 0 1</code></pre>
<p>Unfortunately, there are many missing values in the dataset.
More precisely:</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="classification-with-trees-and-linear-models.html#cb471-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">is.na</span>(edstats<span class="op">$</span>girls_rule_maths)) <span class="co"># count</span></span></code></pre></div>
<pre><code>## [1] 187</code></pre>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="classification-with-trees-and-linear-models.html#cb473-1" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">is.na</span>(edstats<span class="op">$</span>girls_rule_maths)) <span class="co"># proportion</span></span></code></pre></div>
<pre><code>## [1] 0.69776</code></pre>
<p>Countries such as Egypt, India, Iran or Venezuela
are not amongst the 79 members of the Programme for International
Student Assessment. Thus, we’ll have to deal with the data we have.</p>
<p>The percentage of counties where “girls rule” is equal to:</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="classification-with-trees-and-linear-models.html#cb475-1" aria-hidden="true"></a><span class="kw">mean</span>(edstats<span class="op">$</span>girls_rule_maths<span class="op">==</span><span class="dv">1</span>, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 0.33333</code></pre>
<p>Here is the list of those counties:</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="classification-with-trees-and-linear-models.html#cb477-1" aria-hidden="true"></a><span class="kw">as.character</span>(<span class="kw">na.omit</span>(</span>
<span id="cb477-2"><a href="classification-with-trees-and-linear-models.html#cb477-2" aria-hidden="true"></a>    edstats[edstats<span class="op">$</span>girls_rule_maths<span class="op">==</span><span class="dv">1</span>, <span class="st">&quot;CountryName&quot;</span>]</span>
<span id="cb477-3"><a href="classification-with-trees-and-linear-models.html#cb477-3" aria-hidden="true"></a>))</span></code></pre></div>
<pre><code>##  [1] &quot;Albania&quot;              &quot;Algeria&quot;             
##  [3] &quot;Brunei Darussalam&quot;    &quot;Bulgaria&quot;            
##  [5] &quot;Cyprus&quot;               &quot;Dominican Republic&quot;  
##  [7] &quot;Finland&quot;              &quot;Georgia&quot;             
##  [9] &quot;Hong Kong SAR, China&quot; &quot;Iceland&quot;             
## [11] &quot;Indonesia&quot;            &quot;Israel&quot;              
## [13] &quot;Jordan&quot;               &quot;Lithuania&quot;           
## [15] &quot;Malaysia&quot;             &quot;Malta&quot;               
## [17] &quot;Moldova&quot;              &quot;North Macedonia&quot;     
## [19] &quot;Norway&quot;               &quot;Philippines&quot;         
## [21] &quot;Qatar&quot;                &quot;Saudi Arabia&quot;        
## [23] &quot;Sweden&quot;               &quot;Thailand&quot;            
## [25] &quot;Trinidad and Tobago&quot;  &quot;United Arab Emirates&quot;
## [27] &quot;Vietnam&quot;</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Let’s first create a subset of <code>edstats</code> that doesn’t include
the country names as well as the boys’ and girls’ math scores.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="classification-with-trees-and-linear-models.html#cb479-1" aria-hidden="true"></a>edstats_subset &lt;-<span class="st"> </span>edstats[<span class="op">!</span>(<span class="kw">names</span>(edstats) <span class="op">%in%</span></span>
<span id="cb479-2"><a href="classification-with-trees-and-linear-models.html#cb479-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>))]</span></code></pre></div>
<p>Fitting and plotting (see Figure <a href="classification-with-trees-and-linear-models.html#fig:girlsboys7tree">4.10</a>)
of the tree can be performed as follows:</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="classification-with-trees-and-linear-models.html#cb480-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb480-2"><a href="classification-with-trees-and-linear-models.html#cb480-2" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rpart.plot&quot;</span>)</span>
<span id="cb480-3"><a href="classification-with-trees-and-linear-models.html#cb480-3" aria-hidden="true"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(girls_rule_maths<span class="op">~</span>., <span class="dt">data=</span>edstats_subset,</span>
<span id="cb480-4"><a href="classification-with-trees-and-linear-models.html#cb480-4" aria-hidden="true"></a>    <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">model=</span><span class="ot">TRUE</span>)</span>
<span id="cb480-5"><a href="classification-with-trees-and-linear-models.html#cb480-5" aria-hidden="true"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<div class="figure"><span id="fig:girlsboys7tree"></span>
<img src="04-classification-trees_and_logistic-figures/girlsboys7tree-1.png" alt="" />
<p class="caption">Figure 4.10:  A decision tree explaining the <code>girls_rule_maths</code> variable</p>
</div>
<p>The variables included in the model are:</p>
<p>Note that the decision rules are well-interpretable, we can make a whole
story around it. Whether or not it is actually true – is a different… story.</p>
<p>To compute the basic classifier performance scores,
let’s recall the <code>get_metrics()</code> function:</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="classification-with-trees-and-linear-models.html#cb481-1" aria-hidden="true"></a>get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</span>
<span id="cb481-2"><a href="classification-with-trees-and-linear-models.html#cb481-2" aria-hidden="true"></a>{</span>
<span id="cb481-3"><a href="classification-with-trees-and-linear-models.html#cb481-3" aria-hidden="true"></a>    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></span>
<span id="cb481-4"><a href="classification-with-trees-and-linear-models.html#cb481-4" aria-hidden="true"></a>    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb481-5"><a href="classification-with-trees-and-linear-models.html#cb481-5" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></span>
<span id="cb481-6"><a href="classification-with-trees-and-linear-models.html#cb481-6" aria-hidden="true"></a>      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></span>
<span id="cb481-7"><a href="classification-with-trees-and-linear-models.html#cb481-7" aria-hidden="true"></a>      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></span>
<span id="cb481-8"><a href="classification-with-trees-and-linear-models.html#cb481-8" aria-hidden="true"></a>      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></span>
<span id="cb481-9"><a href="classification-with-trees-and-linear-models.html#cb481-9" aria-hidden="true"></a>      <span class="co"># Confusion matrix items:</span></span>
<span id="cb481-10"><a href="classification-with-trees-and-linear-models.html#cb481-10" aria-hidden="true"></a>      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</span>
<span id="cb481-11"><a href="classification-with-trees-and-linear-models.html#cb481-11" aria-hidden="true"></a>      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb481-12"><a href="classification-with-trees-and-linear-models.html#cb481-12" aria-hidden="true"></a>    ) <span class="co"># return a named vector</span></span>
<span id="cb481-13"><a href="classification-with-trees-and-linear-models.html#cb481-13" aria-hidden="true"></a>}</span></code></pre></div>
<p>Now we can judge the tree’s character:</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="classification-with-trees-and-linear-models.html#cb482-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree, edstats_subset, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb482-2"><a href="classification-with-trees-and-linear-models.html#cb482-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.81481  1.00000  0.44444  0.61538 54.00000 15.00000  0.00000 12.00000</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Learn a decision tree that this time doesn’t rely on any of the PISA indicators.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Let’s remove the unwanted variables:</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="classification-with-trees-and-linear-models.html#cb484-1" aria-hidden="true"></a>edstats_subset &lt;-<span class="st"> </span>edstats[<span class="op">!</span>(<span class="kw">names</span>(edstats) <span class="op">%in%</span></span>
<span id="cb484-2"><a href="classification-with-trees-and-linear-models.html#cb484-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;LO.PISA.MAT&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>,</span>
<span id="cb484-3"><a href="classification-with-trees-and-linear-models.html#cb484-3" aria-hidden="true"></a>      <span class="st">&quot;LO.PISA.REA&quot;</span>, <span class="st">&quot;LO.PISA.REA.FE&quot;</span>, <span class="st">&quot;LO.PISA.REA.MA&quot;</span>,</span>
<span id="cb484-4"><a href="classification-with-trees-and-linear-models.html#cb484-4" aria-hidden="true"></a>      <span class="st">&quot;LO.PISA.SCI&quot;</span>, <span class="st">&quot;LO.PISA.SCI.FE&quot;</span>, <span class="st">&quot;LO.PISA.SCI.MA&quot;</span>,</span>
<span id="cb484-5"><a href="classification-with-trees-and-linear-models.html#cb484-5" aria-hidden="true"></a>      <span class="st">&quot;CountryName&quot;</span>))]</span></code></pre></div>
<p>On a side note, this could be done more easily by calling, e.g.,
<code>stri_startswith_fixed(names(edstats), "LO.PISA")</code> from the <code>stringi</code> package.</p>
<p>Fitting and plotting (see Figure <a href="classification-with-trees-and-linear-models.html#fig:girlsboys11tree">4.11</a>) of the tree:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="classification-with-trees-and-linear-models.html#cb485-1" aria-hidden="true"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(girls_rule_maths<span class="op">~</span>., <span class="dt">data=</span>edstats_subset,</span>
<span id="cb485-2"><a href="classification-with-trees-and-linear-models.html#cb485-2" aria-hidden="true"></a>    <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">model=</span><span class="ot">TRUE</span>)</span>
<span id="cb485-3"><a href="classification-with-trees-and-linear-models.html#cb485-3" aria-hidden="true"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<div class="figure"><span id="fig:girlsboys11tree"></span>
<img src="04-classification-trees_and_logistic-figures/girlsboys11tree-1.png" alt="" />
<p class="caption">Figure 4.11:  Another decision tree explaining the <code>girls_rule_maths</code> variable</p>
</div>
<p>Performance metrics:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="classification-with-trees-and-linear-models.html#cb486-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree, edstats, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb486-2"><a href="classification-with-trees-and-linear-models.html#cb486-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.79012  0.69231  0.66667  0.67925 46.00000  9.00000  8.00000 18.00000</code></pre>
<p>It’s interesting to note that some of the
goodness-of-fit measures are actually higher now.</p>
<p>The variables included in the model are:</p>
</details>
</div>
<div id="edstats-and-world-factbook-joining-forces" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> EdStats and World Factbook – Joining Forces</h3>
<p>In the course of our data science journey, we have considered two datasets
dealing with country-level indicators: the World Factbook and
World Bank’s EdStats.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="classification-with-trees-and-linear-models.html#cb488-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</span>
<span id="cb488-2"><a href="classification-with-trees-and-linear-models.html#cb488-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb488-3"><a href="classification-with-trees-and-linear-models.html#cb488-3" aria-hidden="true"></a>edstats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019_wide.csv&quot;</span>,</span>
<span id="cb488-4"><a href="classification-with-trees-and-linear-models.html#cb488-4" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<p>Let’s combine the information they provide
and see if we come up with a better model of where
girls’ math scores are higher.</p>
<!--
#factbook$country[!(factbook$country %in% edstats$CountryName)]
#edstats$CountryName[!(edstats$CountryName %in% factbook$country)]
-->
<div class="exercise"><strong>Exercise.</strong>
<p>Some country names in one dataset don’t match those in the other
one, for instance: Czech Republic vs. Czechia,
Myanmar vs. Burma, etc. Resolve these conflicts as best you can.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To get a list of the mismatched country names, we can call either:</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="classification-with-trees-and-linear-models.html#cb489-1" aria-hidden="true"></a>factbook<span class="op">$</span>country[<span class="op">!</span>(factbook<span class="op">$</span>country <span class="op">%in%</span><span class="st"> </span>edstats<span class="op">$</span>CountryName)]</span></code></pre></div>
<p>or:</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="classification-with-trees-and-linear-models.html#cb490-1" aria-hidden="true"></a>edstats<span class="op">$</span>CountryName[<span class="op">!</span>(edstats<span class="op">$</span>CountryName <span class="op">%in%</span><span class="st"> </span>factbook<span class="op">$</span>country)]</span></code></pre></div>
<p>Unfortunately, the data need to be cleaned manually – it’s a tedious task.
The following consists of what we hope are the best matches
between the two datasets (yet, the list is not perfect;
in particular, the Republic of North Macedonia
is completely missing in one of the datasets):</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="classification-with-trees-and-linear-models.html#cb491-1" aria-hidden="true"></a>from_to &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>, <span class="kw">c</span>(</span>
<span id="cb491-2"><a href="classification-with-trees-and-linear-models.html#cb491-2" aria-hidden="true"></a><span class="co"># FROM (edstats)                  # TO (factbook)</span></span>
<span id="cb491-3"><a href="classification-with-trees-and-linear-models.html#cb491-3" aria-hidden="true"></a><span class="st">&quot;Brunei Darussalam&quot;</span>             , <span class="st">&quot;Brunei&quot;</span>                            ,</span>
<span id="cb491-4"><a href="classification-with-trees-and-linear-models.html#cb491-4" aria-hidden="true"></a><span class="st">&quot;Congo, Dem. Rep.&quot;</span>              , <span class="st">&quot;Congo, Democratic Republic of the&quot;</span> ,</span>
<span id="cb491-5"><a href="classification-with-trees-and-linear-models.html#cb491-5" aria-hidden="true"></a><span class="st">&quot;Congo, Rep.&quot;</span>                   , <span class="st">&quot;Congo, Republic of the&quot;</span>            ,</span>
<span id="cb491-6"><a href="classification-with-trees-and-linear-models.html#cb491-6" aria-hidden="true"></a><span class="st">&quot;Czech Republic&quot;</span>                , <span class="st">&quot;Czechia&quot;</span>                           ,</span>
<span id="cb491-7"><a href="classification-with-trees-and-linear-models.html#cb491-7" aria-hidden="true"></a><span class="st">&quot;Egypt, Arab Rep.&quot;</span>              , <span class="st">&quot;Egypt&quot;</span>                             ,</span>
<span id="cb491-8"><a href="classification-with-trees-and-linear-models.html#cb491-8" aria-hidden="true"></a><span class="st">&quot;Hong Kong SAR, China&quot;</span>          , <span class="st">&quot;Hong Kong&quot;</span>                         ,</span>
<span id="cb491-9"><a href="classification-with-trees-and-linear-models.html#cb491-9" aria-hidden="true"></a><span class="st">&quot;Iran, Islamic Rep.&quot;</span>            , <span class="st">&quot;Iran&quot;</span>                              ,</span>
<span id="cb491-10"><a href="classification-with-trees-and-linear-models.html#cb491-10" aria-hidden="true"></a><span class="st">&quot;Korea, Dem. People’s Rep.&quot;</span>     , <span class="st">&quot;Korea, North&quot;</span>                      ,</span>
<span id="cb491-11"><a href="classification-with-trees-and-linear-models.html#cb491-11" aria-hidden="true"></a><span class="st">&quot;Korea, Rep.&quot;</span>                   , <span class="st">&quot;Korea, South&quot;</span>                      ,</span>
<span id="cb491-12"><a href="classification-with-trees-and-linear-models.html#cb491-12" aria-hidden="true"></a><span class="st">&quot;Kyrgyz Republic&quot;</span>               , <span class="st">&quot;Kyrgyzstan&quot;</span>                        ,</span>
<span id="cb491-13"><a href="classification-with-trees-and-linear-models.html#cb491-13" aria-hidden="true"></a><span class="st">&quot;Lao PDR&quot;</span>                       , <span class="st">&quot;Laos&quot;</span>                              ,</span>
<span id="cb491-14"><a href="classification-with-trees-and-linear-models.html#cb491-14" aria-hidden="true"></a><span class="st">&quot;Macao SAR, China&quot;</span>              , <span class="st">&quot;Macau&quot;</span>                             ,</span>
<span id="cb491-15"><a href="classification-with-trees-and-linear-models.html#cb491-15" aria-hidden="true"></a><span class="st">&quot;Micronesia, Fed. Sts.&quot;</span>         , <span class="st">&quot;Micronesia, Federated States of&quot;</span>   ,</span>
<span id="cb491-16"><a href="classification-with-trees-and-linear-models.html#cb491-16" aria-hidden="true"></a><span class="st">&quot;Myanmar&quot;</span>                       , <span class="st">&quot;Burma&quot;</span>                             ,</span>
<span id="cb491-17"><a href="classification-with-trees-and-linear-models.html#cb491-17" aria-hidden="true"></a><span class="st">&quot;Russian Federation&quot;</span>            , <span class="st">&quot;Russia&quot;</span>                            ,</span>
<span id="cb491-18"><a href="classification-with-trees-and-linear-models.html#cb491-18" aria-hidden="true"></a><span class="st">&quot;Slovak Republic&quot;</span>               , <span class="st">&quot;Slovakia&quot;</span>                          ,</span>
<span id="cb491-19"><a href="classification-with-trees-and-linear-models.html#cb491-19" aria-hidden="true"></a><span class="st">&quot;St. Kitts and Nevis&quot;</span>           , <span class="st">&quot;Saint Kitts and Nevis&quot;</span>             ,</span>
<span id="cb491-20"><a href="classification-with-trees-and-linear-models.html#cb491-20" aria-hidden="true"></a><span class="st">&quot;St. Lucia&quot;</span>                     , <span class="st">&quot;Saint Lucia&quot;</span>                       ,</span>
<span id="cb491-21"><a href="classification-with-trees-and-linear-models.html#cb491-21" aria-hidden="true"></a><span class="st">&quot;St. Martin (French part)&quot;</span>      , <span class="st">&quot;Saint Martin&quot;</span>                      ,</span>
<span id="cb491-22"><a href="classification-with-trees-and-linear-models.html#cb491-22" aria-hidden="true"></a><span class="st">&quot;St. Vincent and the Grenadines&quot;</span>, <span class="st">&quot;Saint Vincent and the Grenadines&quot;</span>  ,</span>
<span id="cb491-23"><a href="classification-with-trees-and-linear-models.html#cb491-23" aria-hidden="true"></a><span class="st">&quot;Syrian Arab Republic&quot;</span>          , <span class="st">&quot;Syria&quot;</span>                             ,</span>
<span id="cb491-24"><a href="classification-with-trees-and-linear-models.html#cb491-24" aria-hidden="true"></a><span class="st">&quot;Venezuela, RB&quot;</span>                 , <span class="st">&quot;Venezuela&quot;</span>                         ,</span>
<span id="cb491-25"><a href="classification-with-trees-and-linear-models.html#cb491-25" aria-hidden="true"></a><span class="st">&quot;Virgin Islands (U.S.)&quot;</span>         , <span class="st">&quot;Virgin Islands&quot;</span>                    ,</span>
<span id="cb491-26"><a href="classification-with-trees-and-linear-models.html#cb491-26" aria-hidden="true"></a><span class="st">&quot;Yemen, Rep.&quot;</span>                   , <span class="st">&quot;Yemen&quot;</span></span>
<span id="cb491-27"><a href="classification-with-trees-and-linear-models.html#cb491-27" aria-hidden="true"></a>))</span></code></pre></div>
<p>Conversion of the names:</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="classification-with-trees-and-linear-models.html#cb492-1" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(from_to)) {</span>
<span id="cb492-2"><a href="classification-with-trees-and-linear-models.html#cb492-2" aria-hidden="true"></a>    edstats<span class="op">$</span>CountryName[edstats<span class="op">$</span>CountryName<span class="op">==</span>from_to[i,<span class="dv">1</span>]] &lt;-<span class="st"> </span>from_to[i,<span class="dv">2</span>]</span>
<span id="cb492-3"><a href="classification-with-trees-and-linear-models.html#cb492-3" aria-hidden="true"></a>}</span></code></pre></div>
<p>On a side note (*), this could be done with a single call to
a function in the <code>stringi</code> package:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="classification-with-trees-and-linear-models.html#cb493-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;stringi&quot;</span>)</span>
<span id="cb493-2"><a href="classification-with-trees-and-linear-models.html#cb493-2" aria-hidden="true"></a>edstats<span class="op">$</span>CountryName &lt;-<span class="st"> </span><span class="kw">stri_replace_all_fixed</span>(edstats<span class="op">$</span>CountryName,</span>
<span id="cb493-3"><a href="classification-with-trees-and-linear-models.html#cb493-3" aria-hidden="true"></a>    from_to[,<span class="dv">1</span>], from_to[,<span class="dv">2</span>], <span class="dt">vectorize_all=</span><span class="ot">FALSE</span>)</span></code></pre></div>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Merge (join) the two datasets based on the country names.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>This can be done by means of the <code>merge()</code> function.</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="classification-with-trees-and-linear-models.html#cb494-1" aria-hidden="true"></a>edbook &lt;-<span class="st"> </span><span class="kw">merge</span>(edstats, factbook, <span class="dt">by.x=</span><span class="st">&quot;CountryName&quot;</span>, <span class="dt">by.y=</span><span class="st">&quot;country&quot;</span>)</span>
<span id="cb494-2"><a href="classification-with-trees-and-linear-models.html#cb494-2" aria-hidden="true"></a><span class="kw">ncol</span>(edbook) <span class="co"># how many columns we have now</span></span></code></pre></div>
<pre><code>## [1] 157</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>We proceed as in one of the previous exercises:</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="classification-with-trees-and-linear-models.html#cb496-1" aria-hidden="true"></a>edbook<span class="op">$</span>girls_rule_maths &lt;-</span>
<span id="cb496-2"><a href="classification-with-trees-and-linear-models.html#cb496-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">factor</span>(<span class="kw">as.numeric</span>(</span>
<span id="cb496-3"><a href="classification-with-trees-and-linear-models.html#cb496-3" aria-hidden="true"></a>        edbook<span class="op">$</span>LO.PISA.MAT.FE<span class="op">&gt;</span>edbook<span class="op">$</span>LO.PISA.MAT.MA</span>
<span id="cb496-4"><a href="classification-with-trees-and-linear-models.html#cb496-4" aria-hidden="true"></a>    ))</span>
<span id="cb496-5"><a href="classification-with-trees-and-linear-models.html#cb496-5" aria-hidden="true"></a>edbook_subset &lt;-<span class="st"> </span>edbook[<span class="op">!</span>(<span class="kw">names</span>(edbook) <span class="op">%in%</span></span>
<span id="cb496-6"><a href="classification-with-trees-and-linear-models.html#cb496-6" aria-hidden="true"></a><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>))]</span></code></pre></div>
<p>Fitting and plotting (see Figure <a href="classification-with-trees-and-linear-models.html#fig:joinedstats9">4.12</a>):</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="classification-with-trees-and-linear-models.html#cb497-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rpart&quot;</span>)</span>
<span id="cb497-2"><a href="classification-with-trees-and-linear-models.html#cb497-2" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rpart.plot&quot;</span>)</span>
<span id="cb497-3"><a href="classification-with-trees-and-linear-models.html#cb497-3" aria-hidden="true"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(girls_rule_maths<span class="op">~</span>., <span class="dt">data=</span>edbook_subset,</span>
<span id="cb497-4"><a href="classification-with-trees-and-linear-models.html#cb497-4" aria-hidden="true"></a>    <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">model=</span><span class="ot">TRUE</span>)</span>
<span id="cb497-5"><a href="classification-with-trees-and-linear-models.html#cb497-5" aria-hidden="true"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<div class="figure"><span id="fig:joinedstats9"></span>
<img src="04-classification-trees_and_logistic-figures/joinedstats9-1.png" alt="" />
<p class="caption">Figure 4.12:  Yet another decision tree explaining the <code>girls_rule_maths</code> variable</p>
</div>
<p>Performance metrics:</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="classification-with-trees-and-linear-models.html#cb498-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree, edbook_subset, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb498-2"><a href="classification-with-trees-and-linear-models.html#cb498-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edbook_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.82716  0.78261  0.66667  0.72000 49.00000  9.00000  5.00000 18.00000</code></pre>
<p>The variables included in the model are:</p>
<p>This is… not at all enlightening.
Rest assured that experts in education
or econometrics for whom we work in this (imaginary) project
would raise many questions at this very point. Merely applying
some computational procedure on a dataset doesn’t cut it;
it’s too early to ask for a paycheque.
Classifiers are just blind <em>tools</em> in our gentle yet firm hands;
new questions are risen, new answers must be sought. Further
explorations are of course left as an exercise to the kind reader.</p>
</details>
</div>
<div id="edstats-fitting-of-binary-logistic-regression-models" class="section level3" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> EdStats – Fitting of Binary Logistic Regression Models</h3>
<p>In this task we’re going to
consider the “wide” version of the EdStats
dataset again:</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="classification-with-trees-and-linear-models.html#cb500-1" aria-hidden="true"></a>edstats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019_wide.csv&quot;</span>,</span>
<span id="cb500-2"><a href="classification-with-trees-and-linear-models.html#cb500-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<p>Let’s re-add the <code>girls_rule_maths</code> column
just as in the previous exercise.
Then, let’s create a subset of <code>edstats</code> that doesn’t include
the country names as well as the boys’ and girls’ math scores.</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="classification-with-trees-and-linear-models.html#cb501-1" aria-hidden="true"></a>edstats<span class="op">$</span>girls_rule_maths &lt;-</span>
<span id="cb501-2"><a href="classification-with-trees-and-linear-models.html#cb501-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">factor</span>(<span class="kw">as.numeric</span>(</span>
<span id="cb501-3"><a href="classification-with-trees-and-linear-models.html#cb501-3" aria-hidden="true"></a>        edstats<span class="op">$</span>LO.PISA.MAT.FE<span class="op">&gt;</span>edstats<span class="op">$</span>LO.PISA.MAT.MA</span>
<span id="cb501-4"><a href="classification-with-trees-and-linear-models.html#cb501-4" aria-hidden="true"></a>    ))</span>
<span id="cb501-5"><a href="classification-with-trees-and-linear-models.html#cb501-5" aria-hidden="true"></a>edstats_subset &lt;-<span class="st"> </span>edstats[<span class="op">!</span>(<span class="kw">names</span>(edstats) <span class="op">%in%</span></span>
<span id="cb501-6"><a href="classification-with-trees-and-linear-models.html#cb501-6" aria-hidden="true"></a><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>))]</span></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit and assess a logistic regression model for <code>girls_rule_maths</code> as a function
of <code>LO.PISA.REA.MA</code>+<code>LO.PISA.SCI</code>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Fitting of the model:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="classification-with-trees-and-linear-models.html#cb502-1" aria-hidden="true"></a>(f1 &lt;-<span class="st"> </span><span class="kw">glm</span>(girls_rule_maths<span class="op">~</span>LO.PISA.REA.MA<span class="op">+</span>LO.PISA.SCI,</span>
<span id="cb502-2"><a href="classification-with-trees-and-linear-models.html#cb502-2" aria-hidden="true"></a>    <span class="dt">data=</span>edstats_subset, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)))</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = girls_rule_maths ~ LO.PISA.REA.MA + LO.PISA.SCI, 
##     family = binomial(&quot;logit&quot;), data = edstats_subset)
## 
## Coefficients:
##    (Intercept)  LO.PISA.REA.MA     LO.PISA.SCI  
##         3.0927         -0.0882          0.0755  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  78 Residual
##   (187 observations deleted due to missingness)
## Null Deviance:       103 
## Residual Deviance: 77.9  AIC: 83.9</code></pre>
<p>Performance metrics:</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="classification-with-trees-and-linear-models.html#cb504-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(f1, edstats_subset, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb504-2"><a href="classification-with-trees-and-linear-models.html#cb504-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.79012  0.75000  0.55556  0.63830 49.00000 12.00000  5.00000 15.00000</code></pre>
<p>Relate the above numbers to those reported for the fitted decision trees.</p>
<p>Note that the fitted model is nicely interpretable:
the lower the boys’ average result on the Reading Scale
or the higher the country’s result on the Science Scale,
the higher the probability for <code>girls_rule_maths</code>:</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="classification-with-trees-and-linear-models.html#cb506-1" aria-hidden="true"></a>example_X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb506-2"><a href="classification-with-trees-and-linear-models.html#cb506-2" aria-hidden="true"></a>    <span class="dt">LO.PISA.REA.MA=</span><span class="kw">c</span>(<span class="dv">475</span>, <span class="dv">450</span>, <span class="dv">475</span>, <span class="dv">500</span>),</span>
<span id="cb506-3"><a href="classification-with-trees-and-linear-models.html#cb506-3" aria-hidden="true"></a>    <span class="dt">LO.PISA.SCI=</span>   <span class="kw">c</span>(<span class="dv">525</span>, <span class="dv">525</span>, <span class="dv">550</span>, <span class="dv">500</span>)</span>
<span id="cb506-4"><a href="classification-with-trees-and-linear-models.html#cb506-4" aria-hidden="true"></a>)</span>
<span id="cb506-5"><a href="classification-with-trees-and-linear-models.html#cb506-5" aria-hidden="true"></a><span class="kw">cbind</span>(example_X,</span>
<span id="cb506-6"><a href="classification-with-trees-and-linear-models.html#cb506-6" aria-hidden="true"></a>    <span class="st">`</span><span class="dt">Pr(Y=1)</span><span class="st">`</span>=<span class="kw">predict</span>(f1, example_X, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>))</span></code></pre></div>
<pre><code>##   LO.PISA.REA.MA LO.PISA.SCI  Pr(Y=1)
## 1            475         525 0.703342
## 2            450         525 0.955526
## 3            475         550 0.939986
## 4            500         500 0.038094</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>(*) Fit and assess a logistic regression model for <code>girls_rule_maths</code>
featuring all <code>LO.PISA.REA*</code> and <code>LO.PISA.SCI*</code> as independent variables.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Model fitting:</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="classification-with-trees-and-linear-models.html#cb508-1" aria-hidden="true"></a>(f2 &lt;-<span class="st"> </span><span class="kw">glm</span>(girls_rule_maths<span class="op">~</span>LO.PISA.REA<span class="op">+</span>LO.PISA.REA.FE<span class="op">+</span>LO.PISA.REA.MA<span class="op">+</span></span>
<span id="cb508-2"><a href="classification-with-trees-and-linear-models.html#cb508-2" aria-hidden="true"></a><span class="st">                            </span>LO.PISA.SCI<span class="op">+</span>LO.PISA.SCI.FE<span class="op">+</span>LO.PISA.SCI.MA,</span>
<span id="cb508-3"><a href="classification-with-trees-and-linear-models.html#cb508-3" aria-hidden="true"></a>    <span class="dt">data=</span>edstats_subset, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)))</span></code></pre></div>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>## 
## Call:  glm(formula = girls_rule_maths ~ LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + LO.PISA.SCI.MA, 
##     family = binomial(&quot;logit&quot;), data = edstats_subset)
## 
## Coefficients:
##    (Intercept)     LO.PISA.REA  LO.PISA.REA.FE  LO.PISA.REA.MA  
##         -2.265           1.268          -0.544          -0.734  
##    LO.PISA.SCI  LO.PISA.SCI.FE  LO.PISA.SCI.MA  
##          1.269          -0.157          -1.112  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  74 Residual
##   (187 observations deleted due to missingness)
## Null Deviance:       103 
## Residual Deviance: 33    AIC: 47</code></pre>
<p>The mysterious <code>fitted probabilities numerically 0 or 1 occurred</code> warning
denotes convergence problems of the underlying optimisation (fitting) procedure:
at least one of the model coefficients has had a fairly large order
of magnitude and hence the fitted probabilities has come very close to 0 or 1.
Recall that the probabilities are modelled by means of the logistic sigmoid
function applied on the output of a linear combination of the dependent variables.
Moreover, cross-entropy features a logarithm, and <span class="math inline">\(\log 0 = -\infty\)</span>.</p>
<p>This can be due to the fact that all the variables in the model are very
correlated with each other (multicollinearity; an ill-conditioned problem).
The obtained solution might be unstable – there might be many local optima
and hence, different parameter vectors might be equally good.
Moreover, it is likely that a small change in one of the inputs might
lead to large change in the estimated model
(* normally, we would attack this problem by employing
some regularisation techniques).</p>
<!--
#(f2 <- glm(girls_rule_maths~LO.PISA.REA+LO.PISA.REA.FE+LO.PISA.REA.MA+
#                            LO.PISA.SCI+LO.PISA.SCI.FE+LO.PISA.SCI.MA,
#    data=edstats_subset, family=binomial("logit"), start=runif(7,-0.1,0.1)))
-->
<p>Of course, the model’s performance metrics can still be computed,
but then it’s better if we treat it as a black box. Or, even better,
reduce the number of independent variables and come up with a simpler
model that serves its purpose better than this one.</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="classification-with-trees-and-linear-models.html#cb511-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(f2, edstats_subset, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb511-2"><a href="classification-with-trees-and-linear-models.html#cb511-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.86420  0.83333  0.74074  0.78431 50.00000  7.00000  4.00000 20.00000</code></pre>
</details>
<!--

standardise variables:  [too little benefit for the reader? omit]


```r
#standardise <- function(x) {
#    (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
#}
```


```r
#sedstats_subset <- edstats_subset # copy
#for (i in 1:(ncol(sedstats_subset)-1)) {
#    # standardise every column of sedstats_subset but
#    # the girls_rule_maths one (it must remain a binary==0/1 variable)
#    sedstats_subset[[i]] <- standardise(sedstats_subset[[i]])
#}
```


```r
#(f1s <- glm(girls_rule_maths~LO.PISA.REA.MA+LO.PISA.SCI,
#    data=sedstats_subset, family=binomial("logit")))
#Y_pred <- as.numeric(predict(f1s, sedstats_subset, type="response")>0.5)
#get_metrics(Y_pred, sedstats_subset$girls_rule_maths)
```


```r
#(f2s <- glm(girls_rule_maths~LO.PISA.REA+LO.PISA.REA.FE+LO.PISA.REA.MA+
#                            LO.PISA.SCI+LO.PISA.SCI.FE+LO.PISA.SCI.MA,
#    data=sedstats_subset, family=binomial("logit")))
#Y_pred <- as.numeric(predict(f2s, sedstats_subset, type="response")>0.5)
#get_metrics(Y_pred, sedstats_subset$girls_rule_maths)
```

-->
</div>
<div id="edstats-variable-selection-in-binary-logistic-regression" class="section level3" number="4.4.5">
<h3><span class="header-section-number">4.4.5</span> EdStats – Variable Selection in Binary Logistic Regression (*)</h3>
<p>Back to our <code>girls_rule_maths</code> example, we still have so much to learn!</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="classification-with-trees-and-linear-models.html#cb513-1" aria-hidden="true"></a>edstats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019_wide.csv&quot;</span>,</span>
<span id="cb513-2"><a href="classification-with-trees-and-linear-models.html#cb513-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb513-3"><a href="classification-with-trees-and-linear-models.html#cb513-3" aria-hidden="true"></a>edstats<span class="op">$</span>girls_rule_maths &lt;-</span>
<span id="cb513-4"><a href="classification-with-trees-and-linear-models.html#cb513-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">factor</span>(<span class="kw">as.numeric</span>(</span>
<span id="cb513-5"><a href="classification-with-trees-and-linear-models.html#cb513-5" aria-hidden="true"></a>        edstats<span class="op">$</span>LO.PISA.MAT.FE<span class="op">&gt;</span>edstats<span class="op">$</span>LO.PISA.MAT.MA</span>
<span id="cb513-6"><a href="classification-with-trees-and-linear-models.html#cb513-6" aria-hidden="true"></a>    ))</span>
<span id="cb513-7"><a href="classification-with-trees-and-linear-models.html#cb513-7" aria-hidden="true"></a>edstats_subset &lt;-<span class="st"> </span>edstats[<span class="op">!</span>(<span class="kw">names</span>(edstats) <span class="op">%in%</span></span>
<span id="cb513-8"><a href="classification-with-trees-and-linear-models.html#cb513-8" aria-hidden="true"></a><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>))]</span></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Construct a binary logistic regression model via forward selection
of variables.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Just as in the linear regression case, we can rely on the <code>step()</code> function.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="classification-with-trees-and-linear-models.html#cb514-1" aria-hidden="true"></a>model_empty &lt;-<span class="st"> </span>girls_rule_maths<span class="op">~</span><span class="dv">1</span></span>
<span id="cb514-2"><a href="classification-with-trees-and-linear-models.html#cb514-2" aria-hidden="true"></a>(model_full &lt;-<span class="st"> </span><span class="kw">formula</span>(<span class="kw">model.frame</span>(girls_rule_maths<span class="op">~</span>.,</span>
<span id="cb514-3"><a href="classification-with-trees-and-linear-models.html#cb514-3" aria-hidden="true"></a>    <span class="dt">data=</span>edstats_subset)))</span></code></pre></div>
<pre><code>## girls_rule_maths ~ HD.HCI.AMRT + HD.HCI.AMRT.FE + HD.HCI.AMRT.MA + 
##     HD.HCI.EYRS + HD.HCI.EYRS.FE + HD.HCI.EYRS.MA + HD.HCI.HLOS + 
##     HD.HCI.HLOS.FE + HD.HCI.HLOS.MA + HD.HCI.MORT + HD.HCI.MORT.FE + 
##     HD.HCI.MORT.MA + HD.HCI.OVRL + HD.HCI.OVRL.FE + HD.HCI.OVRL.MA + 
##     IT.CMP.PCMP.P2 + IT.NET.USER.P2 + LO.PISA.MAT + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     LO.PISA.SCI.MA + NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + NY.GDP.PCAP.PP.CD + 
##     NY.GNP.PCAP.CD + NY.GNP.PCAP.PP.CD + SE.COM.DURS + SE.PRM.CMPT.FE.ZS + 
##     SE.PRM.CMPT.MA.ZS + SE.PRM.CMPT.ZS + SE.PRM.ENRL.TC.ZS + 
##     SE.PRM.ENRR + SE.PRM.ENRR.FE + SE.PRM.ENRR.MA + SE.PRM.NENR + 
##     SE.PRM.NENR.FE + SE.PRM.NENR.MA + SE.PRM.PRIV.ZS + SE.SEC.ENRL.TC.ZS + 
##     SE.SEC.ENRR + SE.SEC.ENRR.FE + SE.SEC.ENRR.MA + SE.SEC.NENR + 
##     SE.SEC.NENR.MA + SE.SEC.PRIV.ZS + SE.TER.ENRR + SE.TER.ENRR.FE + 
##     SE.TER.ENRR.MA + SE.TER.PRIV.ZS + SE.XPD.TOTL.GD.ZS + SL.TLF.ADVN.FE.ZS + 
##     SL.TLF.ADVN.MA.ZS + SL.TLF.ADVN.ZS + SP.POP.TOTL + SP.POP.TOTL.FE.IN + 
##     SP.POP.TOTL.MA.IN + SP.PRM.TOTL.FE.IN + SP.PRM.TOTL.IN + 
##     SP.PRM.TOTL.MA.IN + SP.SEC.TOTL.FE.IN + SP.SEC.TOTL.IN + 
##     SP.SEC.TOTL.MA.IN + UIS.PTRHC.56 + UIS.SAP.CE + UIS.SAP.CE.F + 
##     UIS.SAP.CE.M + UIS.X.PPP.1.FSGOV + UIS.X.PPP.2T3.FSGOV + 
##     UIS.X.PPP.5T8.FSGOV + UIS.X.US.1.FSGOV + UIS.X.US.2T3.FSGOV + 
##     UIS.X.US.5T8.FSGOV + UIS.XGDP.1.FSGOV + UIS.XGDP.23.FSGOV + 
##     UIS.XGDP.56.FSGOV + UIS.XUNIT.GDPCAP.1.FSGOV + UIS.XUNIT.GDPCAP.23.FSGOV + 
##     UIS.XUNIT.GDPCAP.5T8.FSGOV + UIS.XUNIT.PPP.1.FSGOV.FFNTR + 
##     UIS.XUNIT.PPP.2T3.FSGOV.FFNTR + UIS.XUNIT.PPP.5T8.FSGOV.FFNTR + 
##     UIS.XUNIT.US.1.FSGOV.FFNTR + UIS.XUNIT.US.23.FSGOV.FFNTR + 
##     UIS.XUNIT.US.5T8.FSGOV.FFNTR</code></pre>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="classification-with-trees-and-linear-models.html#cb516-1" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">step</span>(<span class="kw">glm</span>(model_empty, <span class="dt">data=</span>edstats_subset, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)),</span>
<span id="cb516-2"><a href="classification-with-trees-and-linear-models.html#cb516-2" aria-hidden="true"></a>    <span class="dt">scope=</span>model_full, <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=105.12
## girls_rule_maths ~ 1</code></pre>
<pre><code>## Error in model.matrix.default(Terms, m, contrasts.arg = object$contrasts): variable 1 has no levels</code></pre>
<p>Melbourne, we have a problem!
Our dataset has too many missing values, and those cannot be present
in a logistic regression model (it’s based on a linear combination of variables,
and sums/products involving <code>NA</code>s yield <code>NA</code>s…).</p>
<p>Looking at the manual of <code>?step</code>, we see that the default
<code>NA</code> handling is via <code>na.omit()</code>, and that, when applied on a data frame,
results in the removal of all the <em>rows</em>, where there is at least one <code>NA</code>.
Sadly, it’s too invasive.</p>
<p>We should get rid of the data blanks manually.
First, definitely, we should remove all the rows where
<code>girls_rule_maths</code> is unknown:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="classification-with-trees-and-linear-models.html#cb519-1" aria-hidden="true"></a>edstats_subset &lt;-</span>
<span id="cb519-2"><a href="classification-with-trees-and-linear-models.html#cb519-2" aria-hidden="true"></a><span class="st">    </span>edstats_subset[<span class="op">!</span><span class="kw">is.na</span>(edstats_subset<span class="op">$</span>girls_rule_maths),]</span></code></pre></div>
<p>We are about to apply the forward selection process, whose purpose is
to choose variables for a model. Therefore, instead of removing any more
rows, we should remove the… columns with missing data:</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="classification-with-trees-and-linear-models.html#cb520-1" aria-hidden="true"></a>edstats_subset &lt;-</span>
<span id="cb520-2"><a href="classification-with-trees-and-linear-models.html#cb520-2" aria-hidden="true"></a><span class="st">    </span>edstats_subset[,<span class="kw">colSums</span>(<span class="kw">sapply</span>(edstats_subset, is.na))<span class="op">==</span><span class="dv">0</span>]</span></code></pre></div>
<p>(*) Alternatively, we could apply some techniques of missing data imputation;
this is beyond the scope of this book. For instance, <code>NA</code>s could be
replaced by the averages of their respective columns.</p>
<p>We are ready now to make use of <code>step()</code>.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="classification-with-trees-and-linear-models.html#cb521-1" aria-hidden="true"></a>model_empty &lt;-<span class="st"> </span>girls_rule_maths<span class="op">~</span><span class="dv">1</span></span>
<span id="cb521-2"><a href="classification-with-trees-and-linear-models.html#cb521-2" aria-hidden="true"></a>(model_full &lt;-<span class="st"> </span><span class="kw">formula</span>(<span class="kw">model.frame</span>(girls_rule_maths<span class="op">~</span>.,</span>
<span id="cb521-3"><a href="classification-with-trees-and-linear-models.html#cb521-3" aria-hidden="true"></a>    <span class="dt">data=</span>edstats_subset)))</span></code></pre></div>
<pre><code>## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.MAT + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     LO.PISA.SCI.MA + NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + SP.POP.TOTL</code></pre>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="classification-with-trees-and-linear-models.html#cb523-1" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">step</span>(<span class="kw">glm</span>(model_empty, <span class="dt">data=</span>edstats_subset, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)),</span>
<span id="cb523-2"><a href="classification-with-trees-and-linear-models.html#cb523-2" aria-hidden="true"></a>    <span class="dt">scope=</span>model_full, <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=105.12
## girls_rule_maths ~ 1
## 
##                  Df Deviance   AIC
## + LO.PISA.REA.MA  1     90.9  94.9
## + LO.PISA.SCI.MA  1     93.3  97.3
## + NY.GDP.MKTP.CD  1     94.2  98.2
## + LO.PISA.REA     1     95.0  99.0
## + LO.PISA.SCI     1     96.9 100.9
## + LO.PISA.MAT     1     97.2 101.2
## + LO.PISA.REA.FE  1     97.9 101.9
## + LO.PISA.SCI.FE  1     99.4 103.4
## &lt;none&gt;                 103.1 105.1
## + SP.POP.TOTL     1    101.9 105.9
## + NY.GDP.PCAP.CD  1    102.3 106.3
## + IT.NET.USER.P2  1    103.1 107.1
## 
## Step:  AIC=94.93
## girls_rule_maths ~ LO.PISA.REA.MA
## 
##                  Df Deviance  AIC
## + LO.PISA.REA     1     42.8 48.8
## + LO.PISA.REA.FE  1     50.5 56.5
## + LO.PISA.SCI.FE  1     65.4 71.4
## + LO.PISA.SCI     1     77.9 83.9
## + LO.PISA.MAT     1     83.5 89.5
## + NY.GDP.MKTP.CD  1     87.4 93.4
## + IT.NET.USER.P2  1     87.5 93.5
## &lt;none&gt;                  90.9 94.9
## + NY.GDP.PCAP.CD  1     89.2 95.2
## + LO.PISA.SCI.MA  1     89.2 95.2
## + SP.POP.TOTL     1     90.2 96.2
## 
## Step:  AIC=48.83
## girls_rule_maths ~ LO.PISA.REA.MA + LO.PISA.REA
## 
##                  Df Deviance  AIC
## &lt;none&gt;                  42.8 48.8
## + LO.PISA.SCI.FE  1     40.9 48.9
## + SP.POP.TOTL     1     41.2 49.2
## + NY.GDP.PCAP.CD  1     41.3 49.3
## + LO.PISA.SCI     1     42.0 50.0
## + LO.PISA.MAT     1     42.4 50.4
## + IT.NET.USER.P2  1     42.7 50.7
## + LO.PISA.SCI.MA  1     42.7 50.7
## + NY.GDP.MKTP.CD  1     42.7 50.7
## + LO.PISA.REA.FE  1     42.8 50.8</code></pre>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="classification-with-trees-and-linear-models.html#cb525-1" aria-hidden="true"></a><span class="kw">print</span>(f)</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = girls_rule_maths ~ LO.PISA.REA.MA + LO.PISA.REA, 
##     family = binomial(&quot;logit&quot;), data = edstats_subset)
## 
## Coefficients:
##    (Intercept)  LO.PISA.REA.MA     LO.PISA.REA  
##         -0.176          -0.600           0.577  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  78 Residual
## Null Deviance:       103 
## Residual Deviance: 42.8  AIC: 48.8</code></pre>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="classification-with-trees-and-linear-models.html#cb527-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(f, edstats_subset, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb527-2"><a href="classification-with-trees-and-linear-models.html#cb527-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.88889  0.84615  0.81481  0.83019 50.00000  5.00000  4.00000 22.00000</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Choose a model via backward elimination.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Having a dataset with missing values removed, this is easy now:</p>
<!--
makes no difference:

# standardise <- function(x) {
#     (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
# }
# for (i in 1:(ncol(edstats_subset)-1)) {
#    # standardise every column of sedstats_subset but
#    # the girls_rule_maths one (it must remain a binary==0/1 variable)
#    edstats_subset[[i]] <- standardise(edstats_subset[[i]])
# }
-->
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="classification-with-trees-and-linear-models.html#cb529-1" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>( <span class="co"># yeah, yeah, yeah...</span></span>
<span id="cb529-2"><a href="classification-with-trees-and-linear-models.html#cb529-2" aria-hidden="true"></a>        <span class="co"># fitted probabilities numerically 0 or 1 occurred</span></span>
<span id="cb529-3"><a href="classification-with-trees-and-linear-models.html#cb529-3" aria-hidden="true"></a>    <span class="kw">step</span>(<span class="kw">glm</span>(model_full, <span class="dt">data=</span>edstats_subset, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="st">&quot;logit&quot;</span>)),</span>
<span id="cb529-4"><a href="classification-with-trees-and-linear-models.html#cb529-4" aria-hidden="true"></a>        <span class="dt">scope=</span>model_empty, <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</span>
<span id="cb529-5"><a href="classification-with-trees-and-linear-models.html#cb529-5" aria-hidden="true"></a>)</span></code></pre></div>
<pre><code>## Start:  AIC=50.83
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.MAT + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     LO.PISA.SCI.MA + NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + SP.POP.TOTL
## 
##                  Df Deviance  AIC
## - LO.PISA.MAT     1     26.8 48.8
## - LO.PISA.SCI.MA  1     26.8 48.8
## - NY.GDP.PCAP.CD  1     26.9 48.9
## - LO.PISA.SCI     1     26.9 48.9
## - LO.PISA.SCI.FE  1     27.1 49.1
## - LO.PISA.REA.FE  1     27.4 49.4
## - LO.PISA.REA     1     27.5 49.5
## - LO.PISA.REA.MA  1     27.6 49.6
## &lt;none&gt;                  26.8 50.8
## - IT.NET.USER.P2  1     29.3 51.3
## - NY.GDP.MKTP.CD  1     29.9 51.9
## - SP.POP.TOTL     1     31.7 53.7
## 
## Step:  AIC=48.84
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + LO.PISA.SCI.MA + 
##     NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + SP.POP.TOTL
## 
##                  Df Deviance  AIC
## - LO.PISA.SCI.MA  1     26.8 46.8
## - NY.GDP.PCAP.CD  1     26.9 46.9
## - LO.PISA.SCI     1     27.0 47.0
## - LO.PISA.SCI.FE  1     27.1 47.1
## - LO.PISA.REA.FE  1     27.4 47.4
## - LO.PISA.REA     1     27.5 47.5
## - LO.PISA.REA.MA  1     27.6 47.6
## &lt;none&gt;                  26.8 48.8
## - IT.NET.USER.P2  1     29.3 49.3
## - NY.GDP.MKTP.CD  1     29.9 49.9
## - SP.POP.TOTL     1     31.7 51.7
## 
## Step:  AIC=46.84
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + NY.GDP.MKTP.CD + 
##     NY.GDP.PCAP.CD + SP.POP.TOTL
## 
##                  Df Deviance  AIC
## - NY.GDP.PCAP.CD  1     26.9 44.9
## &lt;none&gt;                  26.8 46.8
## - IT.NET.USER.P2  1     29.3 47.3
## - NY.GDP.MKTP.CD  1     29.9 47.9
## - LO.PISA.REA.FE  1     31.0 49.0
## - SP.POP.TOTL     1     31.8 49.8
## - LO.PISA.SCI     1     35.6 53.6
## - LO.PISA.SCI.FE  1     36.1 54.1
## - LO.PISA.REA     1     37.5 55.5
## - LO.PISA.REA.MA  1     50.9 68.9
## 
## Step:  AIC=44.87
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + NY.GDP.MKTP.CD + 
##     SP.POP.TOTL
## 
##                  Df Deviance  AIC
## &lt;none&gt;                  26.9 44.9
## - NY.GDP.MKTP.CD  1     30.5 46.5
## - IT.NET.USER.P2  1     31.0 47.0
## - LO.PISA.REA.FE  1     31.1 47.1
## - SP.POP.TOTL     1     33.0 49.0
## - LO.PISA.SCI     1     35.9 51.9
## - LO.PISA.SCI.FE  1     36.4 52.4
## - LO.PISA.REA     1     37.5 53.5
## - LO.PISA.REA.MA  1     50.9 66.9</code></pre>
<p>The obtained model and its quality metrics:</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="classification-with-trees-and-linear-models.html#cb531-1" aria-hidden="true"></a><span class="kw">print</span>(f)</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     NY.GDP.MKTP.CD + SP.POP.TOTL, family = binomial(&quot;logit&quot;), 
##     data = edstats_subset)
## 
## Coefficients:
##    (Intercept)  IT.NET.USER.P2     LO.PISA.REA  LO.PISA.REA.FE  
##      -1.66e+01        1.61e-01        1.85e+00       -8.00e-01  
## LO.PISA.REA.MA     LO.PISA.SCI  LO.PISA.SCI.FE  NY.GDP.MKTP.CD  
##      -1.03e+00       -1.35e+00        1.32e+00       -4.95e-12  
##    SP.POP.TOTL  
##       6.20e-08  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  72 Residual
## Null Deviance:       103 
## Residual Deviance: 26.9  AIC: 44.9</code></pre>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="classification-with-trees-and-linear-models.html#cb533-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">predict</span>(f, edstats_subset, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb533-2"><a href="classification-with-trees-and-linear-models.html#cb533-2" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</span></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.91358  0.88462  0.85185  0.86792 51.00000  4.00000  3.00000 23.00000</code></pre>
<p>Note that we got a better (lower) AIC than in the forward selection
case, which means that backward elimination was better this time.
On the other hand, we needed to suppress the
<code>fitted probabilities numerically 0 or 1 occurred</code> warnings.
The returned model is perhaps <em>unstable</em> as well and consists of too
many variables.</p>
</details>
</div>
</div>
<div id="outro-3" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Outro</h2>
<div id="remarks-3" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Remarks</h3>
<p>Other prominent classification algorithms:</p>
<ul>
<li>Naive Bayes and other probabilistic approaches,</li>
<li>Support Vector Machines (SVMs) and other kernel methods,</li>
<li>(Artificial) (Deep) Neural Networks.</li>
</ul>
<p>Interestingly, in the next chapter we will note that the logistic regression model
is a special case of a <em>feed-forward single layer neural network</em>.</p>
<p>We will also generalise the binary logistic regression to the case of
a multiclass classification.</p>
<p>The state-of-the art classifiers called
<em>Random Forests</em> and <em>XGBoost</em> (see also: <em>AdaBoost</em>) are based on decision trees.
They tend to be more accurate but – at the same time – they fail to
exhibit the decision trees’ important feature: interpretability.</p>
<p>Trees can also be used for regression tasks, see R package <code>rpart</code>.</p>
</div>
<div id="further-reading-3" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(James et al. <a href="#ref-islr" role="doc-biblioref">2017</a>: Chapters 4 and 8)</span></p>
<p>Other: <span class="citation">(Hastie et al. <a href="#ref-esl" role="doc-biblioref">2017</a>: Chapters 4 and 7 as well as (*) Chapters 9, 10, 13, 15)</span></p>

<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-cart">
<p>Breiman L, Friedman J, Stone CJ, Olshen RA (1984) <em>Classification and regression trees</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-esl">
<p>Hastie T, Tibshirani R, Friedman J (2017) <em>The elements of statistical learning</em>. Springer-Verlag <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-islr">
<p>James G, Witten D, Hastie T, Tibshirani R (2017) <em>An introduction to statistical learning with applications in R</em>. Springer-Verlag <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.</p>
</div>
<div id="ref-id3">
<p>Quinlan R (1986) Induction of decision trees. <em>Machine Learning</em> 1, 81–106.</p>
</div>
<div id="ref-c45">
<p>Quinlan R (1993) <em>C4.5: Programs for machine learning</em>. Morgan Kaufmann Publishers.</p>
</div>
<div id="ref-rpart">
<p>Therneau TM, Atkinson EJ (2019) <em>An introduction to recursive partitioning using the RPART routines</em>. <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-with-k-nearest-neighbours.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shallow-and-deep-neural-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
