<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-regression.html"/>
<link rel="next" href="classification-with-trees-and-linear-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>BEGIN {</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a><ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a><ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a><ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#exercises-in-r-2"><i class="fa fa-check"></i><b>4.4</b> Exercises in R</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-preparing-data"><i class="fa fa-check"></i><b>4.4.1</b> EdStats – Preparing Data</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.4.2</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.4.3</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>4.4.4</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>4.4.5</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.5</b> Outro</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="5.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>5.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a><ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="6.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="6.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>6.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="6.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.5</b> Outro</a><ul>
<li class="chapter" data-level="6.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>7.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>7.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#exercises-in-r-3"><i class="fa fa-check"></i><b>7.4</b> Exercises in R</a><ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>7.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>7.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>7.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.5</b> Outro</a><ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.2</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.2.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.3</b> Outro</a><ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.3.1</b> Remarks</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a><ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-16"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="end.html"><a href="end.html"><i class="fa fa-check"></i>} END</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>A</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>A.1</b> Installing R</a></li>
<li class="chapter" data-level="A.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>A.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="A.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>A.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="A.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>A.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>B</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="B.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>B.1</b> Motivation</a></li>
<li class="chapter" data-level="B.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>B.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="B.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>B.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="B.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>B.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="B.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>B.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="B.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>B.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="B.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>B.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="B.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>B.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="B.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>B.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="B.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>B.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>B.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="B.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>B.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="B.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>B.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="B.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>B.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="B.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>B.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>B.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="B.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>B.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="B.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>B.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="B.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>B.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="B.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>B.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="B.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>B.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="B.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>B.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="B.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>B.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="B.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>B.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>B.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="B.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>B.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="B.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>B.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="B.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>B.7</b> Factors</a><ul>
<li class="chapter" data-level="B.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>B.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="B.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>B.7.2</b> Levels</a></li>
<li class="chapter" data-level="B.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>B.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>B.8</b> Lists</a><ul>
<li class="chapter" data-level="B.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>B.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="B.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>B.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="B.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>B.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="B.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>B.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>B.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>C.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="C.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>C.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="C.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>C.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="C.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>C.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="C.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>C.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="C.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>C.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="C.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>C.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>C.2</b> Common Operations</a><ul>
<li class="chapter" data-level="C.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>C.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="C.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>C.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>C.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="C.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>C.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="C.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>C.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="C.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>C.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>C.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="C.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>C.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="C.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>C.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="C.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>C.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="C.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>C.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="C.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>C.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>C.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>D</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="D.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>D.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="D.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>D.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="D.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>D.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>D.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="D.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>D.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>D.4</b> Common Operations</a></li>
<li class="chapter" data-level="D.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>D.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="D.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>D.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2 2020-05-11 22:28 (16b9c8d)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-k-nearest-neighbours" class="section level1">
<h1><span class="header-section-number">3</span> Classification with K-Nearest Neighbours</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<div id="classification-task" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Classification Task</h3>
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space (each of the <span class="math inline">\(n\)</span> objects
is described by means of <span class="math inline">\(p\)</span> numerical features).</p>
<p>Recall that in supervised learning, with each
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output <span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \\
\end{array}
\right]
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]</span></p>
<div style="margin-top: 1em">

</div>
<p>In this chapter we are interested in <strong>classification</strong> tasks;
we assume that each <span class="math inline">\(y_i\)</span> is a <em>label</em> (e.g., a character string) –
it is of quantitative/categorical type.</p>
<p>Most commonly, we are faced with <strong>binary classification</strong> tasks
where there are only two possible distinct labels.</p>
<p>We traditionally denote them with <span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s.</p>
<p>For example:</p>
<table>
<thead>
<tr class="header">
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>false</td>
<td>true</td>
</tr>
<tr class="odd">
<td>failure</td>
<td>success</td>
</tr>
<tr class="even">
<td>healthy</td>
<td>ill</td>
</tr>
</tbody>
</table>
<p>On the other hand, in <strong>multiclass classification</strong>,
we assume that each <span class="math inline">\(y_i\)</span> takes more than two possible values.</p>
<p>Example plot of a synthetic dataset
with the reference binary <span class="math inline">\(y\)</span>s is given in Figure <a href="classification-with-k-nearest-neighbours.html#fig:classify_intro">1</a>.
The “true” decision boundary is at <span class="math inline">\(X_1=0\)</span> but the classes
slightly overlap (the dataset is a bit noisy).</p>
<div class="figure">
<img src="03-classification-neighbours-figures/classify_intro-1.svg" alt="Figure 1: A synthetic 2D dataset with the true decision boundary at X_1=0" id="fig:classify_intro" />
<p class="caption">Figure 1: A synthetic 2D dataset with the true decision boundary at <span class="math inline">\(X_1=0\)</span></p>
</div>
</div>
<div id="data" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Data</h3>
<p>For illustration, let’s consider the Wine Quality dataset <span class="citation">(Cortez et al. <a href="references.html#ref-wines">2009</a>)</span>
that can be downloaded from the UCI Machine Learning Repository
(<a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality" class="uri">https://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>) –
white wines only.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" title="1">wines &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/winequality-all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb285-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>, <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb285-3" title="3">wines &lt;-<span class="st"> </span>wines[wines<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</a>
<a class="sourceLine" id="cb285-4" title="4">(n &lt;-<span class="st"> </span><span class="kw">nrow</span>(wines)) <span class="co"># number of samples</span></a></code></pre></div>
<pre><code>## [1] 4898</code></pre>
<p>These are Vinho Verde wine samples from the north of Portugal,
see <a href="https://www.vinhoverde.pt/en/homepage" class="uri">https://www.vinhoverde.pt/en/homepage</a>.</p>
<p>There are 11 physicochemical features reported.
Moreover, there is a wine rating (which we won’t consider here)
on the scale 0 (bad) to 10 (excellent)
given by wine experts.</p>
<p>The input matrix <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>
consists of the first 10 numeric variables:</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</a>
<a class="sourceLine" id="cb287-2" title="2"><span class="kw">dim</span>(X)</a></code></pre></div>
<pre><code>## [1] 4898   10</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" title="1"><span class="kw">head</span>(X, <span class="dv">2</span>) <span class="co"># first two rows</span></a></code></pre></div>
<pre><code>##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density
## 1600     0.045                  45                  170   1.001
## 1601     0.049                  14                  132   0.994
##       pH sulphates
## 1600 3.0      0.45
## 1601 3.3      0.49</code></pre>
<p>The 11th variable measures the amount of alcohol (in %).</p>
<p>We will convert this dependent variable to a binary one:</p>
<ul>
<li>0 == (<code>alcohol  &lt; 12</code>) == lower-alcohol wines1</li>
<li>1 == (<code>alcohol &gt;= 12</code>) == higher-alcohol wines</li>
</ul>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" title="1"><span class="co"># recall that TRUE == 1</span></a>
<a class="sourceLine" id="cb291-2" title="2">Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</a>
<a class="sourceLine" id="cb291-3" title="3"><span class="kw">table</span>(Y)</a></code></pre></div>
<pre><code>## Y
##    0    1 
## 4085  813</code></pre>
<p>Now <span class="math inline">\((\mathbf{X},\mathbf{y})\)</span> is a basis for an interesting (yet challenging)
binary classification task.</p>
</div>
<div id="training-and-test-sets" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Training and Test Sets</h3>
<p>Recall that we are genuinely interested in the construction of supervised learning models for the two following purposes:</p>
<ul>
<li><strong>description</strong> – to explain a given dataset in simpler terms,</li>
<li><strong>prediction</strong> – to forecast the values of the dependent variable
for inputs that are yet to be observed.</li>
</ul>
<p>In the latter case:</p>
<ul>
<li>we don’t want our models to <em>overfit</em> to current data,</li>
<li>we want our models to <em>generalise</em> well
to new data.</li>
</ul>
<div style="margin-top: 1em">

</div>
<p>One way to assess if a model has sufficient predictive power is based
on a random <strong>train-test split</strong> of the original dataset:</p>
<ul>
<li><em>training sample</em> (usually 60-80% of the observations) – used to construct a model,</li>
<li><em>test sample</em> (remaining 40-20%) – used to assess the goodness of fit.</li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p><strong>Test sample must not be used in the training phase!</strong> (No cheating!)</p>
</dd>
</dl>
<p>60/40% train-test split in R:</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></a>
<a class="sourceLine" id="cb293-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb293-3" title="3"><span class="kw">head</span>(random_indices) <span class="co"># preview</span></a></code></pre></div>
<pre><code>## [1] 2463 2511 2227  526 4291 2986</code></pre>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" title="1"><span class="co"># first 60% of the indices (they are arranged randomly)</span></a>
<a class="sourceLine" id="cb295-2" title="2"><span class="co"># will constitute the train sample:</span></a>
<a class="sourceLine" id="cb295-3" title="3">train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</a>
<a class="sourceLine" id="cb295-4" title="4">X_train &lt;-<span class="st"> </span>X[train_indices,]</a>
<a class="sourceLine" id="cb295-5" title="5">Y_train &lt;-<span class="st"> </span>Y[train_indices]</a>
<a class="sourceLine" id="cb295-6" title="6"><span class="co"># the remaining indices (40%) go to the test sample:</span></a>
<a class="sourceLine" id="cb295-7" title="7">X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</a>
<a class="sourceLine" id="cb295-8" title="8">Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</a></code></pre></div>
</div>
<div id="discussed-methods" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Discussed Methods</h3>
<p>Our aim is to build a classifier that takes 10 wine physicochemical
features and determines whether it’s a “strong” wine.</p>
<p>We will discuss 3 simple and educational (yet practically useful)
classification algorithms:</p>
<ul>
<li><em>K-nearest neighbour scheme</em> – this chapter,</li>
<li><em>Decision trees</em> – the next chapter,</li>
<li><em>Logistic regression</em> – the next chapter.</li>
</ul>
</div>
</div>
<div id="k-nearest-neighbour-classifier" class="section level2">
<h2><span class="header-section-number">3.2</span> K-nearest Neighbour Classifier</h2>
<div id="introduction-4" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Introduction</h3>
<dl>
<dt>Rule.</dt>
<dd><p>“If you don’t know what to do in a situation, just act like the people around you”</p>
</dd>
</dl>
<p>For some integer <span class="math inline">\(K\ge 1\)</span>, the <strong>K-Nearest Neighbour (<em>K-NN</em>) Classifier</strong>
proceeds as follows.</p>
<p>To classify a new point <span class="math inline">\(\mathbf{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>find the <span class="math inline">\(K\)</span> nearest neighbours of a given point <span class="math inline">\(\mathbf{x}&#39;\)</span> amongst the points in the train set,
denoted <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>:
<ol style="list-style-type: lower-alpha">
<li>compute the Euclidean distances between <span class="math inline">\(\mathbf{x}&#39;\)</span> and each <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> from the train set,
<span class="math display">\[d_i = \|\mathbf{x}&#39;-\mathbf{x}_{i,\cdot}\|\]</span></li>
<li>order <span class="math inline">\(d_i\)</span>s in increasing order,
<span class="math inline">\(d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}\)</span></li>
<li>pick first <span class="math inline">\(K\)</span> indices (these are the <em>nearest</em> neighbours)</li>
</ol></li>
<li>fetch the corresponding reference labels <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span></li>
<li>return their <em>mode</em> as a result, i.e., the most frequently occurring label (a.k.a. <em>majority vote</em>)</li>
</ol>
<!-- > If a mode is not unique, return a randomly chosen mode (ties are broken at random). -->
<p>Here is how <span class="math inline">\(K\)</span>-NN classifier works on a synthetic 2D dataset.
Firstly let’s consider <span class="math inline">\(K=1\)</span>, see Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig_plot_knn1">2</a>.
Gray and pink regions depict how new points would be classified.
In particular 1-NN is “greedy” in the sense that we just
locate the nearest point.</p>
<div class="figure">
<img src="03-classification-neighbours-figures/fig_plot_knn1-1.svg" alt="Figure 2: 1-NN class bounds for our 2D synthetic dataset" id="fig:fig_plot_knn1" />
<p class="caption">Figure 2: 1-NN class bounds for our 2D synthetic dataset</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>(*) 1-NN classification is essentially based
on a dataset’s so-called Voronoi diagram.</p>
</dd>
</dl>
<p>Increasing <span class="math inline">\(K\)</span> somehow smoothens the decision boundary (this makes it
less “local” and more “global”).
Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig_plot_knn3">3</a> depicts the <span class="math inline">\(K=3\)</span> case.</p>
<div class="figure">
<img src="03-classification-neighbours-figures/fig_plot_knn3-1.svg" alt="Figure 3: 3-NN class bounds for our 2D synthetic dataset" id="fig:fig_plot_knn3" />
<p class="caption">Figure 3: 3-NN class bounds for our 2D synthetic dataset</p>
</div>
<div class="figure">
<img src="03-classification-neighbours-figures/fig_plot_knn25-1.svg" alt="Figure 4: 25-NN class bounds for our 2D synthetic dataset" id="fig:fig_plot_knn25" />
<p class="caption">Figure 4: 25-NN class bounds for our 2D synthetic dataset</p>
</div>
<p>Recall that the “true” decision boundary for this synthetic dataset
is at <span class="math inline">\(X_1=0\)</span>. The 25-NN classifier did quite a good job, see Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig_plot_knn25">4</a>.</p>
</div>
<div id="example-in-r" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Example in R</h3>
<p>We shall be calling the <code>knn()</code> function from package <code>FNN</code>
to classify the points from the test sample
extracted from the <code>wines</code> dataset:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</a></code></pre></div>
<p>Let’s make prediction using the 5-nn classifier:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" title="1">Y_knn5 &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb297-2" title="2"><span class="kw">head</span>(Y_test, <span class="dv">28</span>) <span class="co"># True Ys</span></a></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" title="1"><span class="kw">head</span>(Y_knn5, <span class="dv">28</span>) <span class="co"># Predicted Ys</span></a></code></pre></div>
<pre><code>##  [1] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.8173469</code></pre>
<p>9-nn classifier:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" title="1">Y_knn9 &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb303-2" title="2"><span class="kw">head</span>(Y_test, <span class="dv">28</span>) <span class="co"># True Ys</span></a></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" title="1"><span class="kw">head</span>(Y_knn9, <span class="dv">28</span>) <span class="co"># Predicted Ys</span></a></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb307-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.8193878</code></pre>
</div>
<div id="feature-engineering" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Feature Engineering</h3>
<p>Note that the Euclidean distance that we used above
implicitly assumes that every feature (independent variable)
is on the same scale.</p>
<p>However, when dealing with, e.g., physical quantities,
we often perform conversions of units of measurement (kg → g, feet → m etc.).</p>
<p>Transforming a single feature may drastically change the metric
structure of the dataset
and therefore highly affect the obtained predictions.</p>
<p>To “bring data to the same scale”, we often apply a trick called <strong>standardisation</strong>.</p>
<p>Computing the so-called <strong>Z-scores</strong> of the <span class="math inline">\(j\)</span>-th feature, <span class="math inline">\(\mathbf{x}_{\cdot,j}\)</span>,
is done by subtracting from each observation the sample mean and dividing the result by the sample
standard deviation:</p>
<p><span class="math display">\[z_{i,j} = \frac{x_{i,j}-\bar{x}_{\cdot,j}}{s_{x_{\cdot,j}}}\]</span></p>
<p>This a new feature <span class="math inline">\(\mathbf{z}_{\cdot,j}\)</span> that always has mean 0 and standard deviation of 1.</p>
<p>Moreover, it is <em>unit-less</em> (e.g., we divide a value in kgs by a value in kgs,
the units are cancelled out).
This, amongst others, prevents one of the features from dominating
the other ones.</p>
<p>Z-scores are easy to interpret, e.g., 0.5 denotes an observation
that is 0.5 standard deviations above the mean
and -3 informs us that a value is 3 standard deviations below the mean.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) If data are normally distributed (bell-shaped histogram),
with very high probability, most (expected value is 99.74%) observations
should have Z-scores between -3 and 3. Those that don’t, are
“suspicious”, maybe they are outliers? We should inspect them manually.</p>
</dd>
</dl>
<p>Let’s compute <code>Z_train</code> and <code>Z_test</code>,
being the standardised versions of <code>X_train</code>
and <code>X_test</code>, respectively.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" title="1">means &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, mean) <span class="co"># column means</span></a>
<a class="sourceLine" id="cb309-2" title="2">sds   &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, sd)   <span class="co"># column standard deviations</span></a>
<a class="sourceLine" id="cb309-3" title="3">Z_train &lt;-<span class="st"> </span>X_train <span class="co"># copy</span></a>
<a class="sourceLine" id="cb309-4" title="4">Z_test  &lt;-<span class="st"> </span>X_test  <span class="co"># copy</span></a>
<a class="sourceLine" id="cb309-5" title="5"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(X)) {</a>
<a class="sourceLine" id="cb309-6" title="6">    Z_train[,j] &lt;-<span class="st"> </span>(Z_train[,j]<span class="op">-</span>means[j])<span class="op">/</span>sds[j]</a>
<a class="sourceLine" id="cb309-7" title="7">    Z_test[,j]  &lt;-<span class="st"> </span>(Z_test[,j] <span class="op">-</span>means[j])<span class="op">/</span>sds[j]</a>
<a class="sourceLine" id="cb309-8" title="8">}</a></code></pre></div>
<p>Note that we have transformed the training and test sample in the very same
way. Computing means and standard deviations separately for these two datasets
is a common error – it is the training set that we use in the course of the
learning process.
The above can be re-written as:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb310-1" title="1">Z_train &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</a>
<a class="sourceLine" id="cb310-2" title="2">Z_test  &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_test,  <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</a></code></pre></div>
<p>See Figure <a href="classification-with-k-nearest-neighbours.html#fig:standardise_depict_hist">5</a> for an illustration.
Note that the righthand figures (histograms of standardised variables)
are on the same scale now.</p>
<div class="figure">
<img src="03-classification-neighbours-figures/standardise_depict_hist-1.svg" alt="Figure 5: Empirical distribution of two variables (pH on the top, fixed.acidity on the bottom) before (left) and after (right) standardising" id="fig:standardise_depict_hist" />
<p class="caption">Figure 5: Empirical distribution of two variables (pH on the top, fixed.acidity on the bottom) before (left) and after (right) standardising</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>Of course, standardisation is only about shifting and scaling, it
preserves the shape of the distribution. If the original variable
is right skewed or bimodal, its standardised version will remain as such.</p>
</dd>
</dl>
<p>Let’s compute the accuracy of K-NN classifiers acting on standardised data.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" title="1">Y_knn5s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb311-2" title="2"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.9142857</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" title="1">Y_knn9s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb313-2" title="2"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.9137755</code></pre>
<p>The accuracy is much better.</p>
<p>Standardisation is an example of <em>feature engineering</em>.</p>
<p>Good models rarely work well “straight out of the box” – if that was the case,
we wouldn’t need data scientists and machine learning engineers!</p>
<p>To increase models’ accuracy, we often spend a lot of time:</p>
<ul>
<li>cleansing data (e.g., removing outliers)</li>
<li>extracting new features</li>
<li>transforming existing features</li>
<li>trying to find a set of features that are relevant</li>
</ul>
<p>This is the “more art than science” part of data science (sic!), and
hence most textbooks are not really eager for discussing such topics
(including this one).</p>
<p>Sorry, this is sad but true. The solutions that work well in the case of dataset
A may fail in the B case and vice versa. However, the more exercises you solve,
the greater the arsenal of ideas/possible approaches you will have at hand
when dealing with real-world problems.</p>
<p>Feature selection – example (manually selected columns):</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" title="1">features &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;density&quot;</span>, <span class="st">&quot;residual.sugar&quot;</span>)</a>
<a class="sourceLine" id="cb315-2" title="2">Y_knn5s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train[,features], Z_test[,features],</a>
<a class="sourceLine" id="cb315-3" title="3">    Y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb315-4" title="4"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.9163265</code></pre>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb317-1" title="1">Y_knn9s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train[,features], Z_test[,features],</a>
<a class="sourceLine" id="cb317-2" title="2">    Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb317-3" title="3"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.925</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Try to find a combination of 2-4 features (by guessing or applying magic tricks)
that increases the accuracy of a <span class="math inline">\(K\)</span>-NN classifier on this dataset.</p>
</div>
</div>
</div>
<div id="model-assessment-and-selection" class="section level2">
<h2><span class="header-section-number">3.3</span> Model Assessment and Selection</h2>
<div id="performance-metrics" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Performance Metrics</h3>
<!--

More metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

-->
<p>Recall that <span class="math inline">\(y_i\)</span> denotes the true label associated with the <span class="math inline">\(i\)</span>-th observation.</p>
<p>Let <span class="math inline">\(\hat{y}_i\)</span> denote the classifier’s output for a given <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>.</p>
<p>Ideally, we’d wish that <span class="math inline">\(\hat{y}_i=y_i\)</span>.</p>
<p>Sadly, in practice we will make errors.</p>
<p>Here are the 4 possible situations (true vs. predicted label):</p>
<table>
<thead>
<tr class="header">
<th>.</th>
<th><span class="math inline">\(y_i=0\)</span></th>
<th><span class="math inline">\(y_i=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{y}_i=0\)</span></td>
<td><strong>True Negative</strong></td>
<td>False Negative (Type II error)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{y}_i=1\)</span></td>
<td>False Positive (Type I error)</td>
<td><strong>True Positive</strong></td>
</tr>
</tbody>
</table>
<p>Note that the terms <strong>positive</strong> and <strong>negative</strong> refer to
the classifier’s output, i.e., occur when <span class="math inline">\(\hat{y}_i\)</span> is equal to <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>, respectively.</p>
<p>A <strong>confusion matrix</strong> is used to summarise
the correctness of predictions for the whole sample:</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb319-2" title="2">(C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test))</a></code></pre></div>
<pre><code>##       Y_test
## Y_pred    0    1
##      0 1607  133
##      1   36  184</code></pre>
<p>For example,</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" title="1">C[<span class="dv">1</span>,<span class="dv">1</span>] <span class="co"># number of TNs</span></a></code></pre></div>
<pre><code>## [1] 1607</code></pre>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" title="1">C[<span class="dv">2</span>,<span class="dv">1</span>] <span class="co"># number of FPs</span></a></code></pre></div>
<pre><code>## [1] 36</code></pre>
<p><strong>Accuracy</strong> is the ratio of the correctly classified instances
to all the instances.</p>
<p>In other words, it is the probability of making a correct prediction.</p>
<p><span class="math display">\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
\]</span>
where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function,
<span class="math inline">\(\mathbb{I}(l)=1\)</span> if logical condition <span class="math inline">\(l\)</span> is true and <span class="math inline">\(0\)</span> otherwise.</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.9137755</code></pre>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" title="1">(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C) <span class="co"># equivalently</span></a></code></pre></div>
<pre><code>## [1] 0.9137755</code></pre>
<p>In many applications we are dealing with <strong>unbalanced problems</strong>, where
the case <span class="math inline">\(y_i=1\)</span> is relatively rare,
yet predicting it correctly is much more important than being
accurate with respect to class <span class="math inline">\(0\)</span>.</p>
<dl>
<dt>Remark.</dt>
<dd><p>Think of medical applications, e.g., HIV testing
or tumour diagnosis.</p>
</dd>
</dl>
<p>In such a case, <em>accuracy</em> as a metric fails to quantify what we are aiming for.</p>
<dl>
<dt>Remark.</dt>
<dd><p>If only 1% of the cases have true <span class="math inline">\(y_i=1\)</span>,
then a dummy classifier that always
outputs <span class="math inline">\(\hat{y}_i=0\)</span> has 99% accuracy.</p>
</dd>
</dl>
<p>Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address this problem.</p>
<p><strong>Precision</strong></p>
<p><span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\]</span></p>
<p>If the classifier outputs <span class="math inline">\(1\)</span>,
what is the probability that this is indeed true?</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># Precision</span></a></code></pre></div>
<pre><code>## [1] 0.8363636</code></pre>
<p><strong>Recall</strong> (a.k.a. sensitivity, hit rate or true positive rate)</p>
<p><span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}
\]</span></p>
<p>If the true class is <span class="math inline">\(1\)</span>, what is the probability that the classifier
will detect it?</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]) <span class="co"># Recall</span></a></code></pre></div>
<pre><code>## [1] 0.5804416</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>Precision or recall? It depends on an application.
Think of medical diagnosis, medical screening, plagiarism detection,
etc. — which measure is more important in each of the settings listed?</p>
</dd>
</dl>
<p>As a compromise, we can use the <strong>F-measure</strong>
(a.k.a. <span class="math inline">\(F_1\)</span>-measure),
which is the harmonic mean of precision
and recall:</p>
<p><span class="math display">\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}
\]</span></p>
<div class="exercise"><strong>Exercise.</strong>
<p>Show that the above equality holds.</p>
</div>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># F</span></a></code></pre></div>
<pre><code>## [1] 0.6852886</code></pre>
<p>The following function can come in handy in the future:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" title="1">get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</a>
<a class="sourceLine" id="cb335-2" title="2">{</a>
<a class="sourceLine" id="cb335-3" title="3">    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></a>
<a class="sourceLine" id="cb335-4" title="4">    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb335-5" title="5">    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></a>
<a class="sourceLine" id="cb335-6" title="6">      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></a>
<a class="sourceLine" id="cb335-7" title="7">      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></a>
<a class="sourceLine" id="cb335-8" title="8">      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></a>
<a class="sourceLine" id="cb335-9" title="9">      <span class="co"># Confusion matrix items:</span></a>
<a class="sourceLine" id="cb335-10" title="10">      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb335-11" title="11">      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb335-12" title="12">    ) <span class="co"># return a named vector</span></a>
<a class="sourceLine" id="cb335-13" title="13">}</a></code></pre></div>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" title="1"><span class="kw">get_metrics</span>(Y_pred, Y_test)</a></code></pre></div>
<pre><code>##          Acc         Prec          Rec            F 
##    0.9137755    0.8363636    0.5804416    0.6852886 
##           TN           FN           FP           TP 
## 1607.0000000  133.0000000   36.0000000  184.0000000</code></pre>
</div>
<div id="how-to-choose-k-for-k-nn-classification" class="section level3">
<h3><span class="header-section-number">3.3.2</span> How to Choose K for K-NN Classification?</h3>
<p>We haven’t yet considered the question which <span class="math inline">\(K\)</span> yields <em>the best</em>
classifier.</p>
<p>Best == one that has the highest <em>predictive power</em>.</p>
<p>Best == with respect to some chosen metric (accuracy, recall, precision, F-measure, …)</p>
<p>Let’s study how the metrics on the test set change as functions of the number of nearest neighbours considered, <span class="math inline">\(K\)</span>.</p>
<p>Auxiliary function:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb338-1" title="1">knn_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(k, X_train, X_test, Y_train, Y_test)</a>
<a class="sourceLine" id="cb338-2" title="2">{</a>
<a class="sourceLine" id="cb338-3" title="3">    Y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span>k) <span class="co"># classify</span></a>
<a class="sourceLine" id="cb338-4" title="4">    <span class="kw">get_metrics</span>(Y_pred, Y_test)</a>
<a class="sourceLine" id="cb338-5" title="5">}</a></code></pre></div>
<p>For example:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" title="1"><span class="kw">knn_metrics</span>(<span class="dv">5</span>, Z_train, Z_test, Y_train, Y_test)</a></code></pre></div>
<pre><code>##          Acc         Prec          Rec            F 
##    0.9102041    0.8025751    0.5899054    0.6800000 
##           TN           FN           FP           TP 
## 1597.0000000  130.0000000   46.0000000  187.0000000</code></pre>
<p>Example call to evaluate metrics as a function of different <span class="math inline">\(K\)</span>s:</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" title="1">Ks &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dt">by=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb341-2" title="2">Ps &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">t</span>(</a>
<a class="sourceLine" id="cb341-3" title="3">    <span class="kw">sapply</span>(Ks, <span class="co"># on each element in this vector</span></a>
<a class="sourceLine" id="cb341-4" title="4">        knn_metrics,     <span class="co"># apply this function</span></a>
<a class="sourceLine" id="cb341-5" title="5">        Z_train, Z_test, Y_train, Y_test <span class="co"># aux args</span></a>
<a class="sourceLine" id="cb341-6" title="6">    )))</a></code></pre></div>
<dl>
<dt>Remark.</dt>
<dd><p>Note that <code>sapply(X, f, arg1, arg2, ...)</code>
outputs a list <code>Y</code> such that
<code>Y[[i]] = f(X[i], arg1, arg2, ...)</code>
which is then simplified to a matrix.</p>
</dd>
<dt>Remark.</dt>
<dd><p>We transpose this result, <code>t()</code>, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g., why <code>t()</code>, play with
the code yourself – it’s fun fun fun.</p>
</dd>
</dl>
<p>Example results:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" title="1"><span class="kw">round</span>(<span class="kw">cbind</span>(<span class="dt">K=</span>Ks, Ps), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##     K  Acc Prec  Rec    F   TN  FN FP  TP
## 1   1 0.92 0.76 0.72 0.74 1573  90 70 227
## 2   3 0.92 0.79 0.66 0.72 1588 108 55 209
## 3   5 0.91 0.80 0.59 0.68 1597 130 46 187
## 4   7 0.91 0.82 0.56 0.67 1605 140 38 177
## 5   9 0.91 0.83 0.57 0.68 1606 135 37 182
## 6  11 0.91 0.84 0.57 0.68 1608 135 35 182
## 7  13 0.91 0.83 0.57 0.67 1605 137 38 180
## 8  15 0.91 0.82 0.55 0.66 1606 144 37 173
## 9  17 0.91 0.83 0.54 0.65 1607 147 36 170
## 10 19 0.90 0.80 0.52 0.63 1602 151 41 166</code></pre>
<p>Figure <a href="classification-with-k-nearest-neighbours.html#fig:whichK5">6</a> is worth a thousand tables though (see <code>?matplot</code> in R). The reader is kindly asked to draw conclusions themself.</p>
<div class="figure">
<img src="03-classification-neighbours-figures/whichK5-1.svg" alt="Figure 6: Performance of K-nn classifiers as a function of K for standardised and raw data" id="fig:whichK5" />
<p class="caption">Figure 6: Performance of <span class="math inline">\(K\)</span>-nn classifiers as a function of <span class="math inline">\(K\)</span> for standardised and raw data</p>
</div>
<!--

(\*) **ROC** (Receiver Operating Characteristic) curve:


```r
TPR <- Ps$TP/(Ps$TP+Ps$FN) # True Positive Rate (recall)
FPR <- Ps$FP/(Ps$FP+Ps$TN) # False Positive Rate
plot(FPR, TPR, asp=1, xlim=c(0,1), ylim=c(0,1))
abline(a=0, b=1, lty=3)
```

![plot of chunk unnamed-chunk-1](03-classification-neighbours-figures/unnamed-chunk-1-1){#fig:unnamed-chunk-1}

-->
</div>
<div id="training-validation-and-test-sets" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Training, Validation and Test sets</h3>
<p>In the <span class="math inline">\(K\)</span>-NN classification task, there are many hyperparameters to tune up:</p>
<ul>
<li><p>Which <span class="math inline">\(K\)</span> should we choose?</p></li>
<li><p>Should we standardise the dataset?</p></li>
<li><p>Which variables should be taken into account when computing the Euclidean distance?</p></li>
</ul>
<!--
- Which metric should be used?
-->
<dl>
<dt>Remark.</dt>
<dd><p><strong>If we select the best hyperparameter set based on test
sample error, we will run into the trap of overfitting again</strong>.
This time we’ll be overfitting to the test set — the model that is optimal
for a given test sample doesn’t have to generalise well to other test samples (!).</p>
</dd>
</dl>
<p>In order to overcome this problem,
we can perform a random <strong>train-validation-test split</strong> of the original dataset:</p>
<ul>
<li><em>training sample</em> (e.g., 60%) – used to construct the models</li>
<li><em>validation sample</em> (e.g., 20%) – used to tune the hyperparameters of the classifier</li>
<li><em>test sample</em> (e.g., 20%) – used to assess the goodness of fit</li>
</ul>
<!--
By the way, this is how most data mining competitions are assessed --
you will never have access to the final test sample used
to determine the winner. The best you can do is to "guess".
-->
<p>An example way to perform a 60/20/20% train-validation-test split:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb344-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></a>
<a class="sourceLine" id="cb344-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb344-3" title="3">n1 &lt;-<span class="st"> </span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)</a>
<a class="sourceLine" id="cb344-4" title="4">n2 &lt;-<span class="st"> </span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb344-5" title="5">X2_train &lt;-<span class="st"> </span>X[random_indices[<span class="dv">1</span>     <span class="op">:</span>n1], ]</a>
<a class="sourceLine" id="cb344-6" title="6">Y2_train &lt;-<span class="st"> </span>Y[random_indices[<span class="dv">1</span>     <span class="op">:</span>n1]  ]</a>
<a class="sourceLine" id="cb344-7" title="7">X2_valid &lt;-<span class="st"> </span>X[random_indices[(n1<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n2], ]</a>
<a class="sourceLine" id="cb344-8" title="8">Y2_valid &lt;-<span class="st"> </span>Y[random_indices[(n1<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n2]  ]</a>
<a class="sourceLine" id="cb344-9" title="9">X2_test  &lt;-<span class="st"> </span>X[random_indices[(n2<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n ], ]</a>
<a class="sourceLine" id="cb344-10" title="10">Y2_test  &lt;-<span class="st"> </span>Y[random_indices[(n2<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n ]  ]</a>
<a class="sourceLine" id="cb344-11" title="11"><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X2_train)<span class="op">+</span><span class="kw">nrow</span>(X2_valid)<span class="op">+</span><span class="kw">nrow</span>(X2_test)</a>
<a class="sourceLine" id="cb344-12" title="12">    <span class="op">==</span><span class="st"> </span><span class="kw">nrow</span>(X))</a></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Find the best <span class="math inline">\(K\)</span> on the validation set and compute the error metrics
on the test set.</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>(*) If our dataset is too small,
we can use various <em>cross-validation</em> techniques
instead of a train-validate-test split.
<!-- TODO: see exercise.... --></p>
</dd>
</dl>
</div>
</div>
<div id="implementing-a-k-nn-classifier" class="section level2">
<h2><span class="header-section-number">3.4</span> Implementing a K-NN Classifier (*)</h2>
<div id="factor-data-type" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Factor Data Type</h3>
<p>Recall that (see Appendix B for more details)
<code>factor</code> type in R is a very convenient means to encode categorical data
(such as <span class="math inline">\(\mathbf{y}\)</span>):</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" title="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>)</a>
<a class="sourceLine" id="cb345-2" title="2">f &lt;-<span class="st"> </span><span class="kw">factor</span>(x, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>))</a>
<a class="sourceLine" id="cb345-3" title="3">f</a></code></pre></div>
<pre><code>## [1] yes no  no  yes no 
## Levels: no yes</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" title="1"><span class="kw">table</span>(f) <span class="co"># counts</span></a></code></pre></div>
<pre><code>## f
##  no yes 
##   3   2</code></pre>
<p>Internally, objects of type <code>factor</code> are represented as integer vectors
with elements in <span class="math inline">\(\{1,\dots,M\}\)</span>, where <span class="math inline">\(M\)</span> is the number of possible levels.</p>
<p>Labels, used to “decipher” the numeric codes, are stored separately.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" title="1"><span class="kw">as.numeric</span>(f) <span class="co"># 2nd label, 1st label, 1st label etc.</span></a></code></pre></div>
<pre><code>## [1] 2 1 1 2 1</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" title="1"><span class="kw">levels</span>(f)</a></code></pre></div>
<pre><code>## [1] &quot;no&quot;  &quot;yes&quot;</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" title="1"><span class="kw">levels</span>(f) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;failure&quot;</span>, <span class="st">&quot;success&quot;</span>) <span class="co"># re-encode</span></a>
<a class="sourceLine" id="cb353-2" title="2">f</a></code></pre></div>
<pre><code>## [1] success failure failure success failure
## Levels: failure success</code></pre>
</div>
<div id="main-routine" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Main Routine (*)</h3>
<p>Let’s implement a K-NN classifier ourselves
by using a top-bottom approach.</p>
<p>We will start with a general description of the admissible inputs
and the expected output.</p>
<p>Then we will arrange the processing of data into
conveniently manageable chunks.</p>
<p>The function’s declaration will look like:</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" title="1">our_knn &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb355-2" title="2">    <span class="co"># k=1 denotes a parameter with a default value</span></a>
<a class="sourceLine" id="cb355-3" title="3">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb355-4" title="4">}</a></code></pre></div>
<p>Load an example dataset on which we will test our algorithm:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb356-1" title="1">wines &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/winequality-all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb356-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>, <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb356-3" title="3">wines &lt;-<span class="st"> </span>wines[wines<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</a>
<a class="sourceLine" id="cb356-4" title="4">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</a>
<a class="sourceLine" id="cb356-5" title="5">Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</a></code></pre></div>
<p>Note that <code>Y</code> is now a factor object.</p>
<p>Train-test split:</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb357-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb357-3" title="3">train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</a>
<a class="sourceLine" id="cb357-4" title="4">X_train &lt;-<span class="st"> </span>X[train_indices,]</a>
<a class="sourceLine" id="cb357-5" title="5">Y_train &lt;-<span class="st"> </span>Y[train_indices]</a>
<a class="sourceLine" id="cb357-6" title="6">X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</a>
<a class="sourceLine" id="cb357-7" title="7">Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</a></code></pre></div>
<p>First, we should specify the type and form of the arguments
we’re expecting:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb358-1" title="1"><span class="co"># this is the body of our_knn() - part 1</span></a>
<a class="sourceLine" id="cb358-2" title="2"><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_train), <span class="kw">is.matrix</span>(X_train))</a>
<a class="sourceLine" id="cb358-3" title="3"><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_test), <span class="kw">is.matrix</span>(X_test))</a>
<a class="sourceLine" id="cb358-4" title="4"><span class="kw">stopifnot</span>(<span class="kw">is.factor</span>(Y_train))</a>
<a class="sourceLine" id="cb358-5" title="5"><span class="kw">stopifnot</span>(<span class="kw">ncol</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">ncol</span>(X_test))</a>
<a class="sourceLine" id="cb358-6" title="6"><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">length</span>(Y_train))</a>
<a class="sourceLine" id="cb358-7" title="7"><span class="kw">stopifnot</span>(k <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb358-8" title="8">n_train &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_train)</a>
<a class="sourceLine" id="cb358-9" title="9">n_test  &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</a>
<a class="sourceLine" id="cb358-10" title="10">p &lt;-<span class="st"> </span><span class="kw">ncol</span>(X_train)</a>
<a class="sourceLine" id="cb358-11" title="11">M &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">levels</span>(Y_train))</a></code></pre></div>
<p>Therefore,</p>
<p><span class="math inline">\(\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}\)</span>,
<span class="math inline">\(\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}\)</span> and
<span class="math inline">\(\mathtt{Y\_train}\in\{1,\dots,M\}^{\mathtt{n\_train}}\)</span></p>
<dl>
<dt>Remark.</dt>
<dd><p>Recall that R <code>factor</code> objects are internally encoded as integer vectors.</p>
</dd>
</dl>
<p>Next, we will call the (to-be-done) function <code>our_get_knnx()</code>,
which seeks nearest neighbours of all the points:</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb359-1" title="1"><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></a>
<a class="sourceLine" id="cb359-2" title="2"><span class="co"># where nn_indices[i,j] denotes the index of</span></a>
<a class="sourceLine" id="cb359-3" title="3"><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></a>
<a class="sourceLine" id="cb359-4" title="4"><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></a>
<a class="sourceLine" id="cb359-5" title="5">nn_indices &lt;-<span class="st"> </span><span class="kw">our_get_knnx</span>(X_train, X_test, k)</a></code></pre></div>
<p>Then, for each point in <code>X_test</code>,
we fetch the labels corresponding to its nearest neighbours
and compute their mode:</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_test) <span class="co"># vector of length n_test</span></a>
<a class="sourceLine" id="cb360-2" title="2"><span class="co"># For now we will operate on the integer labels in {1,...,M}</span></a>
<a class="sourceLine" id="cb360-3" title="3">Y_train_int &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Y_train)</a>
<a class="sourceLine" id="cb360-4" title="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</a>
<a class="sourceLine" id="cb360-5" title="5">    <span class="co"># Get the labels of the NNs of the i-th point:</span></a>
<a class="sourceLine" id="cb360-6" title="6">    nn_labels_i &lt;-<span class="st"> </span>Y_train_int[nn_indices[i,]]</a>
<a class="sourceLine" id="cb360-7" title="7">    <span class="co"># Compute the mode (majority vote):</span></a>
<a class="sourceLine" id="cb360-8" title="8">    Y_pred[i] &lt;-<span class="st"> </span><span class="kw">our_mode</span>(nn_labels_i) <span class="co"># in {1,...,M}</span></a>
<a class="sourceLine" id="cb360-9" title="9">}</a></code></pre></div>
<p>Finally, we should convert the resulting integer vector
to an object of type <code>factor</code>:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" title="1"><span class="co"># Convert Y_pred to factor:</span></a>
<a class="sourceLine" id="cb361-2" title="2"><span class="kw">return</span>(<span class="kw">factor</span>(Y_pred, <span class="dt">labels=</span><span class="kw">levels</span>(Y_train)))</a></code></pre></div>
<!--
**Test-driven development** -- before writing


```r
test_our_knn <- function() {
    # ...
}
```



```r
test_our_mode <- function() {
    stopifnot(our_mode(c(1, 1, 1, 1)) == 1)
    stopifnot(our_mode(c(2, 2, 2, 2)) == 2)
    stopifnot(our_mode(c(3, 1, 3, 3)) == 3)
    stopifnot(our_mode(c(1, 1, 3, 3, 2)) %in% c(1, 3))
}
```


-->
</div>
<div id="mode" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Mode</h3>
<p>To implement the mode, we can use the <code>tabulate()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?tabulate</code>.</p>
</div>
<p>For example:</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb362-1" title="1"><span class="kw">tabulate</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 4 2 0 0 1</code></pre>
<p>There might be multiple modes – in such a case, we should pick one at random.</p>
<p>For that, we can use the <code>sample()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?sample</code>.
Note that its behaviour is different when it’s first argument is a vector of length 1.</p>
</div>
<p>An example implementation:</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb364-1" title="1">our_mode &lt;-<span class="st"> </span><span class="cf">function</span>(Y) {</a>
<a class="sourceLine" id="cb364-2" title="2">    <span class="co"># tabulate() will take care of</span></a>
<a class="sourceLine" id="cb364-3" title="3">    <span class="co"># checking the correctness of Y</span></a>
<a class="sourceLine" id="cb364-4" title="4">    t &lt;-<span class="st"> </span><span class="kw">tabulate</span>(Y)</a>
<a class="sourceLine" id="cb364-5" title="5">    mode_candidates &lt;-<span class="st"> </span><span class="kw">which</span>(t <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(t))</a>
<a class="sourceLine" id="cb364-6" title="6">    <span class="cf">if</span> (<span class="kw">length</span>(mode_candidates) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="kw">return</span>(mode_candidates)</a>
<a class="sourceLine" id="cb364-7" title="7">    <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">sample</span>(mode_candidates, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb364-8" title="8">}</a></code></pre></div>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb365-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb367-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb369-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>))</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb371-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb373-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
</div>
<div id="nn-search-routines" class="section level3">
<h3><span class="header-section-number">3.4.4</span> NN Search Routines (*)</h3>
<p>Last but not least, we should implement the <code>our_get_knnx()</code> function.</p>
<p>It is the function responsible for seeking the indices of nearest neighbours.</p>
<p>It turns out this function will actually constitute the K-NN classifier’s performance
bottleneck in case of big data samples.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb375-1" title="1"><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></a>
<a class="sourceLine" id="cb375-2" title="2"><span class="co"># where nn_indices[i,j] denotes the index of</span></a>
<a class="sourceLine" id="cb375-3" title="3"><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></a>
<a class="sourceLine" id="cb375-4" title="4"><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></a>
<a class="sourceLine" id="cb375-5" title="5">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb375-6" title="6">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb375-7" title="7">}</a></code></pre></div>
<p>A naive approach to <code>our_get_knnx()</code> relies on computing all pairwise distances,
and sorting them.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb376-1" title="1">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb376-2" title="2">    n_test &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</a>
<a class="sourceLine" id="cb376-3" title="3">    nn_indices &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA_real_</span>, <span class="dt">nrow=</span>n_test, <span class="dt">ncol=</span>k)</a>
<a class="sourceLine" id="cb376-4" title="4">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</a>
<a class="sourceLine" id="cb376-5" title="5">        d &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(x)</a>
<a class="sourceLine" id="cb376-6" title="6">            <span class="kw">sqrt</span>(<span class="kw">sum</span>((x<span class="op">-</span>X_test[i,])<span class="op">^</span><span class="dv">2</span>)))</a>
<a class="sourceLine" id="cb376-7" title="7">        <span class="co"># now d[j] is the distance</span></a>
<a class="sourceLine" id="cb376-8" title="8">        <span class="co"># between X_train[j,] and X_test[i,]</span></a>
<a class="sourceLine" id="cb376-9" title="9">        nn_indices[i,] &lt;-<span class="st"> </span><span class="kw">order</span>(d)[<span class="dv">1</span><span class="op">:</span>k]</a>
<a class="sourceLine" id="cb376-10" title="10">    }</a>
<a class="sourceLine" id="cb376-11" title="11">    nn_indices</a>
<a class="sourceLine" id="cb376-12" title="12">}</a></code></pre></div>
<p>A comparison with <code>FNN:knn()</code>:</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" title="1"><span class="kw">system.time</span>(Ya &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.128   0.000   0.127</code></pre>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" title="1"><span class="kw">system.time</span>(Yb &lt;-<span class="st"> </span><span class="kw">our_knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##  15.310   0.012  15.322</code></pre>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" title="1"><span class="kw">mean</span>(Ya <span class="op">==</span><span class="st"> </span>Yb) <span class="co"># 1.0 on perfect match</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Both functions return identical results but our implementation is “slightly” slower.</p>
<p><code>FNN:knn()</code> is efficiently written in C++, which is a compiled programming language.</p>
<p>R, on the other hand (just like Python and Matlab) is interpreted, therefore
as a rule of thumb we should consider it an order of magnitude slower (see, however, the Julia language).</p>
<p>Let’s substitute our naive implementation with the equivalent one,
but written in C++ (available in the <code>FNN</code> package).</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Note that we can write a C++ implementation ourselves,
see the Rcpp package for seamless R and C++ integration.</p>
</dd>
</dl>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" title="1">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb383-2" title="2">    <span class="co"># this is used by our_knn()</span></a>
<a class="sourceLine" id="cb383-3" title="3">    FNN<span class="op">::</span><span class="kw">get.knnx</span>(X_train, X_test, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>)<span class="op">$</span>nn.index</a>
<a class="sourceLine" id="cb383-4" title="4">}</a>
<a class="sourceLine" id="cb383-5" title="5"><span class="kw">system.time</span>(Ya &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.124   0.000   0.123</code></pre>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" title="1"><span class="kw">system.time</span>(Yb &lt;-<span class="st"> </span><span class="kw">our_knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.044   0.000   0.044</code></pre>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" title="1"><span class="kw">mean</span>(Ya <span class="op">==</span><span class="st"> </span>Yb) <span class="co"># 1.0 on perfect match</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Note that our solution requires <span class="math inline">\(c\cdot n_\text{test}\cdot n_\text{train}\cdot p\)</span>
arithmetic operations for some <span class="math inline">\(c&gt;1\)</span>.
The overall cost of sorting is at least <span class="math inline">\(d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}\)</span>
for some <span class="math inline">\(d&gt;1\)</span>.</p>
<p>This does not scale well with both <span class="math inline">\(n_\text{test}\)</span> and <span class="math inline">\(n_\text{train}\)</span>
(think – big data).</p>
<div style="margin-top: 1em">

</div>
<p>It turns out that there are special <strong>spatial data structures</strong>
– such as <em>metric trees</em> – that aim to speed up searching for nearest
neighbours in <em>low-dimensional spaces</em> (for small <span class="math inline">\(p\)</span>).</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Searching in high-dimensional spaces is hard due to the so-called
curse of dimensionality.</p>
</dd>
</dl>
<p>For example, <code>FNN::get.knnx()</code> also implements the so-called
kd-trees.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" title="1"><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</a>
<a class="sourceLine" id="cb389-2" title="2">test_speed &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, k) {</a>
<a class="sourceLine" id="cb389-3" title="3">    A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</a>
<a class="sourceLine" id="cb389-4" title="4">    s &lt;-<span class="st"> </span><span class="kw">summary</span>(microbenchmark<span class="op">::</span><span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb389-5" title="5">        <span class="dt">brute=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>),</a>
<a class="sourceLine" id="cb389-6" title="6">        <span class="dt">kd_tree=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;kd_tree&quot;</span>),</a>
<a class="sourceLine" id="cb389-7" title="7">        <span class="dt">times=</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb389-8" title="8">    ), <span class="dt">unit=</span><span class="st">&quot;s&quot;</span>)</a>
<a class="sourceLine" id="cb389-9" title="9">    <span class="co"># minima of 3 time measurements:</span></a>
<a class="sourceLine" id="cb389-10" title="10">    <span class="kw">structure</span>(s<span class="op">$</span>min, <span class="dt">names=</span><span class="kw">as.character</span>(s<span class="op">$</span>expr))</a>
<a class="sourceLine" id="cb389-11" title="11">}</a></code></pre></div>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb390-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">2</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##      brute    kd_tree 
## 0.29834313 0.01224969</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb392-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">5</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##      brute    kd_tree 
## 0.43729168 0.06362388</code></pre>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb394-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">10</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##     brute   kd_tree 
## 0.6680588 0.6574563</code></pre>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb396-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">20</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##    brute  kd_tree 
## 1.289997 5.342889</code></pre>
</div>
<div id="different-metrics" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Different Metrics (*)</h3>
<p>The Euclidean distance is just one particular example
of many possible <strong>metrics</strong> (metric == a mathematical term,
above we have used this term in a more relaxed fashion, when referring
to accuracy etc.).</p>
<p>Mathematically, we say that <span class="math inline">\(d\)</span> is a metric on a set <span class="math inline">\(X\)</span>
(e.g., <span class="math inline">\(\mathbb{R}^p\)</span>), whenever
it is a function <span class="math inline">\(d:X\times X\to [0,\infty]\)</span> such that for all <span class="math inline">\(x,x&#39;,x&#39;&#39;\in X\)</span>:</p>
<ul>
<li><span class="math inline">\(d(x, x&#39;) = 0\)</span> if and only if <span class="math inline">\(x=x&#39;\)</span>,</li>
<li><span class="math inline">\(d(x, x&#39;) = d(x&#39;, x)\)</span> (it is symmetric)</li>
<li><span class="math inline">\(d(x, x&#39;&#39;) \le d(x, x&#39;) + d(x&#39;, x&#39;&#39;)\)</span> (it fulfils the triangle inequality)</li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Not all the properties are required in all the applications;
sometimes we might need a few additional ones.</p>
</dd>
</dl>
<p>We can easily generalise the way we introduced the K-NN method
to have a classifier that is based on a point’s neighbourhood
with respect to any metric.</p>
<p>Example metrics on <span class="math inline">\(\mathbb{R}^p\)</span>:</p>
<ul>
<li><strong>Euclidean</strong>
<span class="math display">\[
d_2(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \| = \| \mathbf{x}-\mathbf{x}&#39; \|_2 = \sqrt{ \sum_{i=1}^p (x_i-x_i&#39;)^2 }
\]</span></li>
<li><strong>Manhattan</strong> (taxicab)
<span class="math display">\[
d_1(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \|_1 = { \sum_{i=1}^p |x_i-x_i&#39;| }
\]</span></li>
<li><strong>Chebyshev</strong> (maximum)
<span class="math display">\[
d_\infty(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \|_\infty = \max_{i=1,\dots,p} |x_i-x_i&#39;|
\]</span></li>
</ul>
<!--
These are all examples of $L_p$ metrics, $p\ge 1$:
\[
d_p(\mathbf{x}, \mathbf{x}') = \| \mathbf{x}-\mathbf{x}' \|_p = \left( \sum_{i=1}^p |x_i-x_i'|^p \right)^{1/p}
\]
-->
<p>We can define metrics on different spaces too.</p>
<p>For example, the <strong>Levenshtein distance</strong> is a popular choice
for comparing character strings (also DNA sequences etc.)</p>
<p>It is an <em>edit distance</em> – it measures the minimal number of
single-character insertions, deletions or substitutions to change
one string into another.</p>
<p>For instance:</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb398-1" title="1"><span class="kw">adist</span>(<span class="st">&quot;happy&quot;</span>, <span class="st">&quot;nap&quot;</span>)</a></code></pre></div>
<pre><code>##      [,1]
## [1,]    3</code></pre>
<p>This is because we need 1 substitution and 2 deletions,</p>
<p>happy → nappy → napp → nap.</p>
<p>See also:</p>
<ul>
<li>the Hamming distance for categorical vectors (or strings of equal lengths),</li>
<li>the Jaccard distance for sets,</li>
<li>the Kendall tau rank distance for rankings.</li>
</ul>
<p>Moreover, R package <code>stringdist</code> includes implementations
of numerous string metrics.</p>
<!-- Mahalanobis distance -->
</div>
</div>
<div id="outro-2" class="section level2">
<h2><span class="header-section-number">3.5</span> Outro</h2>
<div id="remarks-2" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Remarks</h3>
<p>Note that K-NN is suitable for any kind of multiclass classification.</p>
<p>However, in practice it’s pretty slow for larger datasets – to
classify a single point we have to query the whole training set (which
should be available at all times).</p>
<p>In the next part we will discuss some other well-known classifiers:</p>
<ul>
<li><em>Decision trees</em></li>
<li><em>Logistic regression</em></li>
</ul>
<!--
They are restricted to binary (0/1) outputs.
They will have to be extended
somehow to allow for more classes.
-->
</div>
<div id="side-note-k-nn-regression" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Side Note: K-NN Regression</h3>
<p>The K-Nearest Neighbour scheme is intuitively pleasing.</p>
<p>No wonder it has inspired a similar approach for solving a regression task.</p>
<p>In order to make a prediction for a new point <span class="math inline">\(\mathbf{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>find the K-nearest neighbours of <span class="math inline">\(\mathbf{x}&#39;\)</span> amongst the points in the train set,
denoted <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>,</li>
<li>fetch the corresponding reference outputs <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span>,</li>
<li>return their arithmetic mean as a result,
<span class="math display">\[\hat{y}=\frac{1}{K} \sum_{j=1}^K y_{i_j}.\]</span></li>
</ol>
<p>Recall our modelling of the Credit Rating (<span class="math inline">\(Y\)</span>)
as a function of the average Credit Card Balance (<span class="math inline">\(X\)</span>)
based on the <code>ISLR::Credit</code> dataset.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb400-1" title="1"><span class="kw">library</span>(<span class="st">&quot;ISLR&quot;</span>) <span class="co"># Credit dataset</span></a>
<a class="sourceLine" id="cb400-2" title="2">Xc &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</a>
<a class="sourceLine" id="cb400-3" title="3">Yc &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</a></code></pre></div>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb401-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>) <span class="co"># knn.reg function</span></a>
<a class="sourceLine" id="cb401-2" title="2">x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">seq</span>(<span class="kw">min</span>(Xc), <span class="kw">max</span>(Xc), <span class="dt">length.out=</span><span class="dv">101</span>))</a>
<a class="sourceLine" id="cb401-3" title="3">y1  &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">1</span>)<span class="op">$</span>pred</a>
<a class="sourceLine" id="cb401-4" title="4">y5  &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">5</span>)<span class="op">$</span>pred</a>
<a class="sourceLine" id="cb401-5" title="5">y25 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">25</span>)<span class="op">$</span>pred</a></code></pre></div>
<p>The three models are depicted in Figure <a href="classification-with-k-nearest-neighbours.html#fig:knnreg3">7</a>.
Again, the higher the <span class="math inline">\(K\)</span>, the smoother the curve. On the other hand, for
small <span class="math inline">\(K\)</span> we adapt better to what’s in a point’s neighbourhood.</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb402-1" title="1"><span class="kw">plot</span>(Xc, Yc, <span class="dt">col=</span><span class="st">&quot;#666666c0&quot;</span>,</a>
<a class="sourceLine" id="cb402-2" title="2">    <span class="dt">xlab=</span><span class="st">&quot;Balance&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Rating&quot;</span>)</a>
<a class="sourceLine" id="cb402-3" title="3"><span class="kw">lines</span>(x, y1,  <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb402-4" title="4"><span class="kw">lines</span>(x, y5,  <span class="dt">col=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb402-5" title="5"><span class="kw">lines</span>(x, y25, <span class="dt">col=</span><span class="dv">4</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb402-6" title="6"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;K=1&quot;</span>, <span class="st">&quot;K=5&quot;</span>, <span class="st">&quot;K=25&quot;</span>),</a>
<a class="sourceLine" id="cb402-7" title="7">    <span class="dt">col=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</a></code></pre></div>
<div class="figure">
<img src="03-classification-neighbours-figures/knnreg3-1.svg" alt="Figure 7: K-NN regression example" id="fig:knnreg3" />
<p class="caption">Figure 7: K-NN regression example</p>
</div>
<!--

TODO: exercise -- density, not NN-based classification
regression - consider the epsilon-neighbourhood.

-->
</div>
<div id="further-reading-2" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Section 13.3)</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-with-trees-and-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
