<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="pandoc, pandoc-citeproc, knitr, bookdown (custom), GitBook, and many more" />

  <meta property="og:title" content="3 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-regression.html"/>
<link rel="next" href="classification-with-trees-and-linear-models.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<!-- Use KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<!-- /Use KaTeX -->

<style type="text/css">
/* Marek's custom CSS */

@import url("https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic,700italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans:400,500,600,700,800,900,400italic,500italic,600italic,700italic,800italic,900italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:400,600,700,400italic,600italic,700italic");

span.katex {
    font-size: 100%;
}

</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2.1 2021-02-12 18:56 (727245f)</a></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#exercises-in-r-2"><i class="fa fa-check"></i><b>4.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-preparing-data"><i class="fa fa-check"></i><b>4.4.1</b> EdStats – Preparing Data</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.4.2</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.4.3</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>4.4.4</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>4.4.5</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.5</b> Outro</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="5.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>5.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Continuous Optimisation with Iterative Algorithms</a>
<ul>
<li class="chapter" data-level="6.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="6.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>6.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="6.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.5</b> Outro</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>7.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3</b> Agglomerative Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>7.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#exercises-in-r-3"><i class="fa fa-check"></i><b>7.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>7.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>7.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>7.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.5</b> Outro</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a>
<ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.2</b> Genetic Algorithms</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.2.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.3</b> Outro</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.3.1</b> Remarks</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a>
<ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-16"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> Exercise: The MovieLens Dataset (*)</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a>
<ul>
<li class="chapter" data-level="B.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a>
<ul>
<li class="chapter" data-level="C.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a>
<ul>
<li class="chapter" data-level="C.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a>
<ul>
<li class="chapter" data-level="C.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>C.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a>
<ul>
<li class="chapter" data-level="D.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>D.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a>
<ul>
<li class="chapter" data-level="E.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a>
<ul>
<li class="chapter" data-level="E.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>E.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-k-nearest-neighbours" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Classification with K-Nearest Neighbours</h1>
<div id="introduction-3" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<div id="classification-task" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Classification Task</h3>
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space (each of the <span class="math inline">\(n\)</span> objects
is described by means of <span class="math inline">\(p\)</span> numerical features).</p>
<p>Recall that in supervised learning, with each
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output <span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \\
\end{array}
\right]
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]</span></p>
<div style="margin-top: 1em">

</div>
<p>In this chapter we are interested in <strong>classification</strong> tasks;
we assume that each <span class="math inline">\(y_i\)</span> is a <em>label</em> (e.g., a character string) –
it is of quantitative/categorical type.</p>
<p>Most commonly, we are faced with <strong>binary classification</strong> tasks
where there are only two possible distinct labels.</p>
<p>We traditionally denote them with <span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s.</p>
<p>For example:</p>
<table>
<thead>
<tr class="header">
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>false</td>
<td>true</td>
</tr>
<tr class="odd">
<td>failure</td>
<td>success</td>
</tr>
<tr class="even">
<td>healthy</td>
<td>ill</td>
</tr>
</tbody>
</table>
<p>On the other hand, in <strong>multiclass classification</strong>,
we assume that each <span class="math inline">\(y_i\)</span> takes more than two possible values.</p>
<p>Example plot of a synthetic dataset
with the reference binary <span class="math inline">\(y\)</span>s is given in Figure <a href="classification-with-k-nearest-neighbours.html#fig:classify-intro">3.1</a>.
The “true” decision boundary is at <span class="math inline">\(X_1=0\)</span> but the classes
slightly overlap (the dataset is a bit noisy).</p>
<div class="figure"><span id="fig:classify-intro"></span>
<img src="03-classification-neighbours-figures/classify-intro-1.png" alt="" />
<p class="caption">Figure 3.1:  A synthetic 2D dataset with the true decision boundary at <span class="math inline">\(X_1=0\)</span></p>
</div>
</div>
<div id="data" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Data</h3>
<p>For illustration, let’s consider the Wine Quality dataset <span class="citation">(Cortez et al. <a href="#ref-wines" role="doc-biblioref">2009</a>)</span>
that can be downloaded from the UCI Machine Learning Repository
(<a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality" class="uri">https://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>) –
white wines only.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="classification-with-k-nearest-neighbours.html#cb274-1" aria-hidden="true"></a>wines &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/winequality-all.csv&quot;</span>,</span>
<span id="cb274-2"><a href="classification-with-k-nearest-neighbours.html#cb274-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb274-3"><a href="classification-with-k-nearest-neighbours.html#cb274-3" aria-hidden="true"></a>wines &lt;-<span class="st"> </span>wines[wines<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</span>
<span id="cb274-4"><a href="classification-with-k-nearest-neighbours.html#cb274-4" aria-hidden="true"></a>(n &lt;-<span class="st"> </span><span class="kw">nrow</span>(wines)) <span class="co"># number of samples</span></span></code></pre></div>
<pre><code>## [1] 4898</code></pre>
<p>These are Vinho Verde wine samples from the north of Portugal,
see <a href="https://www.vinhoverde.pt/en/homepage" class="uri">https://www.vinhoverde.pt/en/homepage</a>.</p>
<p>There are 11 physicochemical features reported.
Moreover, there is a wine rating (which we won’t consider here)
on the scale 0 (bad) to 10 (excellent)
given by wine experts.</p>
<p>The input matrix <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>
consists of the first 10 numeric variables:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="classification-with-k-nearest-neighbours.html#cb276-1" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</span>
<span id="cb276-2"><a href="classification-with-k-nearest-neighbours.html#cb276-2" aria-hidden="true"></a><span class="kw">dim</span>(X)</span></code></pre></div>
<pre><code>## [1] 4898   10</code></pre>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="classification-with-k-nearest-neighbours.html#cb278-1" aria-hidden="true"></a><span class="kw">head</span>(X, <span class="dv">2</span>) <span class="co"># first two rows</span></span></code></pre></div>
<pre><code>##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density  pH
## 1600     0.045                  45                  170   1.001 3.0
## 1601     0.049                  14                  132   0.994 3.3
##      sulphates
## 1600      0.45
## 1601      0.49</code></pre>
<p>The 11th variable measures the amount of alcohol (in %).</p>
<p>We will convert this dependent variable to a binary one:</p>
<ul>
<li>0 == (<code>alcohol  &lt; 12</code>) == lower-alcohol wines1</li>
<li>1 == (<code>alcohol &gt;= 12</code>) == higher-alcohol wines</li>
</ul>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="classification-with-k-nearest-neighbours.html#cb280-1" aria-hidden="true"></a><span class="co"># recall that TRUE == 1</span></span>
<span id="cb280-2"><a href="classification-with-k-nearest-neighbours.html#cb280-2" aria-hidden="true"></a>Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</span>
<span id="cb280-3"><a href="classification-with-k-nearest-neighbours.html#cb280-3" aria-hidden="true"></a><span class="kw">table</span>(Y)</span></code></pre></div>
<pre><code>## Y
##    0    1 
## 4085  813</code></pre>
<p>Now <span class="math inline">\((\mathbf{X},\mathbf{y})\)</span> is a basis for an interesting (yet challenging)
binary classification task.</p>
</div>
<div id="training-and-test-sets" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Training and Test Sets</h3>
<p>Recall that we are genuinely interested in the construction of supervised learning models for the two following purposes:</p>
<ul>
<li><strong>description</strong> – to explain a given dataset in simpler terms,</li>
<li><strong>prediction</strong> – to forecast the values of the dependent variable
for inputs that are yet to be observed.</li>
</ul>
<p>In the latter case:</p>
<ul>
<li>we don’t want our models to <em>overfit</em> to current data,</li>
<li>we want our models to <em>generalise</em> well
to new data.</li>
</ul>
<div style="margin-top: 1em">

</div>
<p>One way to assess if a model has sufficient predictive power is based
on a random <strong>train-test split</strong> of the original dataset:</p>
<ul>
<li><em>training sample</em> (usually 60-80% of the observations) – used to construct a model,</li>
<li><em>test sample</em> (remaining 40-20%) – used to assess the goodness of fit.</li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p><strong>Test sample must not be used in the training phase!</strong> (No cheating!)</p>
</dd>
</dl>
<p>60/40% train-test split in R:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="classification-with-k-nearest-neighbours.html#cb282-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></span>
<span id="cb282-2"><a href="classification-with-k-nearest-neighbours.html#cb282-2" aria-hidden="true"></a>random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</span>
<span id="cb282-3"><a href="classification-with-k-nearest-neighbours.html#cb282-3" aria-hidden="true"></a><span class="kw">head</span>(random_indices) <span class="co"># preview</span></span></code></pre></div>
<pre><code>## [1] 2463 2511 2227  526 4291 2986</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="classification-with-k-nearest-neighbours.html#cb284-1" aria-hidden="true"></a><span class="co"># first 60% of the indices (they are arranged randomly)</span></span>
<span id="cb284-2"><a href="classification-with-k-nearest-neighbours.html#cb284-2" aria-hidden="true"></a><span class="co"># will constitute the train sample:</span></span>
<span id="cb284-3"><a href="classification-with-k-nearest-neighbours.html#cb284-3" aria-hidden="true"></a>train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</span>
<span id="cb284-4"><a href="classification-with-k-nearest-neighbours.html#cb284-4" aria-hidden="true"></a>X_train &lt;-<span class="st"> </span>X[train_indices,]</span>
<span id="cb284-5"><a href="classification-with-k-nearest-neighbours.html#cb284-5" aria-hidden="true"></a>Y_train &lt;-<span class="st"> </span>Y[train_indices]</span>
<span id="cb284-6"><a href="classification-with-k-nearest-neighbours.html#cb284-6" aria-hidden="true"></a><span class="co"># the remaining indices (40%) go to the test sample:</span></span>
<span id="cb284-7"><a href="classification-with-k-nearest-neighbours.html#cb284-7" aria-hidden="true"></a>X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</span>
<span id="cb284-8"><a href="classification-with-k-nearest-neighbours.html#cb284-8" aria-hidden="true"></a>Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</span></code></pre></div>
</div>
<div id="discussed-methods" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Discussed Methods</h3>
<p>Our aim is to build a classifier that takes 10 wine physicochemical
features and determines whether it’s a “strong” wine.</p>
<p>We will discuss 3 simple and educational (yet practically useful)
classification algorithms:</p>
<ul>
<li><em>K-nearest neighbour scheme</em> – this chapter,</li>
<li><em>Decision trees</em> – the next chapter,</li>
<li><em>Logistic regression</em> – the next chapter.</li>
</ul>
</div>
</div>
<div id="k-nearest-neighbour-classifier" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> K-nearest Neighbour Classifier</h2>
<div id="introduction-4" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Introduction</h3>
<dl>
<dt>Rule.</dt>
<dd><p>“If you don’t know what to do in a situation, just act like the people around you”</p>
</dd>
</dl>
<p>For some integer <span class="math inline">\(K\ge 1\)</span>, the <strong>K-Nearest Neighbour (<em>K-NN</em>) Classifier</strong>
proceeds as follows.</p>
<p>To classify a new point <span class="math inline">\(\mathbf{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>find the <span class="math inline">\(K\)</span> nearest neighbours of a given point <span class="math inline">\(\mathbf{x}&#39;\)</span> amongst the points in the train set,
denoted <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>:
<ol style="list-style-type: lower-alpha">
<li>compute the Euclidean distances between <span class="math inline">\(\mathbf{x}&#39;\)</span> and each <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> from the train set,
<span class="math display">\[d_i = \|\mathbf{x}&#39;-\mathbf{x}_{i,\cdot}\|\]</span></li>
<li>order <span class="math inline">\(d_i\)</span>s in increasing order,
<span class="math inline">\(d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}\)</span></li>
<li>pick first <span class="math inline">\(K\)</span> indices (these are the <em>nearest</em> neighbours)</li>
</ol></li>
<li>fetch the corresponding reference labels <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span></li>
<li>return their <em>mode</em> as a result, i.e., the most frequently occurring label (a.k.a. <em>majority vote</em>)</li>
</ol>
<!-- > If a mode is not unique, return a randomly chosen mode (ties are broken at random). -->
<p>Here is how <span class="math inline">\(K\)</span>-NN classifier works on a synthetic 2D dataset.
Firstly let’s consider <span class="math inline">\(K=1\)</span>, see Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig-plot-knn1">3.2</a>.
Gray and pink regions depict how new points would be classified.
In particular 1-NN is “greedy” in the sense that we just
locate the nearest point.</p>
<div class="figure"><span id="fig:fig-plot-knn1"></span>
<img src="03-classification-neighbours-figures/fig-plot-knn1-1.png" alt="" />
<p class="caption">Figure 3.2:  1-NN class bounds for our 2D synthetic dataset</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>(*) 1-NN classification is essentially based
on a dataset’s so-called Voronoi diagram.</p>
</dd>
</dl>
<p>Increasing <span class="math inline">\(K\)</span> somehow smoothens the decision boundary (this makes it
less “local” and more “global”).
Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig-plot-knn3">3.3</a> depicts the <span class="math inline">\(K=3\)</span> case.</p>
<div class="figure"><span id="fig:fig-plot-knn3"></span>
<img src="03-classification-neighbours-figures/fig-plot-knn3-1.png" alt="" />
<p class="caption">Figure 3.3:  3-NN class bounds for our 2D synthetic dataset</p>
</div>
<div class="figure"><span id="fig:fig-plot-knn25"></span>
<img src="03-classification-neighbours-figures/fig-plot-knn25-1.png" alt="" />
<p class="caption">Figure 3.4:  25-NN class bounds for our 2D synthetic dataset</p>
</div>
<p>Recall that the “true” decision boundary for this synthetic dataset
is at <span class="math inline">\(X_1=0\)</span>. The 25-NN classifier did quite a good job, see Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig-plot-knn25">3.4</a>.</p>
</div>
<div id="example-in-r" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Example in R</h3>
<p>We shall be calling the <code>knn()</code> function from package <code>FNN</code>
to classify the points from the test sample
extracted from the <code>wines</code> dataset:</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="classification-with-k-nearest-neighbours.html#cb285-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</span></code></pre></div>
<p>Let’s make prediction using the 5-nn classifier:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="classification-with-k-nearest-neighbours.html#cb286-1" aria-hidden="true"></a>Y_knn5 &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</span>
<span id="cb286-2"><a href="classification-with-k-nearest-neighbours.html#cb286-2" aria-hidden="true"></a><span class="kw">head</span>(Y_test, <span class="dv">28</span>) <span class="co"># True Ys</span></span></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="classification-with-k-nearest-neighbours.html#cb288-1" aria-hidden="true"></a><span class="kw">head</span>(Y_knn5, <span class="dv">28</span>) <span class="co"># Predicted Ys</span></span></code></pre></div>
<pre><code>##  [1] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="classification-with-k-nearest-neighbours.html#cb290-1" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.81735</code></pre>
<p>9-nn classifier:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="classification-with-k-nearest-neighbours.html#cb292-1" aria-hidden="true"></a>Y_knn9 &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb292-2"><a href="classification-with-k-nearest-neighbours.html#cb292-2" aria-hidden="true"></a><span class="kw">head</span>(Y_test, <span class="dv">28</span>) <span class="co"># True Ys</span></span></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="classification-with-k-nearest-neighbours.html#cb294-1" aria-hidden="true"></a><span class="kw">head</span>(Y_knn9, <span class="dv">28</span>) <span class="co"># Predicted Ys</span></span></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="classification-with-k-nearest-neighbours.html#cb296-1" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.81939</code></pre>
</div>
<div id="feature-engineering" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Feature Engineering</h3>
<p>Note that the Euclidean distance that we used above
implicitly assumes that every feature (independent variable)
is on the same scale.</p>
<p>However, when dealing with, e.g., physical quantities,
we often perform conversions of units of measurement (kg → g, feet → m etc.).</p>
<p>Transforming a single feature may drastically change the metric
structure of the dataset
and therefore highly affect the obtained predictions.</p>
<p>To “bring data to the same scale”, we often apply a trick called <strong>standardisation</strong>.</p>
<p>Computing the so-called <strong>Z-scores</strong> of the <span class="math inline">\(j\)</span>-th feature, <span class="math inline">\(\mathbf{x}_{\cdot,j}\)</span>,
is done by subtracting from each observation the sample mean and dividing the result by the sample
standard deviation:</p>
<p><span class="math display">\[z_{i,j} = \frac{x_{i,j}-\bar{x}_{\cdot,j}}{s_{x_{\cdot,j}}}\]</span></p>
<p>This a new feature <span class="math inline">\(\mathbf{z}_{\cdot,j}\)</span> that always has mean 0 and standard deviation of 1.</p>
<p>Moreover, it is <em>unit-less</em> (e.g., we divide a value in kgs by a value in kgs,
the units are cancelled out).
This, amongst others, prevents one of the features from dominating
the other ones.</p>
<p>Z-scores are easy to interpret, e.g., 0.5 denotes an observation
that is 0.5 standard deviations above the mean
and -3 informs us that a value is 3 standard deviations below the mean.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) If data are normally distributed (bell-shaped histogram),
with very high probability, most (expected value is 99.74%) observations
should have Z-scores between -3 and 3. Those that don’t, are
“suspicious”, maybe they are outliers? We should inspect them manually.</p>
</dd>
</dl>
<p>Let’s compute <code>Z_train</code> and <code>Z_test</code>,
being the standardised versions of <code>X_train</code>
and <code>X_test</code>, respectively.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="classification-with-k-nearest-neighbours.html#cb298-1" aria-hidden="true"></a>means &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, mean) <span class="co"># column means</span></span>
<span id="cb298-2"><a href="classification-with-k-nearest-neighbours.html#cb298-2" aria-hidden="true"></a>sds   &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, sd)   <span class="co"># column standard deviations</span></span>
<span id="cb298-3"><a href="classification-with-k-nearest-neighbours.html#cb298-3" aria-hidden="true"></a>Z_train &lt;-<span class="st"> </span>X_train <span class="co"># copy</span></span>
<span id="cb298-4"><a href="classification-with-k-nearest-neighbours.html#cb298-4" aria-hidden="true"></a>Z_test  &lt;-<span class="st"> </span>X_test  <span class="co"># copy</span></span>
<span id="cb298-5"><a href="classification-with-k-nearest-neighbours.html#cb298-5" aria-hidden="true"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(X)) {</span>
<span id="cb298-6"><a href="classification-with-k-nearest-neighbours.html#cb298-6" aria-hidden="true"></a>    Z_train[,j] &lt;-<span class="st"> </span>(Z_train[,j]<span class="op">-</span>means[j])<span class="op">/</span>sds[j]</span>
<span id="cb298-7"><a href="classification-with-k-nearest-neighbours.html#cb298-7" aria-hidden="true"></a>    Z_test[,j]  &lt;-<span class="st"> </span>(Z_test[,j] <span class="op">-</span>means[j])<span class="op">/</span>sds[j]</span>
<span id="cb298-8"><a href="classification-with-k-nearest-neighbours.html#cb298-8" aria-hidden="true"></a>}</span></code></pre></div>
<p>Note that we have transformed the training and test sample in the very same
way. Computing means and standard deviations separately for these two datasets
is a common error – it is the training set that we use in the course of the
learning process.
The above can be re-written as:</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="classification-with-k-nearest-neighbours.html#cb299-1" aria-hidden="true"></a>Z_train &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</span>
<span id="cb299-2"><a href="classification-with-k-nearest-neighbours.html#cb299-2" aria-hidden="true"></a>Z_test  &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_test,  <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</span></code></pre></div>
<p>See Figure <a href="classification-with-k-nearest-neighbours.html#fig:standardise-depict-hist">3.5</a> for an illustration.
Note that the righthand figures (histograms of standardised variables)
are on the same scale now.</p>
<div class="figure"><span id="fig:standardise-depict-hist"></span>
<img src="03-classification-neighbours-figures/standardise-depict-hist-1.png" alt="" />
<p class="caption">Figure 3.5:  Empirical distribution of two variables (pH on the top, fixed.acidity on the bottom) before (left) and after (right) standardising</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>Of course, standardisation is only about shifting and scaling, it
preserves the shape of the distribution. If the original variable
is right skewed or bimodal, its standardised version will remain as such.</p>
</dd>
</dl>
<p>Let’s compute the accuracy of K-NN classifiers acting on standardised data.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="classification-with-k-nearest-neighbours.html#cb300-1" aria-hidden="true"></a>Y_knn5s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</span>
<span id="cb300-2"><a href="classification-with-k-nearest-neighbours.html#cb300-2" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5s) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.91429</code></pre>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="classification-with-k-nearest-neighbours.html#cb302-1" aria-hidden="true"></a>Y_knn9s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb302-2"><a href="classification-with-k-nearest-neighbours.html#cb302-2" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9s) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.91378</code></pre>
<p>The accuracy is much better.</p>
<p>Standardisation is an example of <em>feature engineering</em>.</p>
<p>Good models rarely work well “straight out of the box” – if that was the case,
we wouldn’t need data scientists and machine learning engineers!</p>
<p>To increase models’ accuracy, we often spend a lot of time:</p>
<ul>
<li>cleansing data (e.g., removing outliers)</li>
<li>extracting new features</li>
<li>transforming existing features</li>
<li>trying to find a set of features that are relevant</li>
</ul>
<p>This is the “more art than science” part of data science (sic!), and
hence most textbooks are not really eager for discussing such topics
(including this one).</p>
<p>Sorry, this is sad but true. The solutions that work well in the case of dataset
A may fail in the B case and vice versa. However, the more exercises you solve,
the greater the arsenal of ideas/possible approaches you will have at hand
when dealing with real-world problems.</p>
<p>Feature selection – example (manually selected columns):</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="classification-with-k-nearest-neighbours.html#cb304-1" aria-hidden="true"></a>features &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;density&quot;</span>, <span class="st">&quot;residual.sugar&quot;</span>)</span>
<span id="cb304-2"><a href="classification-with-k-nearest-neighbours.html#cb304-2" aria-hidden="true"></a>Y_knn5s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train[,features], Z_test[,features],</span>
<span id="cb304-3"><a href="classification-with-k-nearest-neighbours.html#cb304-3" aria-hidden="true"></a>    Y_train, <span class="dt">k=</span><span class="dv">5</span>)</span>
<span id="cb304-4"><a href="classification-with-k-nearest-neighbours.html#cb304-4" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5s) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.91633</code></pre>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="classification-with-k-nearest-neighbours.html#cb306-1" aria-hidden="true"></a>Y_knn9s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train[,features], Z_test[,features],</span>
<span id="cb306-2"><a href="classification-with-k-nearest-neighbours.html#cb306-2" aria-hidden="true"></a>    Y_train, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb306-3"><a href="classification-with-k-nearest-neighbours.html#cb306-3" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9s) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.925</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Try to find a combination of 2-4 features (by guessing or applying magic tricks)
that increases the accuracy of a <span class="math inline">\(K\)</span>-NN classifier on this dataset.</p>
</div>
</div>
</div>
<div id="model-assessment-and-selection" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Model Assessment and Selection</h2>
<div id="performance-metrics" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Performance Metrics</h3>
<!--

More metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

-->
<p>Recall that <span class="math inline">\(y_i\)</span> denotes the true label associated with the <span class="math inline">\(i\)</span>-th observation.</p>
<p>Let <span class="math inline">\(\hat{y}_i\)</span> denote the classifier’s output for a given <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>.</p>
<p>Ideally, we’d wish that <span class="math inline">\(\hat{y}_i=y_i\)</span>.</p>
<p>Sadly, in practice we will make errors.</p>
<p>Here are the 4 possible situations (true vs. predicted label):</p>
<table>
<thead>
<tr class="header">
<th>.</th>
<th><span class="math inline">\(y_i=0\)</span></th>
<th><span class="math inline">\(y_i=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{y}_i=0\)</span></td>
<td><strong>True Negative</strong></td>
<td>False Negative (Type II error)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{y}_i=1\)</span></td>
<td>False Positive (Type I error)</td>
<td><strong>True Positive</strong></td>
</tr>
</tbody>
</table>
<p>Note that the terms <strong>positive</strong> and <strong>negative</strong> refer to
the classifier’s output, i.e., occur when <span class="math inline">\(\hat{y}_i\)</span> is equal to <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>, respectively.</p>
<p>A <strong>confusion matrix</strong> is used to summarise
the correctness of predictions for the whole sample:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="classification-with-k-nearest-neighbours.html#cb308-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</span>
<span id="cb308-2"><a href="classification-with-k-nearest-neighbours.html#cb308-2" aria-hidden="true"></a>(C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test))</span></code></pre></div>
<pre><code>##       Y_test
## Y_pred    0    1
##      0 1607  133
##      1   36  184</code></pre>
<p>For example,</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="classification-with-k-nearest-neighbours.html#cb310-1" aria-hidden="true"></a>C[<span class="dv">1</span>,<span class="dv">1</span>] <span class="co"># number of TNs</span></span></code></pre></div>
<pre><code>## [1] 1607</code></pre>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="classification-with-k-nearest-neighbours.html#cb312-1" aria-hidden="true"></a>C[<span class="dv">2</span>,<span class="dv">1</span>] <span class="co"># number of FPs</span></span></code></pre></div>
<pre><code>## [1] 36</code></pre>
<p><strong>Accuracy</strong> is the ratio of the correctly classified instances
to all the instances.</p>
<p>In other words, it is the probability of making a correct prediction.</p>
<p><span class="math display">\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
\]</span>
where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function,
<span class="math inline">\(\mathbb{I}(l)=1\)</span> if logical condition <span class="math inline">\(l\)</span> is true and <span class="math inline">\(0\)</span> otherwise.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="classification-with-k-nearest-neighbours.html#cb314-1" aria-hidden="true"></a><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.91378</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="classification-with-k-nearest-neighbours.html#cb316-1" aria-hidden="true"></a>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C) <span class="co"># equivalently</span></span></code></pre></div>
<pre><code>## [1] 0.91378</code></pre>
<p>In many applications we are dealing with <strong>unbalanced problems</strong>, where
the case <span class="math inline">\(y_i=1\)</span> is relatively rare,
yet predicting it correctly is much more important than being
accurate with respect to class <span class="math inline">\(0\)</span>.</p>
<dl>
<dt>Remark.</dt>
<dd><p>Think of medical applications, e.g., HIV testing
or tumour diagnosis.</p>
</dd>
</dl>
<p>In such a case, <em>accuracy</em> as a metric fails to quantify what we are aiming for.</p>
<dl>
<dt>Remark.</dt>
<dd><p>If only 1% of the cases have true <span class="math inline">\(y_i=1\)</span>,
then a dummy classifier that always
outputs <span class="math inline">\(\hat{y}_i=0\)</span> has 99% accuracy.</p>
</dd>
</dl>
<p>Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address this problem.</p>
<p><strong>Precision</strong></p>
<p><span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\]</span></p>
<p>If the classifier outputs <span class="math inline">\(1\)</span>,
what is the probability that this is indeed true?</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="classification-with-k-nearest-neighbours.html#cb318-1" aria-hidden="true"></a>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># Precision</span></span></code></pre></div>
<pre><code>## [1] 0.83636</code></pre>
<p><strong>Recall</strong> (a.k.a. sensitivity, hit rate or true positive rate)</p>
<p><span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}
\]</span></p>
<p>If the true class is <span class="math inline">\(1\)</span>, what is the probability that the classifier
will detect it?</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="classification-with-k-nearest-neighbours.html#cb320-1" aria-hidden="true"></a>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]) <span class="co"># Recall</span></span></code></pre></div>
<pre><code>## [1] 0.58044</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>Precision or recall? It depends on an application.
Think of medical diagnosis, medical screening, plagiarism detection,
etc. — which measure is more important in each of the settings listed?</p>
</dd>
</dl>
<p>As a compromise, we can use the <strong>F-measure</strong>
(a.k.a. <span class="math inline">\(F_1\)</span>-measure),
which is the harmonic mean of precision
and recall:</p>
<p><span class="math display">\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}
\]</span></p>
<div class="exercise"><strong>Exercise.</strong>
<p>Show that the above equality holds.</p>
</div>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="classification-with-k-nearest-neighbours.html#cb322-1" aria-hidden="true"></a>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># F</span></span></code></pre></div>
<pre><code>## [1] 0.68529</code></pre>
<p>The following function can come in handy in the future:</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="classification-with-k-nearest-neighbours.html#cb324-1" aria-hidden="true"></a>get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</span>
<span id="cb324-2"><a href="classification-with-k-nearest-neighbours.html#cb324-2" aria-hidden="true"></a>{</span>
<span id="cb324-3"><a href="classification-with-k-nearest-neighbours.html#cb324-3" aria-hidden="true"></a>    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></span>
<span id="cb324-4"><a href="classification-with-k-nearest-neighbours.html#cb324-4" aria-hidden="true"></a>    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb324-5"><a href="classification-with-k-nearest-neighbours.html#cb324-5" aria-hidden="true"></a>    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></span>
<span id="cb324-6"><a href="classification-with-k-nearest-neighbours.html#cb324-6" aria-hidden="true"></a>      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></span>
<span id="cb324-7"><a href="classification-with-k-nearest-neighbours.html#cb324-7" aria-hidden="true"></a>      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></span>
<span id="cb324-8"><a href="classification-with-k-nearest-neighbours.html#cb324-8" aria-hidden="true"></a>      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></span>
<span id="cb324-9"><a href="classification-with-k-nearest-neighbours.html#cb324-9" aria-hidden="true"></a>      <span class="co"># Confusion matrix items:</span></span>
<span id="cb324-10"><a href="classification-with-k-nearest-neighbours.html#cb324-10" aria-hidden="true"></a>      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</span>
<span id="cb324-11"><a href="classification-with-k-nearest-neighbours.html#cb324-11" aria-hidden="true"></a>      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb324-12"><a href="classification-with-k-nearest-neighbours.html#cb324-12" aria-hidden="true"></a>    ) <span class="co"># return a named vector</span></span>
<span id="cb324-13"><a href="classification-with-k-nearest-neighbours.html#cb324-13" aria-hidden="true"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="classification-with-k-nearest-neighbours.html#cb325-1" aria-hidden="true"></a><span class="kw">get_metrics</span>(Y_pred, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.91378    0.83636    0.58044    0.68529 1607.00000  133.00000 
##         FP         TP 
##   36.00000  184.00000</code></pre>
</div>
<div id="how-to-choose-k-for-k-nn-classification" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> How to Choose K for K-NN Classification?</h3>
<p>We haven’t yet considered the question which <span class="math inline">\(K\)</span> yields <em>the best</em>
classifier.</p>
<p>Best == one that has the highest <em>predictive power</em>.</p>
<p>Best == with respect to some chosen metric (accuracy, recall, precision, F-measure, …)</p>
<p>Let’s study how the metrics on the test set change as functions of the number of nearest neighbours considered, <span class="math inline">\(K\)</span>.</p>
<p>Auxiliary function:</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="classification-with-k-nearest-neighbours.html#cb327-1" aria-hidden="true"></a>knn_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(k, X_train, X_test, Y_train, Y_test)</span>
<span id="cb327-2"><a href="classification-with-k-nearest-neighbours.html#cb327-2" aria-hidden="true"></a>{</span>
<span id="cb327-3"><a href="classification-with-k-nearest-neighbours.html#cb327-3" aria-hidden="true"></a>    Y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span>k) <span class="co"># classify</span></span>
<span id="cb327-4"><a href="classification-with-k-nearest-neighbours.html#cb327-4" aria-hidden="true"></a>    <span class="kw">get_metrics</span>(Y_pred, Y_test)</span>
<span id="cb327-5"><a href="classification-with-k-nearest-neighbours.html#cb327-5" aria-hidden="true"></a>}</span></code></pre></div>
<p>For example:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="classification-with-k-nearest-neighbours.html#cb328-1" aria-hidden="true"></a><span class="kw">knn_metrics</span>(<span class="dv">5</span>, Z_train, Z_test, Y_train, Y_test)</span></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.91429    0.82251    0.59937    0.69343 1602.00000  127.00000 
##         FP         TP 
##   41.00000  190.00000</code></pre>
<p>Example call to evaluate metrics as a function of different <span class="math inline">\(K\)</span>s:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="classification-with-k-nearest-neighbours.html#cb330-1" aria-hidden="true"></a>Ks &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dt">by=</span><span class="dv">2</span>)</span>
<span id="cb330-2"><a href="classification-with-k-nearest-neighbours.html#cb330-2" aria-hidden="true"></a>Ps &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">t</span>(</span>
<span id="cb330-3"><a href="classification-with-k-nearest-neighbours.html#cb330-3" aria-hidden="true"></a>    <span class="kw">sapply</span>(Ks, <span class="co"># on each element in this vector</span></span>
<span id="cb330-4"><a href="classification-with-k-nearest-neighbours.html#cb330-4" aria-hidden="true"></a>        knn_metrics,     <span class="co"># apply this function</span></span>
<span id="cb330-5"><a href="classification-with-k-nearest-neighbours.html#cb330-5" aria-hidden="true"></a>        Z_train, Z_test, Y_train, Y_test <span class="co"># aux args</span></span>
<span id="cb330-6"><a href="classification-with-k-nearest-neighbours.html#cb330-6" aria-hidden="true"></a>    )))</span></code></pre></div>
<dl>
<dt>Remark.</dt>
<dd><p>Note that <code>sapply(X, f, arg1, arg2, ...)</code>
outputs a list <code>Y</code> such that
<code>Y[[i]] = f(X[i], arg1, arg2, ...)</code>
which is then simplified to a matrix.</p>
</dd>
<dt>Remark.</dt>
<dd><p>We transpose this result, <code>t()</code>, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g., why <code>t()</code>, play with
the code yourself – it’s fun fun fun.</p>
</dd>
</dl>
<p>Example results:</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="classification-with-k-nearest-neighbours.html#cb331-1" aria-hidden="true"></a><span class="kw">round</span>(<span class="kw">cbind</span>(<span class="dt">K=</span>Ks, Ps), <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##     K  Acc Prec  Rec    F   TN  FN FP  TP
## 1   1 0.92 0.77 0.72 0.74 1574  90 69 227
## 2   3 0.92 0.79 0.66 0.72 1587 108 56 209
## 3   5 0.91 0.82 0.60 0.69 1602 127 41 190
## 4   7 0.91 0.82 0.56 0.67 1604 138 39 179
## 5   9 0.91 0.84 0.58 0.69 1607 133 36 184
## 6  11 0.91 0.85 0.56 0.68 1611 138 32 179
## 7  13 0.91 0.83 0.57 0.68 1606 136 37 181
## 8  15 0.91 0.83 0.55 0.66 1607 144 36 173
## 9  17 0.91 0.82 0.53 0.64 1607 149 36 168
## 10 19 0.90 0.81 0.52 0.63 1603 151 40 166</code></pre>
<p>Figure <a href="classification-with-k-nearest-neighbours.html#fig:whichK5">3.6</a> is worth a thousand tables though (see <code>?matplot</code> in R). The reader is kindly asked to draw conclusions themself.</p>
<div class="figure"><span id="fig:whichK5"></span>
<img src="03-classification-neighbours-figures/whichK5-1.png" alt="" />
<p class="caption">Figure 3.6:  Performance of <span class="math inline">\(K\)</span>-nn classifiers as a function of <span class="math inline">\(K\)</span> for standardised and raw data</p>
</div>
<!--

(\*) **ROC** (Receiver Operating Characteristic) curve:


```r
TPR <- Ps$TP/(Ps$TP+Ps$FN) # True Positive Rate (recall)
FPR <- Ps$FP/(Ps$FP+Ps$TN) # False Positive Rate
plot(FPR, TPR, asp=1, xlim=c(0,1), ylim=c(0,1))
abline(a=0, b=1, lty=3)
```

![(\#fig:unnamed-chunk-1) plot of chunk unnamed-chunk-1](03-classification-neighbours-figures/unnamed-chunk-1-1)

-->
</div>
<div id="training-validation-and-test-sets" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Training, Validation and Test sets</h3>
<p>In the <span class="math inline">\(K\)</span>-NN classification task, there are many hyperparameters to tune up:</p>
<ul>
<li><p>Which <span class="math inline">\(K\)</span> should we choose?</p></li>
<li><p>Should we standardise the dataset?</p></li>
<li><p>Which variables should be taken into account when computing the Euclidean distance?</p></li>
</ul>
<!--
- Which metric should be used?
-->
<dl>
<dt>Remark.</dt>
<dd><p><strong>If we select the best hyperparameter set based on test
sample error, we will run into the trap of overfitting again</strong>.
This time we’ll be overfitting to the test set — the model that is optimal
for a given test sample doesn’t have to generalise well to other test samples (!).</p>
</dd>
</dl>
<p>In order to overcome this problem,
we can perform a random <strong>train-validation-test split</strong> of the original dataset:</p>
<ul>
<li><em>training sample</em> (e.g., 60%) – used to construct the models</li>
<li><em>validation sample</em> (e.g., 20%) – used to tune the hyperparameters of the classifier</li>
<li><em>test sample</em> (e.g., 20%) – used to assess the goodness of fit</li>
</ul>
<!--
By the way, this is how most data mining competitions are assessed --
you will never have access to the final test sample used
to determine the winner. The best you can do is to "guess".
-->
<p>An example way to perform a 60/20/20% train-validation-test split:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="classification-with-k-nearest-neighbours.html#cb333-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></span>
<span id="cb333-2"><a href="classification-with-k-nearest-neighbours.html#cb333-2" aria-hidden="true"></a>random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</span>
<span id="cb333-3"><a href="classification-with-k-nearest-neighbours.html#cb333-3" aria-hidden="true"></a>n1 &lt;-<span class="st"> </span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)</span>
<span id="cb333-4"><a href="classification-with-k-nearest-neighbours.html#cb333-4" aria-hidden="true"></a>n2 &lt;-<span class="st"> </span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.8</span>)</span>
<span id="cb333-5"><a href="classification-with-k-nearest-neighbours.html#cb333-5" aria-hidden="true"></a>X2_train &lt;-<span class="st"> </span>X[random_indices[<span class="dv">1</span>     <span class="op">:</span>n1], ]</span>
<span id="cb333-6"><a href="classification-with-k-nearest-neighbours.html#cb333-6" aria-hidden="true"></a>Y2_train &lt;-<span class="st"> </span>Y[random_indices[<span class="dv">1</span>     <span class="op">:</span>n1]  ]</span>
<span id="cb333-7"><a href="classification-with-k-nearest-neighbours.html#cb333-7" aria-hidden="true"></a>X2_valid &lt;-<span class="st"> </span>X[random_indices[(n1<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n2], ]</span>
<span id="cb333-8"><a href="classification-with-k-nearest-neighbours.html#cb333-8" aria-hidden="true"></a>Y2_valid &lt;-<span class="st"> </span>Y[random_indices[(n1<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n2]  ]</span>
<span id="cb333-9"><a href="classification-with-k-nearest-neighbours.html#cb333-9" aria-hidden="true"></a>X2_test  &lt;-<span class="st"> </span>X[random_indices[(n2<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n ], ]</span>
<span id="cb333-10"><a href="classification-with-k-nearest-neighbours.html#cb333-10" aria-hidden="true"></a>Y2_test  &lt;-<span class="st"> </span>Y[random_indices[(n2<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n ]  ]</span>
<span id="cb333-11"><a href="classification-with-k-nearest-neighbours.html#cb333-11" aria-hidden="true"></a><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X2_train)<span class="op">+</span><span class="kw">nrow</span>(X2_valid)<span class="op">+</span><span class="kw">nrow</span>(X2_test)</span>
<span id="cb333-12"><a href="classification-with-k-nearest-neighbours.html#cb333-12" aria-hidden="true"></a>    <span class="op">==</span><span class="st"> </span><span class="kw">nrow</span>(X))</span></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Find the best <span class="math inline">\(K\)</span> on the validation set and compute the error metrics
on the test set.</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>(*) If our dataset is too small,
we can use various <em>cross-validation</em> techniques
instead of a train-validate-test split.
<!-- TODO: see exercise.... --></p>
</dd>
</dl>
</div>
</div>
<div id="implementing-a-k-nn-classifier" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Implementing a K-NN Classifier (*)</h2>
<div id="factor-data-type" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Factor Data Type</h3>
<p>Recall that (see Appendix B for more details)
<code>factor</code> type in R is a very convenient means to encode categorical data
(such as <span class="math inline">\(\mathbf{y}\)</span>):</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="classification-with-k-nearest-neighbours.html#cb334-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>)</span>
<span id="cb334-2"><a href="classification-with-k-nearest-neighbours.html#cb334-2" aria-hidden="true"></a>f &lt;-<span class="st"> </span><span class="kw">factor</span>(x, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>))</span>
<span id="cb334-3"><a href="classification-with-k-nearest-neighbours.html#cb334-3" aria-hidden="true"></a>f</span></code></pre></div>
<pre><code>## [1] yes no  no  yes no 
## Levels: no yes</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="classification-with-k-nearest-neighbours.html#cb336-1" aria-hidden="true"></a><span class="kw">table</span>(f) <span class="co"># counts</span></span></code></pre></div>
<pre><code>## f
##  no yes 
##   3   2</code></pre>
<p>Internally, objects of type <code>factor</code> are represented as integer vectors
with elements in <span class="math inline">\(\{1,\dots,M\}\)</span>, where <span class="math inline">\(M\)</span> is the number of possible levels.</p>
<p>Labels, used to “decipher” the numeric codes, are stored separately.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="classification-with-k-nearest-neighbours.html#cb338-1" aria-hidden="true"></a><span class="kw">as.numeric</span>(f) <span class="co"># 2nd label, 1st label, 1st label etc.</span></span></code></pre></div>
<pre><code>## [1] 2 1 1 2 1</code></pre>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="classification-with-k-nearest-neighbours.html#cb340-1" aria-hidden="true"></a><span class="kw">levels</span>(f)</span></code></pre></div>
<pre><code>## [1] &quot;no&quot;  &quot;yes&quot;</code></pre>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="classification-with-k-nearest-neighbours.html#cb342-1" aria-hidden="true"></a><span class="kw">levels</span>(f) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;failure&quot;</span>, <span class="st">&quot;success&quot;</span>) <span class="co"># re-encode</span></span>
<span id="cb342-2"><a href="classification-with-k-nearest-neighbours.html#cb342-2" aria-hidden="true"></a>f</span></code></pre></div>
<pre><code>## [1] success failure failure success failure
## Levels: failure success</code></pre>
</div>
<div id="main-routine" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Main Routine (*)</h3>
<p>Let’s implement a K-NN classifier ourselves
by using a top-bottom approach.</p>
<p>We will start with a general description of the admissible inputs
and the expected output.</p>
<p>Then we will arrange the processing of data into
conveniently manageable chunks.</p>
<p>The function’s declaration will look like:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="classification-with-k-nearest-neighbours.html#cb344-1" aria-hidden="true"></a>our_knn &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">1</span>) {</span>
<span id="cb344-2"><a href="classification-with-k-nearest-neighbours.html#cb344-2" aria-hidden="true"></a>    <span class="co"># k=1 denotes a parameter with a default value</span></span>
<span id="cb344-3"><a href="classification-with-k-nearest-neighbours.html#cb344-3" aria-hidden="true"></a>    <span class="co"># ...</span></span>
<span id="cb344-4"><a href="classification-with-k-nearest-neighbours.html#cb344-4" aria-hidden="true"></a>}</span></code></pre></div>
<p>Load an example dataset on which we will test our algorithm:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="classification-with-k-nearest-neighbours.html#cb345-1" aria-hidden="true"></a>wines &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/winequality-all.csv&quot;</span>,</span>
<span id="cb345-2"><a href="classification-with-k-nearest-neighbours.html#cb345-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb345-3"><a href="classification-with-k-nearest-neighbours.html#cb345-3" aria-hidden="true"></a>wines &lt;-<span class="st"> </span>wines[wines<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</span>
<span id="cb345-4"><a href="classification-with-k-nearest-neighbours.html#cb345-4" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</span>
<span id="cb345-5"><a href="classification-with-k-nearest-neighbours.html#cb345-5" aria-hidden="true"></a>Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</span></code></pre></div>
<p>Note that <code>Y</code> is now a factor object.</p>
<p>Train-test split:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="classification-with-k-nearest-neighbours.html#cb346-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb346-2"><a href="classification-with-k-nearest-neighbours.html#cb346-2" aria-hidden="true"></a>random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</span>
<span id="cb346-3"><a href="classification-with-k-nearest-neighbours.html#cb346-3" aria-hidden="true"></a>train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</span>
<span id="cb346-4"><a href="classification-with-k-nearest-neighbours.html#cb346-4" aria-hidden="true"></a>X_train &lt;-<span class="st"> </span>X[train_indices,]</span>
<span id="cb346-5"><a href="classification-with-k-nearest-neighbours.html#cb346-5" aria-hidden="true"></a>Y_train &lt;-<span class="st"> </span>Y[train_indices]</span>
<span id="cb346-6"><a href="classification-with-k-nearest-neighbours.html#cb346-6" aria-hidden="true"></a>X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</span>
<span id="cb346-7"><a href="classification-with-k-nearest-neighbours.html#cb346-7" aria-hidden="true"></a>Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</span></code></pre></div>
<p>First, we should specify the type and form of the arguments
we’re expecting:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="classification-with-k-nearest-neighbours.html#cb347-1" aria-hidden="true"></a><span class="co"># this is the body of our_knn() - part 1</span></span>
<span id="cb347-2"><a href="classification-with-k-nearest-neighbours.html#cb347-2" aria-hidden="true"></a><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_train), <span class="kw">is.matrix</span>(X_train))</span>
<span id="cb347-3"><a href="classification-with-k-nearest-neighbours.html#cb347-3" aria-hidden="true"></a><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_test), <span class="kw">is.matrix</span>(X_test))</span>
<span id="cb347-4"><a href="classification-with-k-nearest-neighbours.html#cb347-4" aria-hidden="true"></a><span class="kw">stopifnot</span>(<span class="kw">is.factor</span>(Y_train))</span>
<span id="cb347-5"><a href="classification-with-k-nearest-neighbours.html#cb347-5" aria-hidden="true"></a><span class="kw">stopifnot</span>(<span class="kw">ncol</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">ncol</span>(X_test))</span>
<span id="cb347-6"><a href="classification-with-k-nearest-neighbours.html#cb347-6" aria-hidden="true"></a><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">length</span>(Y_train))</span>
<span id="cb347-7"><a href="classification-with-k-nearest-neighbours.html#cb347-7" aria-hidden="true"></a><span class="kw">stopifnot</span>(k <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb347-8"><a href="classification-with-k-nearest-neighbours.html#cb347-8" aria-hidden="true"></a>n_train &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_train)</span>
<span id="cb347-9"><a href="classification-with-k-nearest-neighbours.html#cb347-9" aria-hidden="true"></a>n_test  &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</span>
<span id="cb347-10"><a href="classification-with-k-nearest-neighbours.html#cb347-10" aria-hidden="true"></a>p &lt;-<span class="st"> </span><span class="kw">ncol</span>(X_train)</span>
<span id="cb347-11"><a href="classification-with-k-nearest-neighbours.html#cb347-11" aria-hidden="true"></a>M &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">levels</span>(Y_train))</span></code></pre></div>
<p>Therefore,</p>
<p><span class="math inline">\(\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}\)</span>,
<span class="math inline">\(\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}\)</span> and
<span class="math inline">\(\mathtt{Y\_train}\in\{1,\dots,M\}^{\mathtt{n\_train}}\)</span></p>
<dl>
<dt>Remark.</dt>
<dd><p>Recall that R <code>factor</code> objects are internally encoded as integer vectors.</p>
</dd>
</dl>
<p>Next, we will call the (to-be-done) function <code>our_get_knnx()</code>,
which seeks nearest neighbours of all the points:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="classification-with-k-nearest-neighbours.html#cb348-1" aria-hidden="true"></a><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></span>
<span id="cb348-2"><a href="classification-with-k-nearest-neighbours.html#cb348-2" aria-hidden="true"></a><span class="co"># where nn_indices[i,j] denotes the index of</span></span>
<span id="cb348-3"><a href="classification-with-k-nearest-neighbours.html#cb348-3" aria-hidden="true"></a><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></span>
<span id="cb348-4"><a href="classification-with-k-nearest-neighbours.html#cb348-4" aria-hidden="true"></a><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></span>
<span id="cb348-5"><a href="classification-with-k-nearest-neighbours.html#cb348-5" aria-hidden="true"></a>nn_indices &lt;-<span class="st"> </span><span class="kw">our_get_knnx</span>(X_train, X_test, k)</span></code></pre></div>
<p>Then, for each point in <code>X_test</code>,
we fetch the labels corresponding to its nearest neighbours
and compute their mode:</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="classification-with-k-nearest-neighbours.html#cb349-1" aria-hidden="true"></a>Y_pred &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_test) <span class="co"># vector of length n_test</span></span>
<span id="cb349-2"><a href="classification-with-k-nearest-neighbours.html#cb349-2" aria-hidden="true"></a><span class="co"># For now we will operate on the integer labels in {1,...,M}</span></span>
<span id="cb349-3"><a href="classification-with-k-nearest-neighbours.html#cb349-3" aria-hidden="true"></a>Y_train_int &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Y_train)</span>
<span id="cb349-4"><a href="classification-with-k-nearest-neighbours.html#cb349-4" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</span>
<span id="cb349-5"><a href="classification-with-k-nearest-neighbours.html#cb349-5" aria-hidden="true"></a>    <span class="co"># Get the labels of the NNs of the i-th point:</span></span>
<span id="cb349-6"><a href="classification-with-k-nearest-neighbours.html#cb349-6" aria-hidden="true"></a>    nn_labels_i &lt;-<span class="st"> </span>Y_train_int[nn_indices[i,]]</span>
<span id="cb349-7"><a href="classification-with-k-nearest-neighbours.html#cb349-7" aria-hidden="true"></a>    <span class="co"># Compute the mode (majority vote):</span></span>
<span id="cb349-8"><a href="classification-with-k-nearest-neighbours.html#cb349-8" aria-hidden="true"></a>    Y_pred[i] &lt;-<span class="st"> </span><span class="kw">our_mode</span>(nn_labels_i) <span class="co"># in {1,...,M}</span></span>
<span id="cb349-9"><a href="classification-with-k-nearest-neighbours.html#cb349-9" aria-hidden="true"></a>}</span></code></pre></div>
<p>Finally, we should convert the resulting integer vector
to an object of type <code>factor</code>:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="classification-with-k-nearest-neighbours.html#cb350-1" aria-hidden="true"></a><span class="co"># Convert Y_pred to factor:</span></span>
<span id="cb350-2"><a href="classification-with-k-nearest-neighbours.html#cb350-2" aria-hidden="true"></a><span class="kw">return</span>(<span class="kw">factor</span>(Y_pred, <span class="dt">labels=</span><span class="kw">levels</span>(Y_train)))</span></code></pre></div>
<!--
**Test-driven development** -- before writing


```r
test_our_knn <- function() {
    # ...
}
```



```r
test_our_mode <- function() {
    stopifnot(our_mode(c(1, 1, 1, 1)) == 1)
    stopifnot(our_mode(c(2, 2, 2, 2)) == 2)
    stopifnot(our_mode(c(3, 1, 3, 3)) == 3)
    stopifnot(our_mode(c(1, 1, 3, 3, 2)) %in% c(1, 3))
}
```


-->
</div>
<div id="mode" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Mode</h3>
<p>To implement the mode, we can use the <code>tabulate()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?tabulate</code>.</p>
</div>
<p>For example:</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="classification-with-k-nearest-neighbours.html#cb351-1" aria-hidden="true"></a><span class="kw">tabulate</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 4 2 0 0 1</code></pre>
<p>There might be multiple modes – in such a case, we should pick one at random.</p>
<p>For that, we can use the <code>sample()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?sample</code>.
Note that its behaviour is different when it’s first argument is a vector of length 1.</p>
</div>
<p>An example implementation:</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="classification-with-k-nearest-neighbours.html#cb353-1" aria-hidden="true"></a>our_mode &lt;-<span class="st"> </span><span class="cf">function</span>(Y) {</span>
<span id="cb353-2"><a href="classification-with-k-nearest-neighbours.html#cb353-2" aria-hidden="true"></a>    <span class="co"># tabulate() will take care of</span></span>
<span id="cb353-3"><a href="classification-with-k-nearest-neighbours.html#cb353-3" aria-hidden="true"></a>    <span class="co"># checking the correctness of Y</span></span>
<span id="cb353-4"><a href="classification-with-k-nearest-neighbours.html#cb353-4" aria-hidden="true"></a>    t &lt;-<span class="st"> </span><span class="kw">tabulate</span>(Y)</span>
<span id="cb353-5"><a href="classification-with-k-nearest-neighbours.html#cb353-5" aria-hidden="true"></a>    mode_candidates &lt;-<span class="st"> </span><span class="kw">which</span>(t <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(t))</span>
<span id="cb353-6"><a href="classification-with-k-nearest-neighbours.html#cb353-6" aria-hidden="true"></a>    <span class="cf">if</span> (<span class="kw">length</span>(mode_candidates) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="kw">return</span>(mode_candidates)</span>
<span id="cb353-7"><a href="classification-with-k-nearest-neighbours.html#cb353-7" aria-hidden="true"></a>    <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">sample</span>(mode_candidates, <span class="dv">1</span>))</span>
<span id="cb353-8"><a href="classification-with-k-nearest-neighbours.html#cb353-8" aria-hidden="true"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="classification-with-k-nearest-neighbours.html#cb354-1" aria-hidden="true"></a><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="classification-with-k-nearest-neighbours.html#cb356-1" aria-hidden="true"></a><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="classification-with-k-nearest-neighbours.html#cb358-1" aria-hidden="true"></a><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="classification-with-k-nearest-neighbours.html#cb360-1" aria-hidden="true"></a><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="classification-with-k-nearest-neighbours.html#cb362-1" aria-hidden="true"></a><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
</div>
<div id="nn-search-routines" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> NN Search Routines (*)</h3>
<p>Last but not least, we should implement the <code>our_get_knnx()</code> function.</p>
<p>It is the function responsible for seeking the indices of nearest neighbours.</p>
<p>It turns out this function will actually constitute the K-NN classifier’s performance
bottleneck in case of big data samples.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="classification-with-k-nearest-neighbours.html#cb364-1" aria-hidden="true"></a><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></span>
<span id="cb364-2"><a href="classification-with-k-nearest-neighbours.html#cb364-2" aria-hidden="true"></a><span class="co"># where nn_indices[i,j] denotes the index of</span></span>
<span id="cb364-3"><a href="classification-with-k-nearest-neighbours.html#cb364-3" aria-hidden="true"></a><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></span>
<span id="cb364-4"><a href="classification-with-k-nearest-neighbours.html#cb364-4" aria-hidden="true"></a><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></span>
<span id="cb364-5"><a href="classification-with-k-nearest-neighbours.html#cb364-5" aria-hidden="true"></a>our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</span>
<span id="cb364-6"><a href="classification-with-k-nearest-neighbours.html#cb364-6" aria-hidden="true"></a>    <span class="co"># ...</span></span>
<span id="cb364-7"><a href="classification-with-k-nearest-neighbours.html#cb364-7" aria-hidden="true"></a>}</span></code></pre></div>
<p>A naive approach to <code>our_get_knnx()</code> relies on computing all pairwise distances,
and sorting them.</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="classification-with-k-nearest-neighbours.html#cb365-1" aria-hidden="true"></a>our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</span>
<span id="cb365-2"><a href="classification-with-k-nearest-neighbours.html#cb365-2" aria-hidden="true"></a>    n_test &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</span>
<span id="cb365-3"><a href="classification-with-k-nearest-neighbours.html#cb365-3" aria-hidden="true"></a>    nn_indices &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA_real_</span>, <span class="dt">nrow=</span>n_test, <span class="dt">ncol=</span>k)</span>
<span id="cb365-4"><a href="classification-with-k-nearest-neighbours.html#cb365-4" aria-hidden="true"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</span>
<span id="cb365-5"><a href="classification-with-k-nearest-neighbours.html#cb365-5" aria-hidden="true"></a>        d &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(x)</span>
<span id="cb365-6"><a href="classification-with-k-nearest-neighbours.html#cb365-6" aria-hidden="true"></a>            <span class="kw">sqrt</span>(<span class="kw">sum</span>((x<span class="op">-</span>X_test[i,])<span class="op">^</span><span class="dv">2</span>)))</span>
<span id="cb365-7"><a href="classification-with-k-nearest-neighbours.html#cb365-7" aria-hidden="true"></a>        <span class="co"># now d[j] is the distance</span></span>
<span id="cb365-8"><a href="classification-with-k-nearest-neighbours.html#cb365-8" aria-hidden="true"></a>        <span class="co"># between X_train[j,] and X_test[i,]</span></span>
<span id="cb365-9"><a href="classification-with-k-nearest-neighbours.html#cb365-9" aria-hidden="true"></a>        nn_indices[i,] &lt;-<span class="st"> </span><span class="kw">order</span>(d)[<span class="dv">1</span><span class="op">:</span>k]</span>
<span id="cb365-10"><a href="classification-with-k-nearest-neighbours.html#cb365-10" aria-hidden="true"></a>    }</span>
<span id="cb365-11"><a href="classification-with-k-nearest-neighbours.html#cb365-11" aria-hidden="true"></a>    nn_indices</span>
<span id="cb365-12"><a href="classification-with-k-nearest-neighbours.html#cb365-12" aria-hidden="true"></a>}</span></code></pre></div>
<p>A comparison with <code>FNN:knn()</code>:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="classification-with-k-nearest-neighbours.html#cb366-1" aria-hidden="true"></a><span class="kw">system.time</span>(Ya &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.135   0.000   0.134</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="classification-with-k-nearest-neighbours.html#cb368-1" aria-hidden="true"></a><span class="kw">system.time</span>(Yb &lt;-<span class="st"> </span><span class="kw">our_knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</span></code></pre></div>
<pre><code>##    user  system elapsed 
##  15.387   0.000  15.388</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="classification-with-k-nearest-neighbours.html#cb370-1" aria-hidden="true"></a><span class="kw">mean</span>(Ya <span class="op">==</span><span class="st"> </span>Yb) <span class="co"># 1.0 on perfect match</span></span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Both functions return identical results but our implementation is “slightly” slower.</p>
<p><code>FNN:knn()</code> is efficiently written in C++, which is a compiled programming language.</p>
<p>R, on the other hand (just like Python and Matlab) is interpreted, therefore
as a rule of thumb we should consider it an order of magnitude slower (see, however, the Julia language).</p>
<p>Let’s substitute our naive implementation with the equivalent one,
but written in C++ (available in the <code>FNN</code> package).</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Note that we can write a C++ implementation ourselves,
see the Rcpp package for seamless R and C++ integration.</p>
</dd>
</dl>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="classification-with-k-nearest-neighbours.html#cb372-1" aria-hidden="true"></a>our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</span>
<span id="cb372-2"><a href="classification-with-k-nearest-neighbours.html#cb372-2" aria-hidden="true"></a>    <span class="co"># this is used by our_knn()</span></span>
<span id="cb372-3"><a href="classification-with-k-nearest-neighbours.html#cb372-3" aria-hidden="true"></a>    FNN<span class="op">::</span><span class="kw">get.knnx</span>(X_train, X_test, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>)<span class="op">$</span>nn.index</span>
<span id="cb372-4"><a href="classification-with-k-nearest-neighbours.html#cb372-4" aria-hidden="true"></a>}</span>
<span id="cb372-5"><a href="classification-with-k-nearest-neighbours.html#cb372-5" aria-hidden="true"></a><span class="kw">system.time</span>(Ya &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.137   0.000   0.137</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="classification-with-k-nearest-neighbours.html#cb374-1" aria-hidden="true"></a><span class="kw">system.time</span>(Yb &lt;-<span class="st"> </span><span class="kw">our_knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.046   0.000   0.047</code></pre>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="classification-with-k-nearest-neighbours.html#cb376-1" aria-hidden="true"></a><span class="kw">mean</span>(Ya <span class="op">==</span><span class="st"> </span>Yb) <span class="co"># 1.0 on perfect match</span></span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Note that our solution requires <span class="math inline">\(c\cdot n_\text{test}\cdot n_\text{train}\cdot p\)</span>
arithmetic operations for some <span class="math inline">\(c&gt;1\)</span>.
The overall cost of sorting is at least <span class="math inline">\(d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}\)</span>
for some <span class="math inline">\(d&gt;1\)</span>.</p>
<p>This does not scale well with both <span class="math inline">\(n_\text{test}\)</span> and <span class="math inline">\(n_\text{train}\)</span>
(think – big data).</p>
<div style="margin-top: 1em">

</div>
<p>It turns out that there are special <strong>spatial data structures</strong>
– such as <em>metric trees</em> – that aim to speed up searching for nearest
neighbours in <em>low-dimensional spaces</em> (for small <span class="math inline">\(p\)</span>).</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Searching in high-dimensional spaces is hard due to the so-called
curse of dimensionality.</p>
</dd>
</dl>
<p>For example, <code>FNN::get.knnx()</code> also implements the so-called
kd-trees.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="classification-with-k-nearest-neighbours.html#cb378-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</span>
<span id="cb378-2"><a href="classification-with-k-nearest-neighbours.html#cb378-2" aria-hidden="true"></a>test_speed &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, k) {</span>
<span id="cb378-3"><a href="classification-with-k-nearest-neighbours.html#cb378-3" aria-hidden="true"></a>    A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</span>
<span id="cb378-4"><a href="classification-with-k-nearest-neighbours.html#cb378-4" aria-hidden="true"></a>    s &lt;-<span class="st"> </span><span class="kw">summary</span>(microbenchmark<span class="op">::</span><span class="kw">microbenchmark</span>(</span>
<span id="cb378-5"><a href="classification-with-k-nearest-neighbours.html#cb378-5" aria-hidden="true"></a>        <span class="dt">brute=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>),</span>
<span id="cb378-6"><a href="classification-with-k-nearest-neighbours.html#cb378-6" aria-hidden="true"></a>        <span class="dt">kd_tree=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;kd_tree&quot;</span>),</span>
<span id="cb378-7"><a href="classification-with-k-nearest-neighbours.html#cb378-7" aria-hidden="true"></a>        <span class="dt">times=</span><span class="dv">3</span></span>
<span id="cb378-8"><a href="classification-with-k-nearest-neighbours.html#cb378-8" aria-hidden="true"></a>    ), <span class="dt">unit=</span><span class="st">&quot;s&quot;</span>)</span>
<span id="cb378-9"><a href="classification-with-k-nearest-neighbours.html#cb378-9" aria-hidden="true"></a>    <span class="co"># minima of 3 time measurements:</span></span>
<span id="cb378-10"><a href="classification-with-k-nearest-neighbours.html#cb378-10" aria-hidden="true"></a>    <span class="kw">structure</span>(s<span class="op">$</span>min, <span class="dt">names=</span><span class="kw">as.character</span>(s<span class="op">$</span>expr))</span>
<span id="cb378-11"><a href="classification-with-k-nearest-neighbours.html#cb378-11" aria-hidden="true"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="classification-with-k-nearest-neighbours.html#cb379-1" aria-hidden="true"></a><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">2</span>, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##    brute  kd_tree 
## 0.293616 0.012172</code></pre>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="classification-with-k-nearest-neighbours.html#cb381-1" aria-hidden="true"></a><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">5</span>, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##    brute  kd_tree 
## 0.435148 0.061757</code></pre>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="classification-with-k-nearest-neighbours.html#cb383-1" aria-hidden="true"></a><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##   brute kd_tree 
## 0.64483 0.65084</code></pre>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="classification-with-k-nearest-neighbours.html#cb385-1" aria-hidden="true"></a><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">20</span>, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##   brute kd_tree 
##  1.2602  5.2480</code></pre>
</div>
<div id="different-metrics" class="section level3" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Different Metrics (*)</h3>
<p>The Euclidean distance is just one particular example
of many possible <strong>metrics</strong> (metric == a mathematical term,
above we have used this term in a more relaxed fashion, when referring
to accuracy etc.).</p>
<p>Mathematically, we say that <span class="math inline">\(d\)</span> is a metric on a set <span class="math inline">\(X\)</span>
(e.g., <span class="math inline">\(\mathbb{R}^p\)</span>), whenever
it is a function <span class="math inline">\(d:X\times X\to [0,\infty]\)</span> such that for all <span class="math inline">\(x,x&#39;,x&#39;&#39;\in X\)</span>:</p>
<ul>
<li><span class="math inline">\(d(x, x&#39;) = 0\)</span> if and only if <span class="math inline">\(x=x&#39;\)</span>,</li>
<li><span class="math inline">\(d(x, x&#39;) = d(x&#39;, x)\)</span> (it is symmetric)</li>
<li><span class="math inline">\(d(x, x&#39;&#39;) \le d(x, x&#39;) + d(x&#39;, x&#39;&#39;)\)</span> (it fulfils the triangle inequality)</li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Not all the properties are required in all the applications;
sometimes we might need a few additional ones.</p>
</dd>
</dl>
<p>We can easily generalise the way we introduced the K-NN method
to have a classifier that is based on a point’s neighbourhood
with respect to any metric.</p>
<p>Example metrics on <span class="math inline">\(\mathbb{R}^p\)</span>:</p>
<ul>
<li><strong>Euclidean</strong>
<span class="math display">\[
d_2(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \| = \| \mathbf{x}-\mathbf{x}&#39; \|_2 = \sqrt{ \sum_{i=1}^p (x_i-x_i&#39;)^2 }
\]</span></li>
<li><strong>Manhattan</strong> (taxicab)
<span class="math display">\[
d_1(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \|_1 = { \sum_{i=1}^p |x_i-x_i&#39;| }
\]</span></li>
<li><strong>Chebyshev</strong> (maximum)
<span class="math display">\[
d_\infty(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \|_\infty = \max_{i=1,\dots,p} |x_i-x_i&#39;|
\]</span></li>
</ul>
<!--
These are all examples of $L_p$ metrics, $p\ge 1$:
\[
d_p(\mathbf{x}, \mathbf{x}') = \| \mathbf{x}-\mathbf{x}' \|_p = \left( \sum_{i=1}^p |x_i-x_i'|^p \right)^{1/p}
\]
-->
<p>We can define metrics on different spaces too.</p>
<p>For example, the <strong>Levenshtein distance</strong> is a popular choice
for comparing character strings (also DNA sequences etc.)</p>
<p>It is an <em>edit distance</em> – it measures the minimal number of
single-character insertions, deletions or substitutions to change
one string into another.</p>
<p>For instance:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="classification-with-k-nearest-neighbours.html#cb387-1" aria-hidden="true"></a><span class="kw">adist</span>(<span class="st">&quot;happy&quot;</span>, <span class="st">&quot;nap&quot;</span>)</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    3</code></pre>
<p>This is because we need 1 substitution and 2 deletions,</p>
<p>happy → nappy → napp → nap.</p>
<p>See also:</p>
<ul>
<li>the Hamming distance for categorical vectors (or strings of equal lengths),</li>
<li>the Jaccard distance for sets,</li>
<li>the Kendall tau rank distance for rankings.</li>
</ul>
<p>Moreover, R package <code>stringdist</code> includes implementations
of numerous string metrics.</p>
<!-- Mahalanobis distance -->
</div>
</div>
<div id="outro-2" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Outro</h2>
<div id="remarks-2" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Remarks</h3>
<p>Note that K-NN is suitable for any kind of multiclass classification.</p>
<p>However, in practice it’s pretty slow for larger datasets – to
classify a single point we have to query the whole training set (which
should be available at all times).</p>
<p>In the next part we will discuss some other well-known classifiers:</p>
<ul>
<li><em>Decision trees</em></li>
<li><em>Logistic regression</em></li>
</ul>
<!--
They are restricted to binary (0/1) outputs.
They will have to be extended
somehow to allow for more classes.
-->
</div>
<div id="side-note-k-nn-regression" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Side Note: K-NN Regression</h3>
<p>The K-Nearest Neighbour scheme is intuitively pleasing.</p>
<p>No wonder it has inspired a similar approach for solving a regression task.</p>
<p>In order to make a prediction for a new point <span class="math inline">\(\mathbf{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>find the K-nearest neighbours of <span class="math inline">\(\mathbf{x}&#39;\)</span> amongst the points in the train set,
denoted <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>,</li>
<li>fetch the corresponding reference outputs <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span>,</li>
<li>return their arithmetic mean as a result,
<span class="math display">\[\hat{y}=\frac{1}{K} \sum_{j=1}^K y_{i_j}.\]</span></li>
</ol>
<p>Recall our modelling of the Credit Rating (<span class="math inline">\(Y\)</span>)
as a function of the average Credit Card Balance (<span class="math inline">\(X\)</span>)
based on the <code>ISLR::Credit</code> dataset.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="classification-with-k-nearest-neighbours.html#cb389-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;ISLR&quot;</span>) <span class="co"># Credit dataset</span></span>
<span id="cb389-2"><a href="classification-with-k-nearest-neighbours.html#cb389-2" aria-hidden="true"></a>Xc &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</span>
<span id="cb389-3"><a href="classification-with-k-nearest-neighbours.html#cb389-3" aria-hidden="true"></a>Yc &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</span></code></pre></div>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="classification-with-k-nearest-neighbours.html#cb390-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>) <span class="co"># knn.reg function</span></span>
<span id="cb390-2"><a href="classification-with-k-nearest-neighbours.html#cb390-2" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">seq</span>(<span class="kw">min</span>(Xc), <span class="kw">max</span>(Xc), <span class="dt">length.out=</span><span class="dv">101</span>))</span>
<span id="cb390-3"><a href="classification-with-k-nearest-neighbours.html#cb390-3" aria-hidden="true"></a>y1  &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">1</span>)<span class="op">$</span>pred</span>
<span id="cb390-4"><a href="classification-with-k-nearest-neighbours.html#cb390-4" aria-hidden="true"></a>y5  &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">5</span>)<span class="op">$</span>pred</span>
<span id="cb390-5"><a href="classification-with-k-nearest-neighbours.html#cb390-5" aria-hidden="true"></a>y25 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">25</span>)<span class="op">$</span>pred</span></code></pre></div>
<p>The three models are depicted in Figure <a href="classification-with-k-nearest-neighbours.html#fig:knnreg3">3.7</a>.
Again, the higher the <span class="math inline">\(K\)</span>, the smoother the curve. On the other hand, for
small <span class="math inline">\(K\)</span> we adapt better to what’s in a point’s neighbourhood.</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="classification-with-k-nearest-neighbours.html#cb391-1" aria-hidden="true"></a><span class="kw">plot</span>(Xc, Yc, <span class="dt">col=</span><span class="st">&quot;#666666c0&quot;</span>,</span>
<span id="cb391-2"><a href="classification-with-k-nearest-neighbours.html#cb391-2" aria-hidden="true"></a>    <span class="dt">xlab=</span><span class="st">&quot;Balance&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Rating&quot;</span>)</span>
<span id="cb391-3"><a href="classification-with-k-nearest-neighbours.html#cb391-3" aria-hidden="true"></a><span class="kw">lines</span>(x, y1,  <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb391-4"><a href="classification-with-k-nearest-neighbours.html#cb391-4" aria-hidden="true"></a><span class="kw">lines</span>(x, y5,  <span class="dt">col=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb391-5"><a href="classification-with-k-nearest-neighbours.html#cb391-5" aria-hidden="true"></a><span class="kw">lines</span>(x, y25, <span class="dt">col=</span><span class="dv">4</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb391-6"><a href="classification-with-k-nearest-neighbours.html#cb391-6" aria-hidden="true"></a><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;K=1&quot;</span>, <span class="st">&quot;K=5&quot;</span>, <span class="st">&quot;K=25&quot;</span>),</span>
<span id="cb391-7"><a href="classification-with-k-nearest-neighbours.html#cb391-7" aria-hidden="true"></a>    <span class="dt">col=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:knnreg3"></span>
<img src="03-classification-neighbours-figures/knnreg3-1.png" alt="" />
<p class="caption">Figure 3.7:  K-NN regression example</p>
</div>
<!--

TODO: exercise -- density, not NN-based classification
regression - consider the epsilon-neighbourhood.

-->
</div>
<div id="further-reading-2" class="section level3" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(Hastie et al. <a href="#ref-esl" role="doc-biblioref">2017</a>: Section 13.3)</span></p>

<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wines">
<p>Cortez P, Cerdeira A, Almeida F, Matos T, Reis J (2009) Modeling wine preferences by data mining from physicochemical properties. <em>Decision Support Systems</em> 47, 547–553.</p>
</div>
<div id="ref-esl">
<p>Hastie T, Tibshirani R, Friedman J (2017) <em>The elements of statistical learning</em>. Springer-Verlag <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-with-trees-and-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
