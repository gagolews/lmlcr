<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Clustering | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="pandoc, pandoc-citeproc, knitr, bookdown (custom), GitBook, and many more" />

  <meta property="og:title" content="7 Clustering | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="continuous-optimisation-with-iterative-algorithms.html"/>
<link rel="next" href="optimisation-with-genetic-algorithms.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<!-- Use KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<!-- /Use KaTeX -->

<style type="text/css">
/* Marek's custom CSS */

@import url("https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic,700italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans:400,500,600,700,800,900,400italic,500italic,600italic,700italic,800italic,900italic");
@import url("https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:400,600,700,400italic,600italic,700italic");

span.katex {
    font-size: 100%;
}

</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2.1 2021-05-10 10:04 (0e09aab)</a></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#exercises-in-r-2"><i class="fa fa-check"></i><b>4.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-preparing-data"><i class="fa fa-check"></i><b>4.4.1</b> EdStats – Preparing Data</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.4.2</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.4.3</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>4.4.4</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>4.4.5</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.5</b> Outro</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks (*)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="5.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>5.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Continuous Optimisation with Iterative Algorithms (*)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="6.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="6.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>6.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="6.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.5</b> Outro</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>7.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3</b> Agglomerative Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>7.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#exercises-in-r-3"><i class="fa fa-check"></i><b>7.4</b> Exercises in R</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>7.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>7.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>7.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.5</b> Outro</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms (*)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.2</b> Genetic Algorithms</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.2.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.3</b> Outro</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.3.1</b> Remarks</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems (*)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-16"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> Exercise: The MovieLens Dataset (*)</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a>
<ul>
<li class="chapter" data-level="B.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a>
<ul>
<li class="chapter" data-level="C.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a>
<ul>
<li class="chapter" data-level="C.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a>
<ul>
<li class="chapter" data-level="C.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>C.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a>
<ul>
<li class="chapter" data-level="D.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>D.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a>
<ul>
<li class="chapter" data-level="E.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a>
<ul>
<li class="chapter" data-level="E.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>E.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Clustering</h1>
<blockquote>
<p>This is a slightly older (distributed in the hope that it will be useful)
version of the forthcoming textbook (ETA 2022) preliminarily entitled
<em>Machine Learning in R from Scratch</em> by
<a href="https://www.gagolewski.com">Marek Gagolewski</a>, which is now undergoing
a major revision (when I am not busy with other projects). There will be
not much work on-going in this repository anymore, as its sources have
moved elsewhere; however, if you happen to find any bugs or typos,
please drop me an
<a href="https://github.com/gagolews/lmlcr/blob/master/CODE_OF_CONDUCT.md">email</a>.
I will share a new draft once it’s ripe. Stay tuned.</p>
</blockquote>
<div id="unsupervised-learning" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Unsupervised Learning</h2>
<div id="introduction-12" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Introduction</h3>
<!--
TODO: If this is the first chapter where you use `iris`, tell about it

Definitely some cooler datasets are needed for the Exercises sec.


Exercise: dimensionality reduction with auto-encoders?

Exercise: apply genieclust

Exercise: implement MDS with optim?

-->
<p>In <strong>unsupervised learning</strong> (learning without a teacher),
the input data points <span class="math inline">\(\mathbf{x}_{1,\cdot},\dots,\mathbf{x}_{n,\cdot}\)</span>
are not assigned any reference labels (compare Figure <a href="clustering.html#fig:unsupervised">7.1</a>).</p>
<div class="figure"><span id="fig:unsupervised"></span>
<img src="07-clustering-figures/unsupervised-1.png" alt="" />
<p class="caption">Figure 7.1:  Unsupervised learning: “But what it is exactly that I have to do here?”</p>
</div>
<p>Our aim now is to discover the <strong>underlying structure in the data</strong>,
whatever that means.</p>
</div>
<div id="main-types-of-unsupervised-learning-problems" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Main Types of Unsupervised Learning Problems</h3>
<p>It turns out, however, that certain classes of unsupervised learning
problems are not only intellectually stimulating,
but practically useful at the same time.</p>
<p>In particular, in <strong>dimensionality reduction</strong> we seek a meaningful
<em>projection</em> of a high dimensional
space (think: many variables/columns).</p>
<div class="figure"><span id="fig:princomp"></span>
<img src="07-clustering-figures/princomp-1.png" alt="" />
<p class="caption">Figure 7.2:  Principal component analysis (a dimensionality reduction technique) applied on three features of red wines</p>
</div>
<p>For instance, Figure <a href="clustering.html#fig:princomp">7.2</a> reveals that the
“alcohol”, “response” and “residual.sugar” dimensions of the Wine Quality
dataset that we have studied earlier on can actually be nicely depicted (with
no much loss of information) on a two-dimensional plot.
It turns out that the wine experts’ opinion on a wine’s quality is highly
correlated with the amount of… alcohol in a bottle.
On the other hand, sugar is orthogonal (unrelated) to these two.</p>
<p>Amongst example dimensionality reduction methods we find:</p>
<ul>
<li>Multidimensional scaling (MDS)</li>
<li>Principal component analysis (PCA)</li>
<li>Kernel PCA</li>
<li>t-SNE</li>
<li>Autoencoders (deep learning)</li>
</ul>
<p>See, for example, <span class="citation">(Hastie et al. <a href="#ref-esl" role="doc-biblioref">2017</a>)</span> for more details.</p>
<div style="margin-top: 1em">

</div>
<p>Furthermore, in <strong>anomaly detection</strong>, our task is to identify rare, suspicious, ab-normal
or out-standing items.
For example, these can be cars on walkways in a park’s security camera footage.</p>
<div class="figure">
<img src="07-clustering-figures/anomaly_detection-1.png" alt="" />
<p class="caption">(#fig:anomaly_detection) Outliers can be thought of anomalies of some sort</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Finally, the aim of <strong>clustering</strong> is to automatically discover some <em>naturally occurring</em>
subgroups in the data set, compare Figure <a href="clustering.html#fig:clustering-illustration">7.3</a>.
For example, these may be customers having different shopping patterns
(such as “young parents”, “students”, “boomers”).</p>
<div class="figure"><span id="fig:clustering-illustration"></span>
<img src="07-clustering-figures/clustering-illustration-1.png" alt="" />
<p class="caption">Figure 7.3:  NEWS FLASH! SCIENTISTS SHOWED (by writing about it) THAT SOME VERY IMPORTANT THING (Iris dataset) COMES IN THREE DIFFERENT FLAVOURS (by applying the 3-means clustering algorithm)!</p>
</div>
</div>
<div id="definitions" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Definitions</h3>
<p>Formally, given <span class="math inline">\(K\ge 2\)</span>, <strong>clustering</strong> aims is to find a <em>special kind</em>
of a <strong><span class="math inline">\(K\)</span>-partition</strong> of the input data set <span class="math inline">\(\mathbf{X}\)</span>.</p>
<dl>
<dt>Definition.</dt>
<dd><p>We say that <span class="math inline">\(\mathcal{C}=\{C_1,\dots,C_K\}\)</span> is a <strong><span class="math inline">\(K\)</span>-partition</strong>
of <span class="math inline">\(\mathbf{X}\)</span> of size <span class="math inline">\(n\)</span>,
whenever:</p>
<ul>
<li><span class="math inline">\(C_k\neq\emptyset\)</span> for all <span class="math inline">\(k\)</span> (each set is nonempty),</li>
<li><span class="math inline">\(C_k\cap C_l=\emptyset\)</span> for all <span class="math inline">\(k\neq l\)</span> (sets are pairwise disjoint),</li>
<li><span class="math inline">\(\bigcup_{k=1}^K C_k=\mathbf{X}\)</span> (no point is neglected).</li>
</ul>
</dd>
</dl>
<p>This can also be thought of as assigning each point a unique label <span class="math inline">\(\{1,\dots,K\}\)</span>
(think: colouring of the points, where each number has a colour).
We will consider the point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> as labelled <span class="math inline">\(j\)</span>
if and only if it belongs to cluster <span class="math inline">\(C_j\)</span>, i.e., <span class="math inline">\(\mathbf{x}_{i,\cdot}\in C_j\)</span>.</p>
<p>Example applications of clustering:</p>
<ul>
<li><em>taxonomisation</em>: e.g.,
partition the consumers to more “uniform”
groups to better understand who they are and what do they need,</li>
<li><em>image processing</em>:
e.g., object detection, like tumour tissues on medical images,</li>
<li><em>complex networks analysis</em>:
e.g., detecting communities in friendship,
retweets and other networks,</li>
<li><em>fine-tuning supervised learning algorithms</em>:
e.g., recommender systems indicating content
that was rated highly by users from the same group
or learning multiple manifolds in a dimension reduction task.</li>
</ul>
<p>The number of possible <span class="math inline">\(K\)</span>-partitions of a set with <span class="math inline">\(n\)</span> elements is given by
<em>the Stirling number of the second kind</em>:</p>
<p><span class="math display">\[
\left\{{n \atop K}\right\}={\frac  {1}{K!}}\sum _{{j=0}}^{{K}}(-1)^{{K-j}}{\binom  {K}{j}}j^{n};
\]</span></p>
<p>e.g., already <span class="math inline">\(\left\{{n \atop 2}\right\}=2^{n-1}-1\)</span>
and <span class="math inline">\(\left\{{n \atop 3}\right\}=O(3^n)\)</span> – that is a lot.
Certainly, we are not just interested in “any” partition – some of them
will be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:</p>
<dl>
<dt>“Definition”.</dt>
<dd><p>Clustering concerns “segmenting a collection of objects into subsets
so that those within each cluster are more <strong>closely related</strong>
to one another than objects assigned to different clusters” <span class="citation">(Hastie et al. <a href="#ref-esl" role="doc-biblioref">2017</a>)</span>.</p>
</dd>
</dl>
<p>It is not uncommon <!-- TODO: cite -->
to equate the general definition of data clustering problems with… the
particular outputs yield by specific clustering algorithms. It some sense,
that sounds fair. From this perspective, we might be interested in
identifying the two main types of clustering algorithms:</p>
<ul>
<li><strong>parametric</strong> (model-based):
<ul>
<li>find clusters of specific shapes or following specific multidimensional
probability distributions,</li>
<li>e.g., <span class="math inline">\(K\)</span>-means, expectation-maximisation for Gaussian mixtures (EM),
average linkage agglomerative clustering;</li>
</ul></li>
<li><strong>nonparametric</strong> (model-free):
<ul>
<li>identify high-density or well-separable regions,
perhaps in the presence of noise points,</li>
<li>e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.</li>
</ul></li>
</ul>
<p>In this chapter we’ll take a look at two classical approaches to clustering:</p>
<ul>
<li><em>K-means clustering</em> that looks for a specific number of clusters,</li>
<li><em>(agglomerative) hierarchical clustering</em> that outputs a whole hierarchy
of nested data partitions.</li>
</ul>
</div>
</div>
<div id="k-means-clustering" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> K-means Clustering</h2>
<div id="example-in-r-5" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Example in R</h3>
<p>Let’s begin our clustering adventure
by applying the <span class="math inline">\(K\)</span>-means clustering method to find <span class="math inline">\(K=3\)</span> groups
in the famous Fisher’s <code>iris</code> data set (variables <code>Sepal.Width</code>
and <code>Petal.Length</code> variables only):</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb653-1"><a href="clustering.html#cb653-1" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(iris[,<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>)])</span>
<span id="cb653-2"><a href="clustering.html#cb653-2" aria-hidden="true"></a><span class="co"># never forget to set nstart&gt;&gt;1!</span></span>
<span id="cb653-3"><a href="clustering.html#cb653-3" aria-hidden="true"></a>km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dt">centers=</span><span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</span>
<span id="cb653-4"><a href="clustering.html#cb653-4" aria-hidden="true"></a>km<span class="op">$</span>cluster <span class="co"># labels assigned to each of 150 points:</span></span></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [69] 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [ reached getOption(&quot;max.print&quot;) -- omitted 51 entries ]</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>Later we’ll see that <code>nstart</code> is responsible for random restarting the
(local) optimisation procedure, just as we did in the previous chapter.</p>
</dd>
</dl>
<p>Let’s draw a scatter plot that depicts the detected clusters:</p>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb655-1"><a href="clustering.html#cb655-1" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster)</span></code></pre></div>
<div class="figure"><span id="fig:kmeans12"></span>
<img src="07-clustering-figures/kmeans12-1.png" alt="" />
<p class="caption">Figure 7.4:  3-means clustering on a projection of the Iris dataset</p>
</div>
<p>The colours in Figure <a href="clustering.html#fig:kmeans12">7.4</a> indicate the detected clusters.
The left group is clearly well-separated from the other two.</p>
<p>What can we do with this information? Well, if we were experts on plants
(in the 1930s), that’d definitely be something ground-breaking.
Figure <a href="clustering.html#fig:kmeans123">7.5</a> is a version of the aforementioned scatter plot
now with the true iris species added.</p>
<div class="sourceCode" id="cb656"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb656-1"><a href="clustering.html#cb656-1" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster, <span class="dt">pch=</span><span class="kw">as.numeric</span>(iris<span class="op">$</span>Species))</span></code></pre></div>
<div class="figure"><span id="fig:kmeans123"></span>
<img src="07-clustering-figures/kmeans123-1.png" alt="" />
<p class="caption">Figure 7.5:  3-means clustering (colours) vs true Iris species (shapes)</p>
</div>
<p>Here is a contingency table for detected clusters vs. true iris species:</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="clustering.html#cb657-1" aria-hidden="true"></a>(C &lt;-<span class="st"> </span><span class="kw">table</span>(km<span class="op">$</span>cluster, iris<span class="op">$</span>Species))</span></code></pre></div>
<pre><code>##    
##     setosa versicolor virginica
##   1     50          0         0
##   2      0          2        41
##   3      0         48         9</code></pre>
<p>It turns out that the discovered partition matches the original iris
species very well. We have just made a “discovery” in the field of
botany (actually some research fields classify their objects of study
into families, genres etc. by means of such tools).</p>
<p>Were the actual Iris species what we had hoped to match?
Was that our aim? Well, surely we have had begun our journey with
“clear minds” (yet with hungry eyes). Note that the true class labels
were not used during the clustering procedure – we’re dealing with
an unsupervised learning problem here. The result turned useful,
it’s a win.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) There are several indices that assess the
similarity of two partitions, for example the Adjusted Rand Index (ARI)
the Normalised Mutual Information Score (NMI)
or set matching-based measures,
see, e.g., <span class="citation">(Hubert &amp; Arabie <a href="#ref-comparing_paritions" role="doc-biblioref">1985</a>)</span>, <span class="citation">(Rezaei &amp; Fränti <a href="#ref-external_cluster_validity" role="doc-biblioref">2016</a>)</span>.</p>
</dd>
</dl>
<!-- genieclust.compare_partitions ? -->
<!-- OK, this is not the best measure:

sum(apply(C, 1, max))/sum(C) # "accuracy"

genieclust.compare_partitions ?

-->
</div>
<div id="problem-statement" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Problem Statement</h3>
<p>The aim of <em><span class="math inline">\(K\)</span>-means clustering</em> is to find <span class="math inline">\(K\)</span> “good” cluster centres
<span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p>
<p>Then, a point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> will be assigned to the
cluster represented by the closest centre. Here, by <em>closest</em>
we mean the <em>squared</em> Euclidean distance.</p>
<p>More formally,
assuming all the points are in a <span class="math inline">\(p\)</span>-dimensional space, <span class="math inline">\(\mathbb{R}^p\)</span>,
we define the distance between the <span class="math inline">\(i\)</span>-th point and the <span class="math inline">\(k\)</span>-th
centre as:</p>
<p><span class="math display">\[
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}) = \| \mathbf{x}_{i,\cdot} - \boldsymbol\mu_{k,\cdot} \|^2 =  \sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\]</span></p>
<p>Then the <span class="math inline">\(i\)</span>-th point’s cluster is determined by:</p>
<p><span class="math display">\[
\mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}),
\]</span></p>
<p>where, as usual, <span class="math inline">\(\mathrm{arg}\min\)</span> (argument minimum) is the index
<span class="math inline">\(k\)</span> that minimises the given expression.</p>
<p>In the previous example, the three identified cluster centres in <span class="math inline">\(\mathbb{R}^2\)</span>
are given by (see Figure <a href="clustering.html#fig:kmeans-problem1">7.6</a> for illustration):</p>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="clustering.html#cb659-1" aria-hidden="true"></a>km<span class="op">$</span>centers</span></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1       1.4620      3.4280
## 2       5.6721      3.0326
## 3       4.3281      2.7509</code></pre>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb661-1"><a href="clustering.html#cb661-1" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster, <span class="dt">asp=</span><span class="dv">1</span>) <span class="co"># asp=1 gives the same scale on both axes</span></span>
<span id="cb661-2"><a href="clustering.html#cb661-2" aria-hidden="true"></a><span class="kw">points</span>(km<span class="op">$</span>centers, <span class="dt">cex=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">4</span>, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<div class="figure"><span id="fig:kmeans-problem1"></span>
<img src="07-clustering-figures/kmeans-problem1-1.png" alt="" />
<p class="caption">Figure 7.6:  Cluster centres (blue dots) identified by the 3-means algorithm</p>
</div>
<p>Figure <a href="clustering.html#fig:kmeans-problem2">7.7</a> depicts the partition
of the whole <span class="math inline">\(\mathbb{R}^2\)</span> space
into clusters based on the closeness to the three cluster centres.</p>
<div class="figure"><span id="fig:kmeans-problem2"></span>
<img src="07-clustering-figures/kmeans-problem2-1.png" alt="" />
<p class="caption">Figure 7.7:  The division of the whole space into three sets based on the proximity to cluster centres (a so-called Voronoi diagram)</p>
</div>
<p>To compute the distances between all the points and the cluster centres,
we may call <code>pdist::pdist()</code>:</p>
<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb662-1"><a href="clustering.html#cb662-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;pdist&quot;</span>)</span>
<span id="cb662-2"><a href="clustering.html#cb662-2" aria-hidden="true"></a>D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, km<span class="op">$</span>centers))<span class="op">^</span><span class="dv">2</span></span>
<span id="cb662-3"><a href="clustering.html#cb662-3" aria-hidden="true"></a><span class="kw">head</span>(D)</span></code></pre></div>
<pre><code>##          [,1]   [,2]   [,3]
## [1,] 0.009028 18.469 9.1348
## [2,] 0.187028 18.252 8.6357
## [3,] 0.078228 19.143 9.3709
## [4,] 0.109028 17.411 8.1199
## [5,] 0.033428 18.573 9.2946
## [6,] 0.279428 16.530 8.2272</code></pre>
<p>where <code>D[i,k]</code> gives the squared Euclidean distance between
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> and <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span>.</p>
<p>The cluster memberships the (<span class="math inline">\(\mathrm{arg}\min\)</span>s)
can now be determined by:</p>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb664-1"><a href="clustering.html#cb664-1" aria-hidden="true"></a>(idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)) <span class="co"># for every row of D...</span></span></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [69] 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [ reached getOption(&quot;max.print&quot;) -- omitted 51 entries ]</code></pre>
<div class="sourceCode" id="cb666"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb666-1"><a href="clustering.html#cb666-1" aria-hidden="true"></a><span class="kw">all</span>(km<span class="op">$</span>cluster <span class="op">==</span><span class="st"> </span>idx) <span class="co"># sanity check</span></span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="algorithms-for-the-k-means-problem" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Algorithms for the K-means Problem</h3>
<p>All good, but how do we find “good” cluster centres?
Good, better, best… yet again we are in a need for a goodness-of-fit metric.
In the <span class="math inline">\(K\)</span>-means clustering, we determine
<span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>
that minimise the total within-cluster distances (distances from each point
to each own cluster centre):</p>
<p><span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{C(i),\cdot}),
\]</span></p>
<p>Note that the <span class="math inline">\(\boldsymbol\mu\)</span>s are also “hidden” inside the point-to-cluster
belongingness mapping, <span class="math inline">\(\mathrm{C}\)</span>.
Expanding the above yields:</p>
<p><span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]</span></p>
<p>Unfortunately, the <span class="math inline">\(\min\)</span> operator in the objective function
makes this optimisation problem not tractable
with the methods discussed in the previous chapter.</p>
<p>The above problem is <em>hard</em> to solve (* more precisely,
it is an NP-hard problem).
Therefore, in practice we use various heuristics to solve it.
The <code>kmeans()</code> function itself implements 3 of them:
the Hartigan-Wong, Lloyd (a.k.a. Lloyd-Forgy) and MacQueen algorithms.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Technically, there is no such thing as “<em>the</em> K-means algorithm” –
all the aforementioned methods are particular heuristic
approaches to solving the K-means
clustering problem formalised as the above optimisation task.
By setting <code>nstart = 10</code> above, we ask the (Hartigan-Wong, which is
the default one in <code>kmeans()</code>) algorithm
to find 10 solution candidates obtained by considering different random
initial clusterings and choose the best one (with respect to the sum
of within-cluster distances) amongst them. This does not guarantee
finding the optimal solution, especially for very unbalanced datasets,
but increases the likelihood of such.</p>
</dd>
<dt>Remark.</dt>
<dd><p>The <em>squared</em> Euclidean distance was of course chosen to make computations
easier. It turns out that for any given subset of input points
<span class="math inline">\(\mathbf{x}_{i_1,\cdot},\dots,\mathbf{x}_{i_m,\cdot}\)</span>,
the point <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> that minimises the total distances
to all of them, i.e.,</p>
<p><span class="math display">\[
    \min_{\boldsymbol\mu_{k,\cdot}\in \mathbb{R}^p}
    \sum_{\ell=1}^m \left(
    \sum_{j=1}^p \left(x_{i_\ell,j}-\mu_{k,j}\right)^2
    \right),
\]</span></p>
<p>is exactly these points’ <em>centroid</em> – which is given by
the componentwise arithmetic means of their coordinates.</p>
<p>For example:</p>
<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb668-1"><a href="clustering.html#cb668-1" aria-hidden="true"></a><span class="kw">colMeans</span>(X[km<span class="op">$</span>cluster <span class="op">==</span><span class="st"> </span><span class="dv">1</span>,]) <span class="co"># centroid of the points in the 1st cluster</span></span></code></pre></div>
<pre><code>## Petal.Length  Sepal.Width 
##        1.462        3.428</code></pre>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb670-1"><a href="clustering.html#cb670-1" aria-hidden="true"></a>km<span class="op">$</span>centers[<span class="dv">1</span>,] <span class="co"># the centre of the 1st cluster</span></span></code></pre></div>
<pre><code>## Petal.Length  Sepal.Width 
##        1.462        3.428</code></pre>
</dd>
</dl>
<div style="margin-top: 1em">

</div>
<p>Among the various heuristics to solve the K-means problem,
Lloyd’s algorithm (1957) is perhaps the simplest.
This is probably the reason why it is sometimes referred
to as “the” K-means algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Start with random cluster centres <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p></li>
<li><p>For each point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>, determine its closest centre <span class="math inline">\(C(i)\in\{1,\dots,K\}\)</span>:</p>
<p><span class="math display">\[
 \mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
 d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}).
 \]</span></p></li>
<li><p>For each cluster <span class="math inline">\(k\in\{1,\dots,K\}\)</span>, compute the new cluster centre <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> as the centroid of
all the point indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(C(i)=k\)</span>.</p></li>
<li><p>If the cluster centres changed since the last iteration, go to step 2,
otherwise stop and return the result.</p></li>
</ol>
<div style="margin-top: 1em">

</div>
<p>(*) Here’s an example implementation.
As the initial cluster centres, let’s pick some “noisy” versions
of <span class="math inline">\(K\)</span> randomly chosen points in <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="clustering.html#cb672-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb672-2"><a href="clustering.html#cb672-2" aria-hidden="true"></a>K &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb672-3"><a href="clustering.html#cb672-3" aria-hidden="true"></a></span>
<span id="cb672-4"><a href="clustering.html#cb672-4" aria-hidden="true"></a><span class="co"># Random initial cluster centres:</span></span>
<span id="cb672-5"><a href="clustering.html#cb672-5" aria-hidden="true"></a>M &lt;-<span class="st"> </span><span class="kw">jitter</span>(X[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X), K),])</span>
<span id="cb672-6"><a href="clustering.html#cb672-6" aria-hidden="true"></a>M</span></code></pre></div>
<pre><code>##      Petal.Length Sepal.Width
## [1,]       5.1004      3.0814
## [2,]       4.7091      3.1861
## [3,]       3.3196      2.4094</code></pre>
<p>In what follows, we will be maintaining a matrix
such that <code>D[i,k]</code> is the distance between the <span class="math inline">\(i\)</span>-th point
and the <span class="math inline">\(k\)</span>-th centre and a vector such that <code>idx[i]</code>
denotes the index of the cluster centre closest to the <code>i</code>-th point.</p>
<div class="sourceCode" id="cb674"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb674-1"><a href="clustering.html#cb674-1" aria-hidden="true"></a>D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, M))<span class="op">^</span><span class="dv">2</span></span>
<span id="cb674-2"><a href="clustering.html#cb674-2" aria-hidden="true"></a>idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)</span></code></pre></div>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb675-1"><a href="clustering.html#cb675-1" aria-hidden="true"></a><span class="cf">repeat</span> {</span>
<span id="cb675-2"><a href="clustering.html#cb675-2" aria-hidden="true"></a>    <span class="co"># Determine the new cluster centres:</span></span>
<span id="cb675-3"><a href="clustering.html#cb675-3" aria-hidden="true"></a>    M &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="cf">function</span>(k) {</span>
<span id="cb675-4"><a href="clustering.html#cb675-4" aria-hidden="true"></a>        <span class="co"># the centroid of all points in the k-th cluster:</span></span>
<span id="cb675-5"><a href="clustering.html#cb675-5" aria-hidden="true"></a>        <span class="kw">colMeans</span>(X[idx<span class="op">==</span>k,])</span>
<span id="cb675-6"><a href="clustering.html#cb675-6" aria-hidden="true"></a>    }))</span>
<span id="cb675-7"><a href="clustering.html#cb675-7" aria-hidden="true"></a></span>
<span id="cb675-8"><a href="clustering.html#cb675-8" aria-hidden="true"></a>    <span class="co"># Store the previous cluster belongingness info:</span></span>
<span id="cb675-9"><a href="clustering.html#cb675-9" aria-hidden="true"></a>    old_idx &lt;-<span class="st"> </span>idx</span>
<span id="cb675-10"><a href="clustering.html#cb675-10" aria-hidden="true"></a></span>
<span id="cb675-11"><a href="clustering.html#cb675-11" aria-hidden="true"></a>    <span class="co"># Recompute D and idx:</span></span>
<span id="cb675-12"><a href="clustering.html#cb675-12" aria-hidden="true"></a>    D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, M))<span class="op">^</span><span class="dv">2</span></span>
<span id="cb675-13"><a href="clustering.html#cb675-13" aria-hidden="true"></a>    idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)</span>
<span id="cb675-14"><a href="clustering.html#cb675-14" aria-hidden="true"></a></span>
<span id="cb675-15"><a href="clustering.html#cb675-15" aria-hidden="true"></a>    <span class="co"># Check if converged already:</span></span>
<span id="cb675-16"><a href="clustering.html#cb675-16" aria-hidden="true"></a>    <span class="cf">if</span> (<span class="kw">all</span>(idx <span class="op">==</span><span class="st"> </span>old_idx)) <span class="cf">break</span></span>
<span id="cb675-17"><a href="clustering.html#cb675-17" aria-hidden="true"></a>}</span></code></pre></div>
<p>Let’s compare the obtained cluster centres with the ones returned
by <code>kmeans()</code>:</p>
<div class="sourceCode" id="cb676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb676-1"><a href="clustering.html#cb676-1" aria-hidden="true"></a>M <span class="co"># our result</span></span></code></pre></div>
<pre><code>##      Petal.Length Sepal.Width
## [1,]       5.6721      3.0326
## [2,]       4.3281      2.7509
## [3,]       1.4620      3.4280</code></pre>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="clustering.html#cb678-1" aria-hidden="true"></a>km<span class="op">$</span>center <span class="co"># result of kmeans()</span></span></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1       1.4620      3.4280
## 2       5.6721      3.0326
## 3       4.3281      2.7509</code></pre>
<p>These two represent exactly the same 3-partitions
(note that the actual labels (the order of centres) are not important).</p>
<p>The value of the objective function (total within-cluster distances)
at the identified candidate solution
is equal to:</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="clustering.html#cb680-1" aria-hidden="true"></a><span class="kw">sum</span>(D[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X),idx)]) <span class="co"># indexing with a 2-column matrix!</span></span></code></pre></div>
<pre><code>## [1] 40.737</code></pre>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb682-1"><a href="clustering.html#cb682-1" aria-hidden="true"></a>km<span class="op">$</span>tot.withinss <span class="co"># as reported by kmeans()</span></span></code></pre></div>
<pre><code>## [1] 40.737</code></pre>
<p>We would need it if we were to implement the <code>nstart</code> functionality,
which is left as an:</p>
<div class="exercise"><strong>Exercise.</strong>
<p>(*) Wrap the implementation of the Lloyd algorithm into a standalone R function,
with a similar look-and-feel as the original <code>kmeans()</code>.</p>
</div>
<div class="figure"><span id="fig:kmeanimpl-plot"></span>
<img src="07-clustering-figures/kmeanimpl-plot-1.png" alt="" />
<p class="caption">Figure 7.8:  The arrows denote the cluster centres in each iteration of the Lloyd algorithm</p>
</div>
<p>On a side note, our algorithm needed <code>4</code>
iterations to identify the (locally optimal) cluster centres.
Figure <a href="clustering.html#fig:kmeanimpl-plot">7.8</a> depicts its quest for the clustering grail.</p>
<!--

# TODO: k-medoids???

## TODO




TODO maybe k-medoids, nice for optimisation
plus they allow for different metrics

alternatively, DBSCAN or PCA or MDS or ...



-->
</div>
</div>
<div id="agglomerative-hierarchical-clustering" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Agglomerative Hierarchical Clustering</h2>
<div id="introduction-13" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Introduction</h3>
<p>In K-means, we need to specify the number of clusters, <span class="math inline">\(K\)</span>, in advance.
What if we don’t have any idea how to choose this parameter (which is often
the case)?</p>
<p>Also, the problem with K-means is that there is no guarantee that a
<span class="math inline">\(K\)</span>-partition is any “similar” to the <span class="math inline">\(K&#39;\)</span>-one for <span class="math inline">\(K\neq K&#39;\)</span>,
see Figure <a href="clustering.html#fig:kmeans-different-K">7.9</a>.</p>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb684-1"><a href="clustering.html#cb684-1" aria-hidden="true"></a>km1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</span>
<span id="cb684-2"><a href="clustering.html#cb684-2" aria-hidden="true"></a>km2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dv">4</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</span>
<span id="cb684-3"><a href="clustering.html#cb684-3" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span>km1<span class="op">$</span>cluster, <span class="dt">pch=</span>km2<span class="op">$</span>cluster, <span class="dt">asp=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span id="fig:kmeans-different-K"></span>
<img src="07-clustering-figures/kmeans-different-K-1.png" alt="" />
<p class="caption">Figure 7.9:  3-means (colours) vs. 4-means (symbols) on example data; the “circle” cluster cannot decide if it likes the green or the black one more</p>
</div>
<p>Hierarchical methods, on the other hand, output a whole hierarchy
of mutually <em>nested</em> partitions, which increase the interpretability
of the results.
A <span class="math inline">\(K\)</span>-partition for any <span class="math inline">\(K\)</span> can be extracted later at any time.</p>
<p>In this book we will be interested in <em>agglomerative</em> hierarchical algorithms:</p>
<ul>
<li><p>at the lowest level of the hierarchy, each point belongs to its own
cluster (there are <span class="math inline">\(n\)</span> singletons);</p></li>
<li><p>at the highest level of the hierarchy,
there is one cluster that embraces all the points;</p></li>
<li><p>moving from the <span class="math inline">\(i\)</span>-th to the <span class="math inline">\((i+1)\)</span>-th level,
we select (somehow; see below) a pair of clusters to be merged.</p></li>
</ul>
</div>
<div id="example-in-r-6" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Example in R</h3>
<p>The most basic implementation of a few
agglomerative hierarchical clustering algorithms
is provided by the <code>hclust()</code> function, which works on a pairwise
distance matrix.</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb685-1"><a href="clustering.html#cb685-1" aria-hidden="true"></a><span class="co"># Euclidean distances between all pairs of points:</span></span>
<span id="cb685-2"><a href="clustering.html#cb685-2" aria-hidden="true"></a>D &lt;-<span class="st"> </span><span class="kw">dist</span>(X)</span>
<span id="cb685-3"><a href="clustering.html#cb685-3" aria-hidden="true"></a><span class="co"># Apply Complete Linkage (the default, details below):</span></span>
<span id="cb685-4"><a href="clustering.html#cb685-4" aria-hidden="true"></a>h &lt;-<span class="st"> </span><span class="kw">hclust</span>(D) <span class="co"># method=&quot;complete&quot;</span></span>
<span id="cb685-5"><a href="clustering.html#cb685-5" aria-hidden="true"></a><span class="kw">print</span>(h)</span></code></pre></div>
<pre><code>## 
## Call:
## hclust(d = D)
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 150</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>There are <span class="math inline">\(n(n-1)/2\)</span> unique pairwise distances between <span class="math inline">\(n\)</span> points.
Don’t try calling <code>dist()</code> on large data matrices.
Already <span class="math inline">\(n=100{,}000\)</span> points consumes 40 GB of available memory
(assuming that each distance is stored as an 8-byte double-precision floating
point number); packages <code>fastcluster</code> and <code>genieclust</code>, among other,
aim to solve this problem.</p>
</dd>
</dl>
<!--
#n <- 100000
#8*n*(n-1)/2/1e9
-->
<p>The obtained hierarchy (<em>tree</em>) can be <em>cut</em> at an arbitrary level
by applying the <code>cutree()</code> function.</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="clustering.html#cb687-1" aria-hidden="true"></a><span class="kw">cutree</span>(h, <span class="dt">k=</span><span class="dv">3</span>) <span class="co"># extract the 3-partition</span></span></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [69] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [ reached getOption(&quot;max.print&quot;) -- omitted 51 entries ]</code></pre>
<p>The cuts of the hierarchy at different levels
are depicted in Figure <a href="clustering.html#fig:complete-linkage-hier5-intro">7.10</a>.
The obtained 3-partition also matches the true Iris species quite well.
However, now it makes total sense to “zoom” our partitioning in or out
and investigate how are the subgroups decomposed or aggregated when we change
<span class="math inline">\(K\)</span>.</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="clustering.html#cb689-1" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb689-2"><a href="clustering.html#cb689-2" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span><span class="kw">cutree</span>(h, <span class="dt">k=</span><span class="dv">5</span>), <span class="dt">ann=</span><span class="ot">FALSE</span>)</span>
<span id="cb689-3"><a href="clustering.html#cb689-3" aria-hidden="true"></a><span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, <span class="dt">legend=</span><span class="st">&quot;k=5&quot;</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb689-4"><a href="clustering.html#cb689-4" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span><span class="kw">cutree</span>(h, <span class="dt">k=</span><span class="dv">4</span>), <span class="dt">ann=</span><span class="ot">FALSE</span>)</span>
<span id="cb689-5"><a href="clustering.html#cb689-5" aria-hidden="true"></a><span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, <span class="dt">legend=</span><span class="st">&quot;k=4&quot;</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb689-6"><a href="clustering.html#cb689-6" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span><span class="kw">cutree</span>(h, <span class="dt">k=</span><span class="dv">3</span>), <span class="dt">ann=</span><span class="ot">FALSE</span>)</span>
<span id="cb689-7"><a href="clustering.html#cb689-7" aria-hidden="true"></a><span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, <span class="dt">legend=</span><span class="st">&quot;k=3&quot;</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb689-8"><a href="clustering.html#cb689-8" aria-hidden="true"></a><span class="kw">plot</span>(X, <span class="dt">col=</span><span class="kw">cutree</span>(h, <span class="dt">k=</span><span class="dv">2</span>), <span class="dt">ann=</span><span class="ot">FALSE</span>)</span>
<span id="cb689-9"><a href="clustering.html#cb689-9" aria-hidden="true"></a><span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, <span class="dt">legend=</span><span class="st">&quot;k=2&quot;</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:complete-linkage-hier5-intro"></span>
<img src="07-clustering-figures/complete-linkage-hier5-intro-1.png" alt="" />
<p class="caption">Figure 7.10:  Complete linkage – 4 different cuts</p>
</div>
</div>
<div id="linkage-functions" class="section level3" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Linkage Functions</h3>
<p>Let’s formalise the clustering process.
Initially, <span class="math inline">\(\mathcal{C}^{(0)}=\{\{\mathbf{x}_{1,\cdot}\},\dots,\{\mathbf{x}_{n,\cdot}\}\}\)</span>,
i.e., each point is a member of its own cluster.</p>
<p>While an agglomerative hierarchical clustering algorithm is being computed,
there are <span class="math inline">\(n-k\)</span> clusters at the <span class="math inline">\(k\)</span>-th step of the procedure,
<span class="math inline">\(\mathcal{C}^{(k)}=\{C_1^{(k)},\dots,C_{n-k}^{(k)}\}\)</span>.</p>
<p>When proceeding from step <span class="math inline">\(k\)</span> to <span class="math inline">\(k+1\)</span>,
we determine the two groups <span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span>, <span class="math inline">\(u&lt;v\)</span>,
to be <em>merged</em> together so that the clustering at the higher level
is of the form:
<span class="math display">\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]</span></p>
<p>Thus, <span class="math inline">\((\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})\)</span>
form a sequence of <em>nested</em> partitions of
the input dataset
with the last level being just one big cluster,
<span class="math inline">\(\mathcal{C}^{(n-1)}=\left\{ \{\mathbf{x}_{1,\cdot},\mathbf{x}_{2,\cdot},\dots,\mathbf{x}_{n,\cdot}\} \right\}\)</span>.</p>
<div style="margin-top: 1em">

</div>
<p>There is one component missing – how to determine the pair
of clusters <span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span> to be merged with each other
at the <span class="math inline">\(k\)</span>-th iteration?
Of course this will be expressed as some optimisation problem (although this time,
a simple one)! The decision will be based on:
<span class="math display">\[
\mathrm{arg}\min_{u &lt; v} d^*(C_u^{(k)}, C_v^{(k)}),
\]</span>
where <span class="math inline">\(d^*(C_u^{(k)}, C_v^{(k)})\)</span> is the <em>distance</em> between two clusters
<span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span>.</p>
<p>Note that we usually only consider the distances between <em>individual points</em>,
not sets of points. Hence, <span class="math inline">\(d^*\)</span> must be a suitable extension
of a pointwise distance <span class="math inline">\(d\)</span> (usually the Euclidean metric)
to whole sets.</p>
<p>We will assume that <span class="math inline">\(d^*(\{\mathbf{x}_{i,\cdot}\},\{\mathbf{x}_{j,\cdot}\})= d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})\)</span>, i.e., the distance between
singleton clusters is the same as the distance between the points themselves.
As far as more populous point groups are concerned, there are many popular
choices of <span class="math inline">\(d^*\)</span> (which in the context of hierarchical clustering we call
<em>linkage functions</em>):</p>
<ul>
<li><p>single linkage:</p>
<p><span class="math display">\[
  d_\text{S}^*(C_u^{(k)}, C_v^{(k)}) =
  \min_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
  \]</span></p></li>
<li><p>complete linkage:</p>
<p><span class="math display">\[
  d_\text{C}^*(C_u^{(k)}, C_v^{(k)}) =
  \max_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
  \]</span></p></li>
<li><p>average linkage:</p>
<p><span class="math display">\[
  d_\text{A}^*(C_u^{(k)}, C_v^{(k)}) =
  \frac{1}{|C_u^{(k)}| |C_v^{(k)}|} \sum_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}}\sum_{\mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}).
  \]</span></p></li>
</ul>
<p>An illustration of the way different linkages are computed
is given in Figure <a href="clustering.html#fig:linkages">7.11</a>.</p>
<div class="figure"><span id="fig:linkages"></span>
<img src="07-clustering-figures/linkages-1.png" alt="" />
<p class="caption">Figure 7.11:  In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all pairwise distances</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Assuming <span class="math inline">\(d_\text{S}^*\)</span>, <span class="math inline">\(d_\text{C}^*\)</span> or <span class="math inline">\(d_\text{A}^*\)</span>
in the aforementioned procedure leads to single, complete or average
linkage-based agglomerative hierarchical clustering algorithms,
respectively
(referred to as single linkage etc. for brevity).</p>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb690-1"><a href="clustering.html#cb690-1" aria-hidden="true"></a>hs &lt;-<span class="st"> </span><span class="kw">hclust</span>(D, <span class="dt">method=</span><span class="st">&quot;single&quot;</span>)</span>
<span id="cb690-2"><a href="clustering.html#cb690-2" aria-hidden="true"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(D, <span class="dt">method=</span><span class="st">&quot;complete&quot;</span>)</span>
<span id="cb690-3"><a href="clustering.html#cb690-3" aria-hidden="true"></a>ha &lt;-<span class="st"> </span><span class="kw">hclust</span>(D, <span class="dt">method=</span><span class="st">&quot;average&quot;</span>)</span></code></pre></div>
<p>Figure <a href="clustering.html#fig:linkages-hier52">7.12</a> compares the 5-, 4- and 3-partitions
obtained by applying the 3 above linkages. Note that it’s in very nature of
the single linkage algorithm that it’s highly sensitive to outliers.</p>
<div class="figure"><span id="fig:linkages-hier52"></span>
<img src="07-clustering-figures/linkages-hier52-1.png" alt="" />
<p class="caption">Figure 7.12:  3 cuts of 3 different hierarchies</p>
</div>
</div>
<div id="cluster-dendrograms" class="section level3" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Cluster Dendrograms</h3>
<p>A <em>dendrogram</em> (which we can plot by calling <code>plot(h)</code>, where <code>h</code> is
the result returned by <code>hclust()</code>) depicts the distances
(as defined by the linkage function) between the clusters merged at every stage
of the agglomerative procedure.
This can provide us with some insight into the underlying data structure
as well as with hits about at which level the tree could be cut.</p>
<p>Figure <a href="clustering.html#fig:dendrograms">7.13</a> depicts the three dendrograms
that correspond to the clusterings obtained by applying different linkages.
Each tree has 150 leaves (at the bottom) that represent the 150 points
in our example dataset. Each “edge” (joint) represents a group of points
being merged. For instance, the very top joint in the middle subfigure
is located at height of <span class="math inline">\(\simeq 6\)</span>, which is exactly the
maximal pairwise distance (complete linkage) between the points
in the last two last clusters.</p>
<div class="figure"><span id="fig:dendrograms"></span>
<img src="07-clustering-figures/dendrograms-1.png" alt="" />
<p class="caption">Figure 7.13:  Cluster dendrograms for the single, complete and average linkages</p>
</div>
</div>
</div>
<div id="exercises-in-r-3" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Exercises in R</h2>
<div id="clustering-of-the-world-factbook" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Clustering of the World Factbook</h3>
<p>Let’s perform a cluster analysis of countries
based on the information contained in the World Factbook dataset:</p>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="clustering.html#cb691-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</span>
<span id="cb691-2"><a href="clustering.html#cb691-2" aria-hidden="true"></a>    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Remove all the columns that consist of more than 40 missing values.
Then remove all the rows with at least 1 missing value.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To remove appropriate columns, we must first count the number
of <code>NA</code>s in them.</p>
<div class="sourceCode" id="cb692"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb692-1"><a href="clustering.html#cb692-1" aria-hidden="true"></a>count_na_in_columns &lt;-<span class="st"> </span><span class="kw">sapply</span>(factbook, <span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x)))</span>
<span id="cb692-2"><a href="clustering.html#cb692-2" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span>factbook[count_na_in_columns <span class="op">&lt;=</span><span class="st"> </span><span class="dv">40</span>] <span class="co"># column removal</span></span></code></pre></div>
<p>Getting rid of the rows plagued by missing values is as simple as calling
the <code>na.omit()</code> function:</p>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb693-1"><a href="clustering.html#cb693-1" aria-hidden="true"></a>factbook &lt;-<span class="st"> </span><span class="kw">na.omit</span>(factbook) <span class="co"># row removal</span></span>
<span id="cb693-2"><a href="clustering.html#cb693-2" aria-hidden="true"></a><span class="kw">dim</span>(factbook) <span class="co"># how many rows and cols remained</span></span></code></pre></div>
<pre><code>## [1] 203  23</code></pre>
<p>Missing value removal is necessary for metric-based
clustering methods, especially K-means. Otherwise, some of the computed distances
would be not available.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Standardise all the numeric columns.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Distance-based
methods are very sensitive to the order of magnitude of
the variables, and our dataset is a mess with regards to this
(population, GDP, birth rate, oil production etc.) – standardisation
of variables is definitely a good idea:</p>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="clustering.html#cb695-1" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(factbook)) <span class="co"># skip `country`</span></span>
<span id="cb695-2"><a href="clustering.html#cb695-2" aria-hidden="true"></a>    factbook[[i]] &lt;-<span class="st"> </span>(factbook[[i]]<span class="op">-</span><span class="kw">mean</span>(factbook[[i]]))<span class="op">/</span></span>
<span id="cb695-3"><a href="clustering.html#cb695-3" aria-hidden="true"></a><span class="st">                        </span><span class="kw">sd</span>(factbook[[i]])</span></code></pre></div>
<p>Recall that Z-scores (values of the standardised variables)
have a very intuitive interpretation: <span class="math inline">\(0\)</span> is the value equal to the column
mean, <span class="math inline">\(1\)</span> is one standard deviation above the mean, <span class="math inline">\(-2\)</span> is two standard deviations
below the mean etc.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the 2-means algorithm, i.e., K-means with <span class="math inline">\(K=2\)</span>.
Analyse the results.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Calling <code>kmeans()</code>:</p>
<div class="sourceCode" id="cb696"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb696-1"><a href="clustering.html#cb696-1" aria-hidden="true"></a>km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(factbook[<span class="op">-</span><span class="dv">1</span>], <span class="dv">2</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</span></code></pre></div>
<p>Let’s split the country list w.r.t. the obtained cluster labels.
It turns out that the obtained partition is heavily imbalanced, so we’ll
print only the contents of the first group:</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="clustering.html#cb697-1" aria-hidden="true"></a>km_countries &lt;-<span class="st"> </span><span class="kw">split</span>(factbook[[<span class="dv">1</span>]], km<span class="op">$</span>cluster)</span>
<span id="cb697-2"><a href="clustering.html#cb697-2" aria-hidden="true"></a>km_countries[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] &quot;China&quot;         &quot;India&quot;         &quot;United States&quot;</code></pre>
<p>With regards to which criteria has the K-means algorithm distinguished
the countries? Let’s inspect the cluster centres to check the average
Z-scores of all the countries in each cluster:</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="clustering.html#cb699-1" aria-hidden="true"></a><span class="kw">t</span>(km<span class="op">$</span>centers) <span class="co"># transposed for readability</span></span></code></pre></div>
<pre><code>##                                        1          2
## area                            3.661581 -0.0549237
## population                      6.987279 -0.1048092
## median_age                      0.477991 -0.0071699
## population_growth_rate         -0.252774  0.0037916
## birth_rate                     -0.501030  0.0075155
## death_rate                      0.153915 -0.0023087
## net_migration_rate              0.236449 -0.0035467
## infant_mortality_rate          -0.139577  0.0020937
## life_expectancy_at_birth        0.251541 -0.0037731
## total_fertility_rate           -0.472716  0.0070907
## gdp_purchasing_power_parity     7.213681 -0.1082052
## gdp_real_growth_rate            0.369499 -0.0055425
## gdp_per_capita_ppp              0.298103 -0.0044715
## labor_force                     6.914319 -0.1037148
## taxes_and_other_revenues       -0.922735  0.0138410
## budget_surplus_or_deficit      -0.012627  0.0001894
## inflation_rate_consumer_prices -0.096626  0.0014494
## exports                         5.341178 -0.0801177
## imports                         5.956538 -0.0893481
## telephones_fixed_lines          5.989858 -0.0898479
## internet_users                  6.997126 -0.1049569
## airports                        4.551832 -0.0682775</code></pre>
<p>Countries in Cluster 2 are… average (Z-scores <span class="math inline">\(\simeq 0\)</span>).
On the other hand, the three countries in Cluster 1
dominate the others w.r.t. area, population, GDP PPP, labour force etc.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the complete linkage agglomerative hierarchical clustering algorithm.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Recall that the complete linkage-based method is implemented in the
<code>hclust()</code> function:</p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="clustering.html#cb701-1" aria-hidden="true"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(factbook[<span class="op">-</span><span class="dv">1</span>]) <span class="co"># skip `country`</span></span>
<span id="cb701-2"><a href="clustering.html#cb701-2" aria-hidden="true"></a>h &lt;-<span class="st"> </span><span class="kw">hclust</span>(d, <span class="dt">method=</span><span class="st">&quot;complete&quot;</span>)</span></code></pre></div>
<p>A “nice” number of clusters to divide our dataset into
can be read from the dendrogram, see Figure <a href="clustering.html#fig:clustering-factbook7">7.14</a>.</p>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="clustering.html#cb702-1" aria-hidden="true"></a><span class="kw">plot</span>(h, <span class="dt">labels=</span><span class="ot">FALSE</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>); <span class="kw">box</span>()</span></code></pre></div>
<div class="figure"><span id="fig:clustering-factbook7"></span>
<img src="07-clustering-figures/clustering-factbook7-1.png" alt="" />
<p class="caption">Figure 7.14:  Cluster dendrogram for the World Factbook dataset – Complete linkage</p>
</div>
<p>It seems that a 9-partition might reveal something interesting,
because it will distinguish two larger country groups.
However, there will be many singletons if we do so either way.</p>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb703-1"><a href="clustering.html#cb703-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">cutree</span>(h, <span class="dv">9</span>)</span>
<span id="cb703-2"><a href="clustering.html#cb703-2" aria-hidden="true"></a>h_countries &lt;-<span class="st"> </span><span class="kw">split</span>(factbook[[<span class="dv">1</span>]], y)</span>
<span id="cb703-3"><a href="clustering.html#cb703-3" aria-hidden="true"></a><span class="kw">sapply</span>(h_countries, length) <span class="co"># number of elements in each cluster</span></span></code></pre></div>
<pre><code>##   1   2   3   4   5   6   7   8   9 
## 138  56   1   1   3   1   1   1   1</code></pre>
<p>Most likely this is not an interesting partitioning of this dataset,
therefore we’ll not be exploring it any further.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the Genie clustering algorithm.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>The Genie algorithm <span class="citation">(Gagolewski et al. <a href="#ref-genie" role="doc-biblioref">2016</a>)</span> is a hierarchical clustering algorithm
implemented in R package <a href="https://genieclust.gagolewski.com/"><code>genieclust</code></a>.
It’s interface is compatible with <code>hclust()</code>.</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="clustering.html#cb705-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;genieclust&quot;</span>)</span>
<span id="cb705-2"><a href="clustering.html#cb705-2" aria-hidden="true"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(factbook[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb705-3"><a href="clustering.html#cb705-3" aria-hidden="true"></a>g &lt;-<span class="st"> </span><span class="kw">gclust</span>(d)</span></code></pre></div>
<p>The cluster dendrogram in Figure <a href="clustering.html#fig:clustering-factbook10">7.15</a>
reveals 3 evident clusters.</p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="clustering.html#cb706-1" aria-hidden="true"></a><span class="kw">plot</span>(g, <span class="dt">labels=</span><span class="ot">FALSE</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>); <span class="kw">box</span>()</span></code></pre></div>
<div class="figure"><span id="fig:clustering-factbook10"></span>
<img src="07-clustering-figures/clustering-factbook10-1.png" alt="" />
<p class="caption">Figure 7.15:  Cluster dendrogram for the World Factbook dataset – Genie algorithm</p>
</div>
<p>Let’s determine the 3-partition of the data set.</p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="clustering.html#cb707-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">cutree</span>(g, <span class="dv">3</span>)</span></code></pre></div>
<p>Here are few countries in each cluster:</p>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="clustering.html#cb708-1" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">cutree</span>(g, <span class="dv">3</span>)</span>
<span id="cb708-2"><a href="clustering.html#cb708-2" aria-hidden="true"></a><span class="kw">sapply</span>(<span class="kw">split</span>(factbook<span class="op">$</span>countr, y), sample, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##      1                    2                                  
## [1,] &quot;Dominican Republic&quot; &quot;Congo, Republic of the&quot;           
## [2,] &quot;Venezuela&quot;          &quot;Sao Tome and Principe&quot;            
## [3,] &quot;Sri Lanka&quot;          &quot;Tanzania&quot;                         
## [4,] &quot;Malta&quot;              &quot;Botswana&quot;                         
## [5,] &quot;China&quot;              &quot;Congo, Democratic Republic of the&quot;
## [6,] &quot;Tajikistan&quot;         &quot;Malawi&quot;                           
##      3             
## [1,] &quot;Lithuania&quot;   
## [2,] &quot;Portugal&quot;    
## [3,] &quot;Korea, South&quot;
## [4,] &quot;Bulgaria&quot;    
## [5,] &quot;Germany&quot;     
## [6,] &quot;Moldova&quot;</code></pre>
<p>We can draw the countries in each cluster
on a map by using the <code>rworldmap</code> package (see its documentation for more details),
see Figure <a href="clustering.html#fig:clustering-factbook12">7.16</a>.</p>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="clustering.html#cb710-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;rworldmap&quot;</span>)</span>
<span id="cb710-2"><a href="clustering.html#cb710-2" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;RColorBrewer&quot;</span>)</span>
<span id="cb710-3"><a href="clustering.html#cb710-3" aria-hidden="true"></a>mapdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">country=</span>factbook<span class="op">$</span>country, <span class="dt">cluster=</span>y)</span>
<span id="cb710-4"><a href="clustering.html#cb710-4" aria-hidden="true"></a><span class="co"># 3 country names must be adjusted to get a match</span></span>
<span id="cb710-5"><a href="clustering.html#cb710-5" aria-hidden="true"></a>mapdata<span class="op">$</span>country[mapdata<span class="op">$</span>country <span class="op">==</span><span class="st"> &quot;Czechia&quot;</span>] &lt;-<span class="st"> &quot;Czech Republic&quot;</span></span>
<span id="cb710-6"><a href="clustering.html#cb710-6" aria-hidden="true"></a>mapdata<span class="op">$</span>country[mapdata<span class="op">$</span>country <span class="op">==</span><span class="st"> &quot;Eswatini&quot;</span>] &lt;-<span class="st"> &quot;Swaziland&quot;</span></span>
<span id="cb710-7"><a href="clustering.html#cb710-7" aria-hidden="true"></a>mapdata<span class="op">$</span>country[mapdata<span class="op">$</span>country <span class="op">==</span><span class="st"> &quot;Cabo Verde&quot;</span>] &lt;-<span class="st"> &quot;Cape Verde&quot;</span></span>
<span id="cb710-8"><a href="clustering.html#cb710-8" aria-hidden="true"></a>mapdata &lt;-<span class="st"> </span><span class="kw">joinCountryData2Map</span>(mapdata, <span class="dt">joinCode=</span><span class="st">&quot;NAME&quot;</span>,</span>
<span id="cb710-9"><a href="clustering.html#cb710-9" aria-hidden="true"></a>  <span class="dt">nameJoinColumn=</span><span class="st">&quot;country&quot;</span>)</span></code></pre></div>
<pre><code>## 203 codes from your data successfully matched countries in the map
## 0 codes from your data failed to match with a country code in the map
## 40 codes from the map weren&#39;t represented in your data</code></pre>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="clustering.html#cb712-1" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb712-2"><a href="clustering.html#cb712-2" aria-hidden="true"></a><span class="kw">mapCountryData</span>(mapdata, <span class="dt">nameColumnToPlot=</span><span class="st">&quot;cluster&quot;</span>,</span>
<span id="cb712-3"><a href="clustering.html#cb712-3" aria-hidden="true"></a>    <span class="dt">catMethod=</span><span class="st">&quot;categorical&quot;</span>, <span class="dt">missingCountryCol=</span><span class="st">&quot;gray&quot;</span>,</span>
<span id="cb712-4"><a href="clustering.html#cb712-4" aria-hidden="true"></a>    <span class="dt">colourPalette=</span><span class="kw">brewer.pal</span>(<span class="dv">3</span>, <span class="st">&quot;Set1&quot;</span>),</span>
<span id="cb712-5"><a href="clustering.html#cb712-5" aria-hidden="true"></a>    <span class="dt">mapTitle=</span><span class="st">&quot;&quot;</span>, <span class="dt">addLegend=</span><span class="ot">TRUE</span>)</span></code></pre></div>
<div class="figure"><span id="fig:clustering-factbook12"></span>
<img src="07-clustering-figures/clustering-factbook12-1.png" alt="" />
<p class="caption">Figure 7.16:  3 clusters discovered by the Genie algorithm</p>
</div>
<p>Here are the average Z-scores in each cluster:</p>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="clustering.html#cb713-1" aria-hidden="true"></a><span class="kw">round</span>(<span class="kw">sapply</span>(<span class="kw">split</span>(factbook[<span class="op">-</span><span class="dv">1</span>], y), colMeans), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##                                     1      2      3
## area                            0.124 -0.068 -0.243
## population                      0.077 -0.058 -0.130
## median_age                      0.118 -1.219  1.261
## population_growth_rate         -0.227  1.052 -0.757
## birth_rate                     -0.316  1.370 -0.930
## death_rate                     -0.439  0.071  1.075
## net_migration_rate             -0.123  0.053  0.260
## infant_mortality_rate          -0.366  1.399 -0.835
## life_expectancy_at_birth        0.354 -1.356  0.812
## total_fertility_rate           -0.363  1.332 -0.758
## gdp_purchasing_power_parity     0.084 -0.213  0.052
## gdp_real_growth_rate           -0.062  0.126  0.002
## gdp_per_capita_ppp              0.021 -0.744  0.905
## labor_force                     0.087 -0.096 -0.107
## taxes_and_other_revenues       -0.095 -0.584  1.006
## budget_surplus_or_deficit      -0.113 -0.188  0.543
## inflation_rate_consumer_prices  0.044 -0.013 -0.099
## exports                        -0.013 -0.318  0.447
## imports                         0.007 -0.308  0.379
## telephones_fixed_lines          0.048 -0.244  0.186
## internet_users                  0.093 -0.178 -0.016
## airports                        0.104 -0.131 -0.107</code></pre>
<p>That is really interesting! The interpretation of the above is left
to the reader.</p>
<!--
# factbook$country[!(factbook$country %in% mapdata$country)]
-->
</details>
</div>
<div id="unbalance-dataset-k-means-needs-multiple-starts" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Unbalance Dataset – K-Means Needs Multiple Starts</h3>
<p>Let us consider a benchmark (artificial) dataset
proposed in <span class="citation">(Rezaei &amp; Fränti <a href="#ref-external_cluster_validity" role="doc-biblioref">2016</a>)</span>:</p>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="clustering.html#cb715-1" aria-hidden="true"></a>unbalance &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">read.csv</span>(<span class="st">&quot;datasets/sipu_unbalance.csv&quot;</span>,</span>
<span id="cb715-2"><a href="clustering.html#cb715-2" aria-hidden="true"></a>    <span class="dt">header=</span><span class="ot">FALSE</span>, <span class="dt">sep=</span><span class="st">&quot; &quot;</span>, <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>))</span>
<span id="cb715-3"><a href="clustering.html#cb715-3" aria-hidden="true"></a>unbalance &lt;-<span class="st"> </span>unbalance<span class="op">/</span><span class="dv">10000-30</span> <span class="co"># a more user-friendly scale</span></span></code></pre></div>
<p>According to its authors, this dataset is comprised of 8 clusters:
there are 3 groups on the lefthand side (2000 points each)
and 5 on the right side (100 each).</p>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="clustering.html#cb716-1" aria-hidden="true"></a><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="figure">
<img src="07-clustering-figures/sipu_unbalance2-1.png" alt="" />
<p class="caption">(#fig:sipu_unbalance2) <code>sipu_unbalance</code> dataset</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the K-means algorithm with <span class="math inline">\(K=8\)</span>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Of course, here by “the” K-means we mean the default method
available in the <code>kmeans()</code> function.
The clustering results are depicted in Figure <a href="clustering.html#fig:sipu-unbalance3a">7.17</a>.</p>
<div class="sourceCode" id="cb717"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb717-1"><a href="clustering.html#cb717-1" aria-hidden="true"></a>km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(unbalance, <span class="dv">8</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</span>
<span id="cb717-2"><a href="clustering.html#cb717-2" aria-hidden="true"></a><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>km<span class="op">$</span>cluster)</span></code></pre></div>
<div class="figure"><span id="fig:sipu-unbalance3a"></span>
<img src="07-clustering-figures/sipu-unbalance3a-1.png" alt="" />
<p class="caption">Figure 7.17:  Results of K-means on the <code>sipu_unbalance</code> dataset</p>
</div>
<p>This is far from what we expected.
The total within-cluster distances are equal to:</p>
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb718-1"><a href="clustering.html#cb718-1" aria-hidden="true"></a>km<span class="op">$</span>tot.withinss</span></code></pre></div>
<pre><code>## [1] 21713</code></pre>
<p>Increasing the number of restarts even further improves the solution,
but the local minimum is still far from the global one,
compare Figure <a href="clustering.html#fig:sipu-unbalance3b">7.18</a>.</p>
<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb720-1"><a href="clustering.html#cb720-1" aria-hidden="true"></a>km &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(<span class="kw">kmeans</span>(unbalance, <span class="dv">8</span>, <span class="dt">nstart=</span><span class="dv">1000</span>, <span class="dt">iter.max=</span><span class="dv">1000</span>))</span>
<span id="cb720-2"><a href="clustering.html#cb720-2" aria-hidden="true"></a><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>km<span class="op">$</span>cluster)</span></code></pre></div>
<div class="figure"><span id="fig:sipu-unbalance3b"></span>
<img src="07-clustering-figures/sipu-unbalance3b-1.png" alt="" />
<p class="caption">Figure 7.18:  Results of K-means on the <code>sipu_unbalance</code> dataset – many more restarts</p>
</div>
<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb721-1"><a href="clustering.html#cb721-1" aria-hidden="true"></a>km<span class="op">$</span>tot.withinss</span></code></pre></div>
<pre><code>## [1] 4378</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the K-means algorithm starting from a “good” initial guess
on the true cluster centres.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Clustering is – in its essence – an unsupervised learning method,
so what we’re going to do now could be called, let’s be blunt about it, cheating.
Luckily, we have an oracle at our disposal – it has provided us
with the following educated guesses (by looking at the scatter plot)
about the localisation of the cluster centres:</p>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb723-1"><a href="clustering.html#cb723-1" aria-hidden="true"></a>cntr &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>, <span class="kw">c</span>(</span>
<span id="cb723-2"><a href="clustering.html#cb723-2" aria-hidden="true"></a>   <span class="dv">-15</span>,   <span class="dv">5</span>,</span>
<span id="cb723-3"><a href="clustering.html#cb723-3" aria-hidden="true"></a>   <span class="dv">-12</span>,   <span class="dv">10</span>,</span>
<span id="cb723-4"><a href="clustering.html#cb723-4" aria-hidden="true"></a>   <span class="dv">-10</span>,   <span class="dv">5</span>,</span>
<span id="cb723-5"><a href="clustering.html#cb723-5" aria-hidden="true"></a>    <span class="dv">15</span>,   <span class="dv">0</span>,</span>
<span id="cb723-6"><a href="clustering.html#cb723-6" aria-hidden="true"></a>    <span class="dv">15</span>,   <span class="dv">10</span>,</span>
<span id="cb723-7"><a href="clustering.html#cb723-7" aria-hidden="true"></a>    <span class="dv">20</span>,   <span class="dv">5</span>,</span>
<span id="cb723-8"><a href="clustering.html#cb723-8" aria-hidden="true"></a>    <span class="dv">25</span>,   <span class="dv">0</span>,</span>
<span id="cb723-9"><a href="clustering.html#cb723-9" aria-hidden="true"></a>    <span class="dv">25</span>,   <span class="dv">10</span>))</span></code></pre></div>
<p>Running <code>kmeans()</code> yields the clustering depicted in Figure <a href="clustering.html#fig:sipu-unbalance6">7.19</a>.</p>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb724-1"><a href="clustering.html#cb724-1" aria-hidden="true"></a>km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(unbalance, cntr)</span>
<span id="cb724-2"><a href="clustering.html#cb724-2" aria-hidden="true"></a><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>km<span class="op">$</span>cluster)</span></code></pre></div>
<div class="figure"><span id="fig:sipu-unbalance6"></span>
<img src="07-clustering-figures/sipu-unbalance6-1.png" alt="" />
<p class="caption">Figure 7.19:  Results of K-means on the <code>sipu_unbalance</code> dataset – an educated guess on the cluster centres’ locations</p>
</div>
<p>The total within-cluster distances are now equal to:</p>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="clustering.html#cb725-1" aria-hidden="true"></a>km<span class="op">$</span>tot.withinss</span></code></pre></div>
<pre><code>## [1] 2144.9</code></pre>
<p>This is finally the globally optimal solution to the K-means problem
we were asked to solve. Recall that the algorithms implemented in the <code>kmeans()</code>
function are just fast heuristics that are supposed to find
local optima of the K-means objective function, which is given
by the within-cluster sum of squared Euclidean distances.</p>
<!--
http://cs.joensuu.fi/sipu/datasets/:

cntr <- matrix(ncol=2, byrow=TRUE, c(
    209948,   349963,
    539379,   299653,
    440134,   400135,
    440754,   298283,
    491036,   349798,
    150007,   350104,
    538884,   400947,
    179955,   380008))
-->
</details>
</div>
<div id="clustering-of-typical-2d-benchmark-datasets" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Clustering of Typical 2D Benchmark Datasets</h3>
<p>Let us consider a few clustering benchmark datasets
available at <a href="https://github.com/gagolews/clustering_benchmarks_v1" class="uri">https://github.com/gagolews/clustering_benchmarks_v1</a>
and <a href="http://cs.joensuu.fi/sipu/datasets/" class="uri">http://cs.joensuu.fi/sipu/datasets/</a>.
Here is a list of file names together with the
corresponding numbers of clusters (as given by datasets’ authors):</p>
<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb727-1"><a href="clustering.html#cb727-1" aria-hidden="true"></a>files &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;datasets/wut_isolation.csv&quot;</span>,</span>
<span id="cb727-2"><a href="clustering.html#cb727-2" aria-hidden="true"></a>           <span class="st">&quot;datasets/wut_mk2.csv&quot;</span>,</span>
<span id="cb727-3"><a href="clustering.html#cb727-3" aria-hidden="true"></a>           <span class="st">&quot;datasets/wut_z3.csv&quot;</span>,</span>
<span id="cb727-4"><a href="clustering.html#cb727-4" aria-hidden="true"></a>           <span class="st">&quot;datasets/sipu_aggregation.csv&quot;</span>,</span>
<span id="cb727-5"><a href="clustering.html#cb727-5" aria-hidden="true"></a>           <span class="st">&quot;datasets/sipu_pathbased.csv&quot;</span>,</span>
<span id="cb727-6"><a href="clustering.html#cb727-6" aria-hidden="true"></a>           <span class="st">&quot;datasets/sipu_unbalance.csv&quot;</span>)</span>
<span id="cb727-7"><a href="clustering.html#cb727-7" aria-hidden="true"></a>Ks &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">8</span>)</span></code></pre></div>
<p>All the datasets are two-dimensional, hence we’ll be able to visualise
the obtained results and assess the sensibility of the obtained clusterings.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the K-means, the single, average and complete linkage
and the Genie algorithm on the aforementioned datasets and discuss the results.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Apart from a call to the Genie algorithm with the default parameters,
we will also look at the results it generates when we set
<code>giniThreshold</code> of <code>0.5</code>.</p>
<p>The following function is our workhorse that will perform all the computations
and will draw all the figures for a single dataset:</p>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb728-1"><a href="clustering.html#cb728-1" aria-hidden="true"></a>clusterise &lt;-<span class="st"> </span><span class="cf">function</span>(file, K) {</span>
<span id="cb728-2"><a href="clustering.html#cb728-2" aria-hidden="true"></a>    X &lt;-<span class="st"> </span><span class="kw">read.csv</span>(file,</span>
<span id="cb728-3"><a href="clustering.html#cb728-3" aria-hidden="true"></a>        <span class="dt">header=</span><span class="ot">FALSE</span>, <span class="dt">sep=</span><span class="st">&quot; &quot;</span>, <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</span>
<span id="cb728-4"><a href="clustering.html#cb728-4" aria-hidden="true"></a>    d &lt;-<span class="st"> </span><span class="kw">dist</span>(X)</span>
<span id="cb728-5"><a href="clustering.html#cb728-5" aria-hidden="true"></a>    <span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb728-6"><a href="clustering.html#cb728-6" aria-hidden="true"></a>    <span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">2</span>, <span class="fl">0.5</span>))</span>
<span id="cb728-7"><a href="clustering.html#cb728-7" aria-hidden="true"></a></span>
<span id="cb728-8"><a href="clustering.html#cb728-8" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, K, <span class="dt">nstart=</span><span class="dv">10</span>)<span class="op">$</span>cluster</span>
<span id="cb728-9"><a href="clustering.html#cb728-9" aria-hidden="true"></a>    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</span>
<span id="cb728-10"><a href="clustering.html#cb728-10" aria-hidden="true"></a>    <span class="kw">mtext</span>(<span class="st">&quot;K-means&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</span>
<span id="cb728-11"><a href="clustering.html#cb728-11" aria-hidden="true"></a></span>
<span id="cb728-12"><a href="clustering.html#cb728-12" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(<span class="kw">hclust</span>(d, <span class="st">&quot;complete&quot;</span>), K)</span>
<span id="cb728-13"><a href="clustering.html#cb728-13" aria-hidden="true"></a>    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</span>
<span id="cb728-14"><a href="clustering.html#cb728-14" aria-hidden="true"></a>    <span class="kw">mtext</span>(<span class="st">&quot;Complete Linkage&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</span>
<span id="cb728-15"><a href="clustering.html#cb728-15" aria-hidden="true"></a></span>
<span id="cb728-16"><a href="clustering.html#cb728-16" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(<span class="kw">hclust</span>(d, <span class="st">&quot;average&quot;</span>), K)</span>
<span id="cb728-17"><a href="clustering.html#cb728-17" aria-hidden="true"></a>    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</span>
<span id="cb728-18"><a href="clustering.html#cb728-18" aria-hidden="true"></a>    <span class="kw">mtext</span>(<span class="st">&quot;Average Linkage&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</span>
<span id="cb728-19"><a href="clustering.html#cb728-19" aria-hidden="true"></a></span>
<span id="cb728-20"><a href="clustering.html#cb728-20" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(<span class="kw">hclust</span>(d, <span class="st">&quot;single&quot;</span>), K)</span>
<span id="cb728-21"><a href="clustering.html#cb728-21" aria-hidden="true"></a>    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</span>
<span id="cb728-22"><a href="clustering.html#cb728-22" aria-hidden="true"></a>    <span class="kw">mtext</span>(<span class="st">&quot;Single Linkage&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</span>
<span id="cb728-23"><a href="clustering.html#cb728-23" aria-hidden="true"></a></span>
<span id="cb728-24"><a href="clustering.html#cb728-24" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(genieclust<span class="op">::</span><span class="kw">gclust</span>(d), K) <span class="co"># gini_threshold=0.3</span></span>
<span id="cb728-25"><a href="clustering.html#cb728-25" aria-hidden="true"></a>    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</span>
<span id="cb728-26"><a href="clustering.html#cb728-26" aria-hidden="true"></a>    <span class="kw">mtext</span>(<span class="st">&quot;Genie (default)&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</span>
<span id="cb728-27"><a href="clustering.html#cb728-27" aria-hidden="true"></a></span>
<span id="cb728-28"><a href="clustering.html#cb728-28" aria-hidden="true"></a>    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(genieclust<span class="op">::</span><span class="kw">gclust</span>(d, <span class="dt">gini_threshold=</span><span class="fl">0.5</span>), K)</span>
<span id="cb728-29"><a href="clustering.html#cb728-29" aria-hidden="true"></a>    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</span>
<span id="cb728-30"><a href="clustering.html#cb728-30" aria-hidden="true"></a>    <span class="kw">mtext</span>(<span class="st">&quot;Genie (g=0.5)&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</span>
<span id="cb728-31"><a href="clustering.html#cb728-31" aria-hidden="true"></a>}</span></code></pre></div>
<p>Applying the above as <code>clusterise(files[i], Ks[i])</code> yields
Figures <a href="clustering.html#fig:clustering-benchmarks-plot1">7.20</a>-<a href="clustering.html#fig:clustering-benchmarks-plot6">7.25</a>.</p>
<div class="figure"><span id="fig:clustering-benchmarks-plot1"></span>
<img src="07-clustering-figures/clustering-benchmarks-plot1-1.png" alt="" />
<p class="caption">Figure 7.20:  Clustering of the <code>wut_isolation</code> dataset</p>
</div>
<div class="figure"><span id="fig:clustering-benchmarks-plot2"></span>
<img src="07-clustering-figures/clustering-benchmarks-plot2-1.png" alt="" />
<p class="caption">Figure 7.21:  Clustering of the <code>wut_mk2</code> dataset</p>
</div>
<div class="figure"><span id="fig:clustering-benchmarks-plot3"></span>
<img src="07-clustering-figures/clustering-benchmarks-plot3-1.png" alt="" />
<p class="caption">Figure 7.22:  Clustering of the <code>wut_z3</code> dataset</p>
</div>
<div class="figure"><span id="fig:clustering-benchmarks-plot4"></span>
<img src="07-clustering-figures/clustering-benchmarks-plot4-1.png" alt="" />
<p class="caption">Figure 7.23:  Clustering of the <code>sipu_aggregation</code> dataset</p>
</div>
<div class="figure"><span id="fig:clustering-benchmarks-plot5"></span>
<img src="07-clustering-figures/clustering-benchmarks-plot5-1.png" alt="" />
<p class="caption">Figure 7.24:  Clustering of the <code>sipu_pathbased</code> dataset</p>
</div>
<div class="figure"><span id="fig:clustering-benchmarks-plot6"></span>
<img src="07-clustering-figures/clustering-benchmarks-plot6-1.png" alt="" />
<p class="caption">Figure 7.25:  Clustering of the <code>sipu_unbalance</code> dataset</p>
</div>
<p>Note that, by definition, K-means is only able to detect clusters
of convex shapes. The Genie algorithm, on the other hand, might
fail to detect clusters of very small sizes amongst the more populous ones.
Single linkage is very sensitive to outliers in data – it often outputs
clusters of cardinality 1.</p>
</details>
</div>
</div>
<div id="outro-6" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Outro</h2>
<div id="remarks-6" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Remarks</h3>
<p>Unsupervised learning is often performed during the data pre-processing
and exploration stage.
Assessing the quality of clustering is particularly challenging as,
unlike in a supervised setting,
we have no access to “ground truth” information.</p>
<p>In practice, we often apply different clustering algorithms
and just see where they lead us. There’s no teacher that would
tell us what we should do, so whatever we do is awesome, right?
Well, not precisely. Most frequently, you, my dear reader, will work
for some party that’s genuinely
interested in your explaining why did you spent the last month coming up
with nothing useful at all. Thus, the main body of work related to proving
the use-ful<em>l</em>/less-ness will be on you.</p>
<p>Clustering methods can aid us in supervised tasks
– instead of fitting a single “large model”, it might be
useful to fit separate models to each cluster.</p>
<div style="margin-top: 1em">

</div>
<p>To sum up, the aim of K-means is to find
<span class="math inline">\(K\)</span> clusters based on the notion of the points’ closeness
to the cluster centres. Remember that <span class="math inline">\(K\)</span> must be set in advance.
By definition (* via its relation to Voronoi diagrams),
all clusters will be of convex shapes.</p>
<p>However, we may try applying <span class="math inline">\(K&#39;\)</span>-means for <span class="math inline">\(K&#39; \gg K\)</span>
to obtain a “fine grained” compressed representation of data and then
combine the (sub)clusters into more meaningful groups
using other methods (such as the hierarchical ones).</p>
<p>Iterative K-means algorithms are very fast (e.g., a mini-batch
version of the algorithm can be implement to speed up the optimisation
process) even for large data sets,
but they may fail to find a desirable solution, especially if clusters
are unbalanced.</p>
<div style="margin-top: 1em">

</div>
<p>Hierarchical methods, on the other hand, output
a whole family of mutually nested partitions, which may provide us
with insight into the underlying structure of data data.
Unfortunately, there is no easy way to assign new points
to existing clusters; yet, you can always build a classifier
(e.g., a decision tree or a neural network) that learns
the discovered labels.</p>
<p>A linkage scheme must be chosen with care, for instance, single linkage
can be sensitive to outliers. However, it is generally the fastest.
The methods implemented in <code>hclust()</code> are generally slow; they have
time complexity between <span class="math inline">\(O(n^2)\)</span> and <span class="math inline">\(O(n^3)\)</span>.</p>
<dl>
<dt>Remark.</dt>
<dd><p>Note that the <code>fastcluster</code> package provides a more efficient
and memory-saving
implementation of some methods available via a call to <code>hclust()</code>.
See also the <code>genieclust</code> package for a super-robust version of the single linkage
algorithm based on the datasets’s Euclidean minimum spanning tree,
which can be computed quite quickly.</p>
</dd>
</dl>
<p>Finally, note that all the discussed clustering methods are based on the
notion of pairwise distances. These of course tend
to behave weirdly in high-dimensional
spaces (“the curse of dimensionality”). Moreover, some hardcore
feature engineering might be needed to obtain meaningful results.</p>
</div>
<div id="further-reading-6" class="section level3" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(James et al. <a href="#ref-islr" role="doc-biblioref">2017</a>: Section 10.3)</span></p>
<p>Other: <span class="citation">(Hastie et al. <a href="#ref-esl" role="doc-biblioref">2017</a>: Section 14.3)</span></p>
<p>Additionally, check out other noteworthy clustering approaches:</p>
<ul>
<li>Genie (see R package <code>genieclust</code>) <span class="citation">(Gagolewski et al. <a href="#ref-genie" role="doc-biblioref">2016</a>, Cena &amp; Gagolewski <a href="#ref-genieowa" role="doc-biblioref">2020</a>)</span></li>
<li>ITM <span class="citation">(Müller et al. <a href="#ref-itm" role="doc-biblioref">2012</a>)</span></li>
<li>DBSCAN, HDBSCAN* <span class="citation">(Ling <a href="#ref-pre_dbscan" role="doc-biblioref">1973</a>, Ester et al. <a href="#ref-dbscan" role="doc-biblioref">1996</a>, Campello et al. <a href="#ref-hdbscan" role="doc-biblioref">2015</a>)</span></li>
<li>K-medoids, K-medians</li>
<li>Fuzzy C-means (a.k.a. weighted K-means) <span class="citation">(Bezdek et al. <a href="#ref-cmeans" role="doc-biblioref">1984</a>)</span></li>
<li>Spectral clustering; e.g., <span class="citation">(Ng et al. <a href="#ref-spectral_nips" role="doc-biblioref">2001</a>)</span></li>
<li>BIRCH <span class="citation">(Zhang et al. <a href="#ref-birch" role="doc-biblioref">1996</a>)</span></li>
</ul>

<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-cmeans">
<p>Bezdek JC, Ehrlich R, Full W (1984) FCM: The fuzzy c-means clustering algorithm. <em>Computer and Geosciences</em> 10, 191–203.</p>
</div>
<div id="ref-hdbscan">
<p>Campello RJGB, Moulavi D, Zimek A, Sander J (2015) Hierarchical density estimates for data clustering, visualization, and outlier detection. <em>ACM Transactions on Knowledge Discovery from Data</em> 10, 5:1–5:51.</p>
</div>
<div id="ref-genieowa">
<p>Cena A, Gagolewski M (2020) Genie+OWA: Robustifying hierarchical clustering with OWA-based linkages. <em>Information Sciences</em> 520, 324–336.</p>
</div>
<div id="ref-dbscan">
<p>Ester M, Kriegel H-P, Sander J, Xu X (1996) A density-based algorithm for discovering clusters in large spatial databases with noise <em>Proc. KDD’96</em>, pp. 226–231.</p>
</div>
<div id="ref-genie">
<p>Gagolewski M, Bartoszuk M, Cena A (2016) Genie: A new, fast, and outlier-resistant hierarchical clustering algorithm. <em>Information Sciences</em> 363, 8–23.</p>
</div>
<div id="ref-esl">
<p>Hastie T, Tibshirani R, Friedman J (2017) <em>The elements of statistical learning</em>. Springer-Verlag <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-comparing_paritions">
<p>Hubert L, Arabie P (1985) Comparing partitions. <em>Journal of Classification</em> 2, 193–218.</p>
</div>
<div id="ref-islr">
<p>James G, Witten D, Hastie T, Tibshirani R (2017) <em>An introduction to statistical learning with applications in R</em>. Springer-Verlag <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.</p>
</div>
<div id="ref-pre_dbscan">
<p>Ling RF (1973) A probability theory of cluster analysis. <em>Journal of the American Statistical Association</em> 68, 159–164.</p>
</div>
<div id="ref-itm">
<p>Müller AC, Nowozin S, Lampert CH (2012) Information theoretic clustering using minimum spanning trees <em>Proc. German conference on pattern recognition</em>, <a href="https://github.com/amueller/information-theoretic-mst">https://github.com/amueller/information-theoretic-mst</a>.</p>
</div>
<div id="ref-spectral_nips">
<p>Ng AY, Jordan MI, Weiss Y (2001) On spectral clustering: Analysis and an algorithm <em>Proc. Advances in neural information processing systems 14 (NIPS’01)</em>, <a href="https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf">https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf</a>.</p>
</div>
<div id="ref-external_cluster_validity">
<p>Rezaei M, Fränti P (2016) Set-matching measures for external cluster validity. <em>IEEE Transactions on Knowledge and Data Engineering</em> 28, 2173–2186.</p>
</div>
<div id="ref-birch">
<p>Zhang T, Ramakrishnan R, Livny M (1996) BIRCH: An efficient data clustering method for large databases <em>Proc. ACM SIGMOD international conference on management of data – SIGMOD’96</em>, pp. 103–114.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="continuous-optimisation-with-iterative-algorithms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimisation-with-genetic-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
