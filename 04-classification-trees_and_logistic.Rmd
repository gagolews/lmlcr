<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020-2021, Marek Gagolewski <https://www.gagolewski.com>
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->

# Classification with Trees and Linear Models

```{r chapter-header-motd,echo=FALSE,results="asis"}
cat(readLines("chapter-header-motd.md"), sep="\n")
```


## Introduction

### Classification Task


Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be an input matrix
that consists of $n$ points in a $p$-dimensional space (each of the $n$ objects
is described by means of $p$ numerical features)

Recall that in supervised learning, with each
$\mathbf{x}_{i,\cdot}$ we associate the desired output $y_i$.

Hence, our dataset is $[\mathbf{X}\ \mathbf{y}]$ --
where each object is represented as a row vector
$[\mathbf{x}_{i,\cdot}\ y_i]$, $i=1,\dots,n$:

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right].
\]


. . .



In this chapter we are still  interested in  **classification** tasks;
we assume that each $y_i$ is a descriptive label.





Let's assume that we are faced with **binary classification** tasks.

Hence, there are only two possible labels that we traditionally denote with $0$s and $1$s.

For example:

0       | 1
--------|------------
no      | yes
false   | true
failure | success
healthy | ill







Let's recall the synthetic 2D dataset from the previous chapter
(true decision boundary is at $X_1=0$), see Figure \@ref(fig:synthetic).


```{r synthetic,echo=FALSE,fig.cap="A synthetic 2D dataset with the true decision boundary at $X_1=0$"}
set.seed(123)
n0 <- 50 # n0 points in class 0
n1 <- 50 # n0 points in class 0
Xs <- rbind(
    cbind(rnorm(n0, -1, 1), rnorm(n0, 0, 1)), # N( (-1, 0), (1, 1) )
    cbind(rnorm(n1, +1, 1), rnorm(n1, 0, 1))  # N( (+1, 0), (1, 1) )
)
Ys <- factor(rep(c("0", "1"), c(n0, n1)))

plot(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys], xlab="X1", ylab="X2", asp=1)
legend("topleft", col=c(1,2), pch=c(1,2), legend=c("Class 0", "Class 1"), bg="white")
abline(v=0, lty=3, col="blue")
```








### Data




For illustration, we'll be considering the Wine Quality dataset
(white wines only):


```{r load1,cache=TRUE}
wines <- read.csv("datasets/winequality-all.csv",
    comment.char="#")
wines <- wines[wines$color == "white",]
(n <- nrow(wines)) # number of samples
```




The input matrix $\mathbf{X}\in\mathbb{R}^{n\times p}$
consists of the first 10 numeric variables:

```{r load2,dependson='load1',cache=TRUE}
X <- as.matrix(wines[,1:10])
dim(X)
head(X, 2) # first two rows
```







The 11th variable measures the amount of alcohol (in %).

We will convert this dependent variable to a binary one:

- 0 == (`alcohol  < 12`) == lower-alcohol wines
- 1 == (`alcohol >= 12`) == higher-alcohol wines

```{r load3,dependson='load2',cache=TRUE}
# recall that TRUE == 1
Y <- factor(as.character(as.numeric(wines$alcohol >= 12)))
table(Y)
```






60/40% train-test split:

```{r load4,dependson='load3',cache=TRUE}
set.seed(123) # reproducibility matters
random_indices <- sample(n)
head(random_indices) # preview
# first 60% of the indices (they are arranged randomly)
# will constitute the train sample:
train_indices <- random_indices[1:floor(n*0.6)]
X_train <- X[train_indices,]
Y_train <- Y[train_indices]
# the remaining indices (40%) go to the test sample:
X_test  <- X[-train_indices,]
Y_test  <- Y[-train_indices]
```






Let's also compute `Z_train` and `Z_test`, being the standardised versions of `X_train`
and `X_test`, respectively.

```{r load5,dependson='load4',cache=TRUE}
means <- apply(X_train, 2, mean) # column means
sds   <- apply(X_train, 2, sd)   # column standard deviations
Z_train <- t(apply(X_train, 1, function(r) (r-means)/sds))
Z_test  <- t(apply(X_test,  1, function(r) (r-means)/sds))
```







```{r loaded,dependson='load5',cache=TRUE}
get_metrics <- function(Y_pred, Y_test)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```






Let's go back to the K-NN algorithm.

```{r fnn1}
library("FNN")
Y_knn5   <- knn(X_train, X_test, Y_train, k=5)
Y_knn9   <- knn(X_train, X_test, Y_train, k=9)
Y_knn5s  <- knn(Z_train, Z_test, Y_train, k=5)
Y_knn9s  <- knn(Z_train, Z_test, Y_train, k=9)
```

Recall the quality metrics we have obtained previously (as a point of reference):

```{r fnn2}
cbind(
    Knn5=get_metrics(Y_knn5, Y_test),
    Knn9=get_metrics(Y_knn9, Y_test),
    Knn5s=get_metrics(Y_knn5s, Y_test),
    Knn9s=get_metrics(Y_knn9s, Y_test)
)
```


In this chapter we discuss the following simple and educational
(yet practically useful)
classification algorithms:

- *decision trees*,
- *binary logistic regression*.




## Decision Trees

### Introduction


Note that a K-NN classifier discussed in the previous chapter
is **model-free**.
The whole training set must be stored and referred to at all times.

Therefore, it doesn't *explain* the data we have -- we may use it solely
for the purpose of *prediction*.


Perhaps one of the most interpretable (and hence human-friendly) models
consist of decision rules of the form:

**IF $x_{i,j_1}\le v_1$ AND ... AND $x_{i,j_r}\le v_r$ THEN $\hat{y}_i=1$.**

These can be organised into a **hierarchy** for greater readability.

This idea inspired the notion of **decision trees** [@cart].






```{r plot_rpart,echo=FALSE,fig.cap="The simplest decision tree for the synthetic 2D dataset and the corresponding decision boundaries"}
library("rpart")
library("rpart.plot")
set.seed(123)



plot_rpart <- function(ts, XYs) {
  xx1 <- seq(-4, 4, length.out=250)
  xx2 <- seq(-4, 4, length.out=250)
  xx <- expand.grid(xx1, xx2)

  dimnames(xx)[[2]] <- names(XYs)[1:2]

  yy <- predict(ts, data.frame(xx, names=c("V1", "V2")), type="class")
  image(xx1, xx2, matrix(as.numeric(yy)-1, nrow=length(xx1), ncol=length(xx2)),
        col=c("#00000044", "#ff000044"), xlab="X1", ylab="X2", asp=1)


  points(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys])
  legend("topleft", col=c(1,2), pch=c(1,2), legend=c("Class 0", "Class 1"), bg="white")
  abline(v=0, lty=3, col="blue")
}


XYs <- as.data.frame(cbind(Xs, Y=as.numeric(as.character(Ys))))
names(XYs) <- c("X1", "X2", "Y")
cp <- 0.5
ts <- rpart(Y~., data=XYs, method="class", cp=cp)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
par(mar=c(0,0,0,0))
par(ann=FALSE)
rpart.plot(ts)
```


Figure \@ref(fig:plot-rpart2) depicts a very simple decision tree
for the aforementioned synthetic dataset.
There is only one decision boundary (based on $X_1$) that splits
data into the "left" and "right" sides.
Each tree node reports 3 pieces of information:

- dominating class (0 or 1)
- (relative) proportion of 1s  represented in a node
- (absolute) proportion of all observations in a node




Figures \@ref(fig:plot-rpart2) and \@ref(fig:plot-rpart3) depict
trees with more decision rules.
Take a moment to contemplate how the corresponding decision boundaries
changed with the introduction of new decision rules.


```{r plot-rpart2,echo=FALSE,fig.cap="A more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries"}
cp <- 0.01
ts <- rpart(Y~., data=XYs, method="class", cp=cp)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
par(mar=c(0,0,0,0))
par(ann=FALSE)
rpart.plot(ts)
```




```{r plot-rpart3,echo=FALSE,fig.cap="An even more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries"}
cp <- 0.001
control <- list(minsplit=1, minbucket=5)
ts <- rpart(Y~., data=XYs, method="class", cp=cp, control=control)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
par(mar=c(0,0,0,0))
par(ann=FALSE)
rpart.plot(ts, tweak=1.05)
```






### Example in R






We will use the `rpart()` function from the `rpart` package
to build a classification tree.

```{r rpart_load}
library("rpart")
library("rpart.plot")
set.seed(123)
```

`rpart()` uses a formula (`~`) interface, hence it will be easier
to feed it with data in a data.frame form.

```{r xy_data}
XY_train <- cbind(as.data.frame(X_train), Y=Y_train)
XY_test <- cbind(as.data.frame(X_test), Y=Y_test)
```





Fit and plot a decision tree, see Figure \@ref(fig:plot-rpart1).

```{r plot-rpart1,fig.height=5,echo=-(1:2),fig.cap="A decision tree for the `wines` dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
t1 <- rpart(Y~., data=XY_train, method="class")
rpart.plot(t1, tweak=1.1, fallen.leaves=FALSE, digits=3)
```


We can build  less or more complex trees by playing
with the `cp` parameter, see Figures \@ref(fig:plot-rpart222)
and \@ref(fig:tree333).

```{r plot-rpart222,echo=-(1:2),fig.cap="A (simpler) decision tree for the `wines` dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
# cp = complexity parameter, smaller → more complex tree
t2 <- rpart(Y~., data=XY_train, method="class", cp=0.1)
rpart.plot(t2, tweak=1.1, fallen.leaves=FALSE, digits=3)
```


```{r tree333,fig.height=8,echo=-(1:2),fig.cap="A (more complex) decision tree for the `wines` dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
# cp = complexity parameter, smaller → more complex tree
t3 <- rpart(Y~., data=XY_train, method="class", cp=0.00001)
rpart.plot(t3, tweak=1.1, fallen.leaves=FALSE, digits=3)
```

Trees with few decision rules actually are very nicely interpretable.
On the other hand, plotting of the complex ones is just hopeless;
we should treat them as "black boxes" instead.



<!--
The fitted model is rather... simple.

Only the `alcohol` variable is taken into account.


Well note how these two distributions are shifted:

```{r,echo=-1}
#vioplot::vioplot(alcohol~Y, data=XY_train,
#    horizontal=TRUE)
```





-->

Let's make some predictions:

```{r predict_rpart1}
Y_pred <- predict(t1, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```



```{r predict_rpart222}
Y_pred <- predict(t2, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```


```{r tree333pred}
Y_pred <- predict(t3, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```


Remark.

: (\*) Interestingly, `rpart()` also provides us with information
about the importance degrees of each independent variable.

```{r importances}
t1$variable.importance/sum(t1$variable.importance)
```

















### A Note on Decision Tree Learning




Learning an optimal decision tree is a computationally hard problem
-- we need some heuristics.

Examples:

* ID3 (Iterative Dichotomiser 3) [@id3]
* C4.5 algorithm [@c45]
* CART by Leo Breiman et al., [@cart]

(\*\*) Decision trees are most often constructed by a *greedy*, *top-down*
*recursive partitioning*, see., e.g., [@rpart].






<!--

## TODO Random Forests and XGBoost

## ...





just show how to use it

boosting

bagging

```{r}
#library("randomForest")
#rf <- randomForest(X_train, Y_train)
#Y_pred <- predict(rf, X_test)
#get_metrics(Y_pred, Y_test)
```






```{r}
#library("xgboost")
#xg <- xgboost(X_train, Y_train)
#Y_pred <- predict(xg, X_test)
#get_metrics(Y_pred, Y_test)
```

-->






## Binary Logistic Regression

### Motivation





Recall that for a regression task, we fitted a very simple family of models
-- the linear ones -- by minimising the sum of squared residuals.

This approach was pretty effective.

(Very) theoretically, we could treat the class labels as numeric $0$s and $1$s
and apply regression models in a binary classification task.



```{r lm1,dependson='loaded',echo=-1,cache=TRUE}
set.seed(123)
XY_train_r <- cbind(as.data.frame(X_train),
    Y=as.numeric(Y_train)-1 # 0.0 or 1.0
)
XY_test_r <- cbind(as.data.frame(X_test),
    Y=as.numeric(Y_test)-1 # 0.0 or 1.0
)
f_r <- lm(Y~density+residual.sugar+pH, data=XY_train_r)
```





```{r lm2,dependson='lm1',cache=TRUE}
Y_pred_r <- predict(f_r, XY_test_r)
summary(Y_pred_r)
```

The predicted outputs, $\hat{Y}$, are arbitrary real numbers,
but we can convert them to binary ones by checking if, e.g., $\hat{Y}>0.5$.

```{r lm3,dependson='lm2',cache=TRUE}
Y_pred <- as.numeric(Y_pred_r>0.5)
round(get_metrics(Y_pred, XY_test_r$Y), 3)
```



Remark.

: (\*) The threshold $T=0.5$  could even be treated as a free parameter
we optimise for (w.r.t. different metrics over the validation sample),
see Figure \@ref(fig:lm3b).

```{r lm3b,echo=FALSE,cache=TRUE,dependson='lm3',cache=TRUE,fig.cap="Quality metrics for a binary classifier \"Classify X as 1 if $f(X)>T$ and as 0 if $f(X)\\le T$\""}
set.seed(123)
f_r <- lm(Y~density+residual.sugar+pH, data=XY_train_r)
Y_pred_r <- predict(f_r, XY_test_r)
Ts <- seq(0.1, 0.85, by=0.05)
Ps <- as.data.frame(t(sapply(Ts, function(t) {
  Y_pred <- as.numeric(Y_pred_r>t)
  get_metrics(factor(Y_pred, levels=c("0", "1")), factor(XY_test_r$Y, levels=c("0", "1")))
})))

matplot(Ts, Ps[,1:4], xlab="T", ylab="Metric", type="l", ylim=c(0,1))
legend("bottomleft", legend=names(Ps[,1:4]), col=1:4, lty=1:4, ncol=2, bg="white")
```


Despite we can, we shouldn't use linear regression for classification.
Treating class labels  "0" and "1" as ordinary real numbers just doesn't
cut it -- we intuitively feel that we are doing something *ugly*.
Luckily, there is a better, more meaningful approach that
still relies on a linear model, but has the *right* semantics.






### Logistic Model




Inspired by this idea, we could try modelling
the ***probability* that a given point belongs to class $1$**.

This could also provide us with the *confidence* in our prediction.

Probability is a number in $[0,1]$, but the outputs of a linear model are arbitrary real numbers.

However, we could transform those real-valued outputs by means
of some function $\phi:\mathbb{R}\to[0,1]$ (preferably S-shaped == sigmoid),
so as to get:

\[
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=\phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p).
\]


Remark.

: The above reads as "Probability that $Y$ is from class 1 given $\mathbf{X}$
and $\boldsymbol\beta$".




A popular choice is the **logistic sigmoid function**,
see Figure \@ref(fig:sigmoid):

\[
\phi(t) = \frac{1}{1+e^{-t}} = \frac{e^t}{1+e^t}.
\]

```{r sigmoid,echo=FALSE,cache=TRUE,fig.cap="The logistic sigmoid function, $\\varphi$"}
xxx <- seq(-3, 3, length.out=101)
yyy <- 1/(1+exp(-xxx))
plot(xxx, yyy, type='l', xlab='y', ylab=expression(phi(y)), ylim=c(0,1))
abline(v=0, lty=3)
abline(h=c(0, 0.5, 1), lty=3)
```





Hence our model becomes:

\[
Y=\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
\]

It is an instance of a **generalised linear model** (glm)
(there are of course many other possible generalisations).


### Example in R




Let us first fit a simple  (i.e., $p=1$) logistic regression model
using the `density` variable. The goodness-of-fit measure used in this
problem will be discussed a bit later.



```{r glm1}
(f <- glm(Y~density, data=XY_train, family=binomial("logit")))
```

"logit" above denotes the inverse of the logistic sigmoid function.
The fitted coefficients are equal to:


```{r glm1b}
f$coefficients
```

Figure \@ref(fig:glm2) depicts the obtained model, which can be written
as:
\[
\Pr(Y=1|x)=\displaystyle\frac{1}{1+e^{-\left(
`r sprintf("%g%+gx", f$coef[1], f$coef[2])`
\right)}
}
\]
with $x=\text{density}$.


```{r glm2,echo=FALSE,cache=TRUE,eval=TRUE,fig.cap="The probability that a given wine is a high-alcohol one given its density; black and red points denote the actual observed data points from the class 0 and 1, respectively"}
xxx <- seq(min(XY_train$density), max(XY_train$density), length.out=101)
yyy <- 1/(1+exp(-(f$coef[1]+f$coef[2]*xxx)))
plot(xxx, yyy, type='l', xlab='density', ylab="P(Y=1|density)", ylim=c(0,1))
points(XY_train$density, jitter(as.numeric(XY_train$Y)-1, 0.1), col=
    c("#00000011", "#ff000011")[as.numeric(XY_train$Y)], pch=16)
abline(h=c(0, 0.5, 1), lty=3)
legend(8, 0.95, legend=sprintf("1/(1+exp(-(%+g%+g*density))", f$coef[1], f$coef[2]), lty=1, bg="white")
```




Some predicted probabilities:


```{r}
round(head(predict(f, XY_test, type="response"), 12), 2)
```

We classify $Y$ as 1 if the corresponding membership probability
is greater than $0.5$.

```{r}
Y_pred <- as.numeric(predict(f, XY_test, type="response")>0.5)
get_metrics(Y_pred, Y_test)
```




And now a fit based on some other input variables:

```{r}
(f <- glm(Y~density+residual.sugar+total.sulfur.dioxide,
    data=XY_train, family=binomial("logit")))
Y_pred <- as.numeric(predict(f, XY_test, type="response")>0.5)
get_metrics(Y_pred, Y_test)
```


{ BEGIN exercise }
Try fitting different models based on other sets of features.
{ END exercise }





### Loss Function: Cross-entropy




The fitting of the model can be written as an optimisation task:

\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\frac{1}{n} \sum_{i=1}^n
\epsilon\left(\hat{y}_i, y_i \right)
\]

where $\epsilon(\hat{y}_i, y_i)$ denotes the penalty that measures the
"difference" between the true $y_i$ and its predicted version
$\hat{y}_i=\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)$.

In the ordinary regression, we used the squared residual
$\epsilon(\hat{y}_i, y_i) = (\hat{y}_i-y_i)^2$.
In **logistic regression** (the kind of a classifier we are
interested in right now), we use
the **cross-entropy** (a.k.a. **log-loss**, binary cross-entropy),

\[
\epsilon(\hat{y}_i,y_i) = - \left(y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i)\right)
\]

The corresponding loss function has not only
many nice statistical properties (\*\* related to maximum likelihood
estimation etc.)
but also an intuitive interpretation.




Note that the predicted $\hat{y}_i$ is in $(0,1)$ and the true $y_i$
equals to either 0 or 1.
Recall also that $\log t\in(-\infty, 0)$ for $t\in (0,1)$.
Therefore, the formula for $\epsilon(\hat{y}_i,y_i)$
has a very intuitive behaviour:

- if true $y_i=1$, then the penalty becomes $\epsilon(\hat{y}_i, 1) = -\log(\hat{y}_i)$

    - $\hat{y}_i$ is the probability that the classified input is indeed from class $1$
    - we'd be happy if the classifier outputted $\hat{y}_i\simeq 1$ in this case;
    this is not  penalised as $-\log(t)\to 0$ as $t\to 1$
    - however, if the classifier is totally wrong, i.e., it thinks that
    $\hat{y}_i\simeq 0$, then the penalty will be very high, as $-\log(t)\to+\infty$
    as $t\to 0$


- if true $y_i=0$, then the penalty becomes $\epsilon(\hat{y}_i, 0) = -\log(1-\hat{y}_i)$

    - $1-\hat{y}_i$ is the predicted probability that the input is from class $0$
    - we penalise heavily the case where $1-\hat{y}_i$ is small (we'd be happy
    if the classifier was sure that $1-\hat{y}_i\simeq 1$, because this is the ground-truth)


. . .

(\*) Having said that, let's expand the above formulae.
The task of minimising cross-entropy in the binary logistic regression
can be written as $\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}} E(\boldsymbol\beta)$
with:

\[
E(\boldsymbol\beta)=
-\frac{1}{n} \sum_{i=1}^n
y_i \log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
+ (1-y_i) \log(1-\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta))
\]

Taking into account that:

\[
\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)=
\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}},
\]

we get:

\[
E(\boldsymbol\beta)=
-\frac{1}{n}
\sum_{i=1}^n \left(
\begin{array}{r}
y_i \log \displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}\\
+
(1-y_i) \log \displaystyle\frac{e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}
\end{array}
\right).
\]

Logarithms are really practitioner-friendly functions,
it holds:

* $\log 1=0$,
* $\log e=1$ (where $e \simeq 2.71828$ is the Euler constant;
note that by writing $\log$ we mean the natural a.k.a. base-$e$ logarithm),
* $\log xy = \log x + \log y$,
* $\log x^p = p\log x$ (this is $\log (x^p)$, not $(\log x)^p$).

These facts imply, amongst others that:

* $\log e^x = x \log e = x$,
* $\log \frac{x}{y} = \log x y^{-1} = \log x+\log y^{-1} = \log x - \log y$
(of course for $y\neq 0$),
* $\log \frac{1}{y} = -\log y$

and so forth. Therefore,
based on the fact that
$1/(1+e^{-x})=e^x/(1+e^x)$,
the above optimisation problem can be rewritten as:

\[
E(\boldsymbol\beta)=
\frac{1}{n}
\sum_{i=1}^n \left(
\begin{array}{r}
y_i \log \left(1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)\\
+
(1-y_i) \log \left(1+e^{+(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)
\end{array}
\right)
\]

or, if someone prefers:

\[
E(\boldsymbol\beta)=
\frac{1}{n}
\sum_{i=1}^n \left(
(1-y_i)\left(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)
+\log \left(1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)
\right).
\]

It turns out that there is no analytical formula
for the optimal set of parameters ($\beta_0,\beta_1,\dots,\beta_p$
minimising the log-loss).
In the chapter on optimisation, we shall see that
the solution to the logistic regression can be solved numerically
by means of quite simple iterative algorithms.
The two expanded formulae have lost the appealing interpretation
of the original one, however, it's more numerically well-behaving,
see, e.g., the `log1p()` function in base R or, even better,
`fermi_dirac_0()` in the `gsl` package.

```{r gsl,echo=FALSE,eval=FALSE}
library("gsl")
```

<!-- TODO: cite Goldberg's paper -->








## Exercises in R




### EdStats -- Preparing Data

In this exercise, we will prepare the EdStats dataset
for further analysis.
The file `edstats_2019.csv` provides us with many
country-level Education Statistics extracted from the World Bank's Databank,
see https://databank.worldbank.org/.
Databank aggregates information from such sources as
the UNESCO Institute for Statistics,
OECD Programme for International Student Assessment (PISA)
etc. The official description reads:

> "The World Bank EdStats Query holds around 2,500 internationally comparable
education indicators for access, progression, completion, literacy, teachers,
population, and expenditures. The indicators cover the education cycle from
pre-primary to tertiary education. The query also holds learning outcome data
from international learning assessments (PISA, TIMSS, etc.), equity data
from household surveys, and projection data to 2050."

`edstats_2019.csv` was compiled on 24 April 2020 and lists
indicators reported between 2010 and 2019.
First, let's load the dataset:

```{r edstats_wide_1}
edstats_2019 <- read.csv("datasets/edstats_2019.csv",
    comment.char="#")
head(edstats_2019)
```

This data frame is in a "long" format, where each indicator
for each country is given in its own row. Note that some indicators
are not surveyed/updated every year.

{ BEGIN exercise }
Convert `edstats_2019` to a "wide" format (one row per country,
each indicator in its own column) based on the most recent observed indicators.
{ END exercise }

{ BEGIN solution }
First we need a function that returns the last non-missing value in a
given numeric vector.
To recall, `na.omit()`, removes all missing values and `tail()` can be
used to access the last observation easily. Unfortunately, if the vector
is consists of missing values only, the removal of `NA`s leads
to an empty sequence. However, the trick we can use is that
by extracting the first element from an empty vector by using
`[...]`, we get a `NA`.


```{r edstats_wide_2}
last_non_na <- function(x) tail(na.omit(x), 1)[1]
last_non_na(c(1, 2, NA, 3,NA,NA)) # example 1
last_non_na(c(NA,NA,NA,NA,NA,NA)) # example 2
```

Let's extract the most recent indicator from each row in `edstats_2019`.

```{r edstats_wide_3}
values <- apply(edstats_2019[-(1:4)], 1, last_non_na)
head(values)
```

Now, we shall create a data frame with 3 columns:
name of the country, indicator code, indicator value.
Let's order it with respect to the first two columns.

```{r edstats_wide_4}
edstats_2019 <- edstats_2019[c("CountryName", "Code")]
# add a new column at the righthand end:
edstats_2019["Value"] <- values
edstats_2019 <- edstats_2019[
    order(edstats_2019$CountryName, edstats_2019$Code), ]
head(edstats_2019)
```


To convert the data frame to a "wide" format, many readers would choose
the `pivot_wider()` function from the `tidyr` package
(amongst others).


```{r edstats_wide_5}
library("tidyr")
edstats <- as.data.frame(
    pivot_wider(edstats_2019, names_from="Code", values_from="Value")
)
edstats[1, 1:7]
```

. . .

On a side note (\*), the above solution is of course perfectly fine
and we can now live long and prosper.
Nevertheless, we are here to learn new skills, so let's note
that it has the drawback that it required
us to search for the answer on the internet (and go through many "answers"
that actually don't work). If we are not converting between the long and the
wide formats on a daily basis, this might not be worth the hassle
(moreover, there's no guarantee that this function will work the same
way in the future, that the package we relied on will provide the same API etc.).

Instead, by relaying on a bit deeper knowledge of R programming
(which we already have, see Appendices A-D of our book),
we could implement the relevant procedure manually. The downside is that
this requires us to get out of our comfort zone and... think.

First, let's generate the list of all countries and indicators:

```{r edstats_wide_6}
countries  <- unique(edstats_2019$CountryName)
head(countries)
indicators <- unique(edstats_2019$Code)
head(indicators)
```

Second, note that `edstats_2019` gives all the possible combinations (pairs)
of the indexes and countries:

```{r edstats_wide_7}
nrow(edstats_2019) # number of rows in edstats_2019
length(countries)*length(indicators) # number of pairs
```

Looking at the numbers in the `Value` column of `edstats_2019`,
this will exactly provide us with our desired "wide" data matrix,
if we read it in a rowwise manner. Hence, we can use
`matrix(..., byrow=TRUE)` to generate it:

```{r edstats_wide_8}
# edstats_2019 is already sorted w.r.t. CountryName and Code
edstats2 <- cbind(
    CountryName=countries, # first column
    as.data.frame(
        matrix(edstats_2019$Value,
            byrow=TRUE,
            ncol=length(indicators),
            dimnames=list(NULL, indicators)
    )))
identical(edstats, edstats2)
```

{ END solution }




{ BEGIN exercise }
Export `edstats` to a CSV file.
{ END exercise }

{ BEGIN solution }

This can be done as follows:

```{r edstats_wide_9,eval=FALSE}
write.csv(edstats, "edstats_2019_wide.csv", row.names=FALSE)
```

We didn't export the row names, because they're useless in our case.

{ END solution }





{ BEGIN exercise }
Explore `edstats_meta.csv` to understand the meaning of the
EdStats indicators.
{ END exercise }

{ BEGIN solution }

First, let's load the dataset:

```{r edstats_wide_10}
meta <- read.csv("datasets/edstats_meta.csv")
names(meta) # column names
```

The `Series` column deciphers each indicator's meaning.
For instance, `LO.PISA.MAT` gives:

```{r edstats_wide_11}
meta[meta$Code=="LO.PISA.MAT", "Series"]
```

To get more information, we can take a look at the `Definition`
column:

```{r edstats_wide_12,eval=FALSE}
meta[meta$Code=="LO.PISA.MAT", "Definition"]
```

which reads:
*`r meta[meta$Code=="LO.PISA.MAT", "Definition"]`*.

{ END solution }




### EdStats -- Where Girls Are Better at Maths Than Boys?


In this task we will consider the "wide" version of the EdStats
dataset:

```{r girlsboys1}
edstats <- read.csv("datasets/edstats_2019_wide.csv",
    comment.char="#")
edstats[1, 1:6]
meta <- read.csv("datasets/edstats_meta.csv")
```


This dataset is small, moreover, we'll be more interested  in the description
(understanding) of data, not prediction of the response variable
to unobserved samples. Note that we have the *population* of the World
countries at hand here (new countries do not arise on a daily basis).
Therefore, a train-test split won't be performed.

<!--
```{r}
#pairs(edstats[c("LO.PISA.MAT", "LO.PISA.SCI", "LO.PISA.REA")])
```
-->




{ BEGIN exercise }
Add a 0/1 factor-type variable `girls_rule_maths` that is equal to 1
if and only if a country's average score of 15-year-old female students on the PISA
mathematics scale is greater than the corresponding indicator for the male ones.
{ END exercise }

{ BEGIN solution }

Recall that a conversion of a logical value to a number
yields 1 for `TRUE` and `0` for `FALSE`. Hence:

```{r girlsboys2}
edstats$girls_rule_maths <-
    factor(as.numeric(
        edstats$LO.PISA.MAT.FE>edstats$LO.PISA.MAT.MA
    ))
head(edstats$girls_rule_maths, 10)
```

Unfortunately, there are many missing values in the dataset.
More precisely:

```{r girlsboys3}
sum(is.na(edstats$girls_rule_maths)) # count
mean(is.na(edstats$girls_rule_maths)) # proportion
```

Countries such as Egypt, India, Iran or Venezuela
are not amongst the 79 members of the Programme for International
Student Assessment. Thus, we'll have to deal with the data we have.

The percentage of counties where "girls rule" is equal to:

```{r girlsboys4}
mean(edstats$girls_rule_maths==1, na.rm=TRUE)
```

Here is the list of those counties:

```{r girlsboys5}
as.character(na.omit(
    edstats[edstats$girls_rule_maths==1, "CountryName"]
))
```

{ END solution }




{ BEGIN exercise }
Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.
{ END exercise }

{ BEGIN solution }

Let's first create a subset of `edstats` that doesn't include
the country names as well as the boys' and girls' math scores.

```{r girlsboys6}
edstats_subset <- edstats[!(names(edstats) %in%
    c("CountryName", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA"))]
```

Fitting and plotting (see Figure \@ref(fig:girlsboys7tree))
of the tree can be performed as follows:

```{r girlsboys7tree,echo=-(1:2),fig.cap="A decision tree explaining the `girls_rule_maths` variable"}
set.seed(123)
par(ann=FALSE)
library("rpart")
library("rpart.plot")
tree <- rpart(girls_rule_maths~., data=edstats_subset,
    method="class", model=TRUE)
rpart.plot(tree)
```

The variables included in the model are:

```{r girlsboys8,results='asis',echo=FALSE}
for (var in levels(tree$frame$var)[-1]) {
    cat(sprintf("* %s: *%s*\n", var, meta$Series[meta$Code==var]))
}
```

Note that the decision rules are well-interpretable, we can make a whole
story around it. Whether or not it is actually true -- is a different... story.

To compute the basic classifier performance scores,
let's recall the `get_metrics()` function:

```{r girlsboys9}
get_metrics <- function(Y_pred, Y_test)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```

Now we can judge the tree's character:

```{r girlsboys10}
Y_pred <- predict(tree, edstats_subset, type="class")
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```



{ END solution }


{ BEGIN exercise }
Learn a decision tree that this time doesn't rely on any of the PISA indicators.
{ END exercise }

{ BEGIN solution }

Let's remove the unwanted variables:

```{r girlsboys11,echo=-1,fig.cap="Another decision tree explaining the `girls_rule_maths` variable"}
set.seed(123)
edstats_subset <- edstats[!(names(edstats) %in%
    c("LO.PISA.MAT", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA",
      "LO.PISA.REA", "LO.PISA.REA.FE", "LO.PISA.REA.MA",
      "LO.PISA.SCI", "LO.PISA.SCI.FE", "LO.PISA.SCI.MA",
      "CountryName"))]
```

On a side note, this could be done more easily by calling, e.g.,
`stri_startswith_fixed(names(edstats), "LO.PISA")` from the `stringi` package.

Fitting and plotting (see Figure \@ref(fig:girlsboys11tree)) of the tree:

```{r girlsboys11tree,echo=-1,fig.cap="Another decision tree explaining the `girls_rule_maths` variable"}
par(ann=FALSE)
tree <- rpart(girls_rule_maths~., data=edstats_subset,
    method="class", model=TRUE)
rpart.plot(tree)
```

Performance metrics:

```{r girlsboys12}
Y_pred <- predict(tree, edstats, type="class")
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```

It's interesting to note that some of the
goodness-of-fit measures are actually higher now.

The variables included in the model are:

```{r girlsboys13,results='asis',echo=FALSE}
for (var in levels(tree$frame$var)[-1]) {
    cat(sprintf("* %s: *%s*\n", var, meta$Series[meta$Code==var]))
}
```

{ END solution }



### EdStats and World Factbook -- Joining Forces

In the course of our data science journey, we have considered two datasets
dealing with country-level indicators: the World Factbook and
World Bank's EdStats.


```{r joinedstats1}
factbook <- read.csv("datasets/world_factbook_2020.csv",
    comment.char="#")
edstats <- read.csv("datasets/edstats_2019_wide.csv",
    comment.char="#")
```

Let's combine the information they provide
and see if we come up with a better model of where
girls' math scores are higher.

<!--
#factbook$country[!(factbook$country %in% edstats$CountryName)]
#edstats$CountryName[!(edstats$CountryName %in% factbook$country)]
-->

{ BEGIN exercise }
Some country names in one dataset don't match those in the other
one, for instance: Czech Republic vs. Czechia,
Myanmar vs. Burma, etc. Resolve these conflicts as best you can.
{ END exercise }

{ BEGIN solution }
To get a list of the mismatched country names, we can call either:

```{r joinedstats2,eval=FALSE}
factbook$country[!(factbook$country %in% edstats$CountryName)]
```

or:

```{r joinedstats3,eval=FALSE}
edstats$CountryName[!(edstats$CountryName %in% factbook$country)]
```

Unfortunately, the data need to be cleaned manually -- it's a tedious task.
The following consists of what we hope are the best matches
between the two datasets (yet, the list is not perfect;
in particular, the Republic of North Macedonia
is completely missing in one of the datasets):

```{r joinedstats4}
from_to <- matrix(ncol=2, byrow=TRUE, c(
# FROM (edstats)                  # TO (factbook)
"Brunei Darussalam"             , "Brunei"                            ,
"Congo, Dem. Rep."              , "Congo, Democratic Republic of the" ,
"Congo, Rep."                   , "Congo, Republic of the"            ,
"Czech Republic"                , "Czechia"                           ,
"Egypt, Arab Rep."              , "Egypt"                             ,
"Hong Kong SAR, China"          , "Hong Kong"                         ,
"Iran, Islamic Rep."            , "Iran"                              ,
"Korea, Dem. People’s Rep."     , "Korea, North"                      ,
"Korea, Rep."                   , "Korea, South"                      ,
"Kyrgyz Republic"               , "Kyrgyzstan"                        ,
"Lao PDR"                       , "Laos"                              ,
"Macao SAR, China"              , "Macau"                             ,
"Micronesia, Fed. Sts."         , "Micronesia, Federated States of"   ,
"Myanmar"                       , "Burma"                             ,
"Russian Federation"            , "Russia"                            ,
"Slovak Republic"               , "Slovakia"                          ,
"St. Kitts and Nevis"           , "Saint Kitts and Nevis"             ,
"St. Lucia"                     , "Saint Lucia"                       ,
"St. Martin (French part)"      , "Saint Martin"                      ,
"St. Vincent and the Grenadines", "Saint Vincent and the Grenadines"  ,
"Syrian Arab Republic"          , "Syria"                             ,
"Venezuela, RB"                 , "Venezuela"                         ,
"Virgin Islands (U.S.)"         , "Virgin Islands"                    ,
"Yemen, Rep."                   , "Yemen"
))
```

Conversion of the names:

```{r joinedstats5}
for (i in 1:nrow(from_to)) {
    edstats$CountryName[edstats$CountryName==from_to[i,1]] <- from_to[i,2]
}
```


On a side note (\*), this could be done with a single call to
a function in the `stringi` package:

```{r joinedstats6}
library("stringi")
edstats$CountryName <- stri_replace_all_fixed(edstats$CountryName,
    from_to[,1], from_to[,2], vectorize_all=FALSE)
```

{ END solution }


{ BEGIN exercise }
Merge (join) the two datasets based on the country names.
{ END exercise }

{ BEGIN solution }
This can be done by means of the `merge()` function.

```{r joinedstats7}
edbook <- merge(edstats, factbook, by.x="CountryName", by.y="country")
ncol(edbook) # how many columns we have now
```

{ END solution }



{ BEGIN exercise }
Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.
{ END exercise }

{ BEGIN solution }

We proceed as in one of the previous exercises:

```{r joinedstats8,echo=-1}
set.seed(123)
edbook$girls_rule_maths <-
    factor(as.numeric(
        edbook$LO.PISA.MAT.FE>edbook$LO.PISA.MAT.MA
    ))
edbook_subset <- edbook[!(names(edbook) %in%
    c("CountryName", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA"))]
```

Fitting and plotting (see Figure \@ref(fig:joinedstats9)):

```{r joinedstats9,echo=-1,fig.cap="Yet another decision tree explaining the `girls_rule_maths` variable"}
par(ann=FALSE)
library("rpart")
library("rpart.plot")
tree <- rpart(girls_rule_maths~., data=edbook_subset,
    method="class", model=TRUE)
rpart.plot(tree)
```

Performance metrics:

```{r joinedstats10}
Y_pred <- predict(tree, edbook_subset, type="class")
get_metrics(Y_pred, edbook_subset$girls_rule_maths)
```


The variables included in the model are:

```{r joinedstats11,results='asis',echo=FALSE}
for (var in levels(tree$frame$var)[-1]) {
    if (length(meta$Series[meta$Code==var])==1)
        cat(sprintf("* %s: *%s*\n", var, meta$Series[meta$Code==var]))
    else
        cat(sprintf("* %s\n", var))
}
```

This is... not at all enlightening.
Rest assured that experts in education
or econometrics for whom we work in this (imaginary) project
would raise many questions at this very point. Merely applying
some computational procedure on a dataset doesn't cut it;
it's too early to ask for a paycheque.
Classifiers are just blind *tools* in our gentle yet firm hands;
new questions are risen, new answers must be sought. Further
explorations are of course left as an exercise to the kind reader.

{ END solution }



### EdStats -- Fitting of Binary Logistic Regression Models


In this task we're going to
consider the "wide" version of the EdStats
dataset again:

```{r edstatslogi1}
edstats <- read.csv("datasets/edstats_2019_wide.csv",
    comment.char="#")
```

Let's re-add the `girls_rule_maths` column
just as in the previous exercise.
Then, let's create a subset of `edstats` that doesn't include
the country names as well as the boys' and girls' math scores.


```{r edstatslogi2}
edstats$girls_rule_maths <-
    factor(as.numeric(
        edstats$LO.PISA.MAT.FE>edstats$LO.PISA.MAT.MA
    ))
edstats_subset <- edstats[!(names(edstats) %in%
    c("CountryName", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA"))]
```

{ BEGIN exercise }
Fit and assess a logistic regression model for `girls_rule_maths` as a function
of `LO.PISA.REA.MA`+`LO.PISA.SCI`.
{ END exercise }


{ BEGIN solution }

Fitting of the model:

```{r edstatslogi3}
(f1 <- glm(girls_rule_maths~LO.PISA.REA.MA+LO.PISA.SCI,
    data=edstats_subset, family=binomial("logit")))
```

Performance metrics:

```{r edstatslogi4}
Y_pred <- as.numeric(predict(f1, edstats_subset, type="response")>0.5)
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```

Relate the above numbers to those reported for the fitted decision trees.

Note that the fitted model is nicely interpretable:
the lower the boys' average result on the Reading Scale
or the higher the country's result on the Science Scale,
the higher the probability for `girls_rule_maths`:

```{r edstatslogi5}
example_X <- data.frame(
    LO.PISA.REA.MA=c(475, 450, 475, 500),
    LO.PISA.SCI=   c(525, 525, 550, 500)
)
cbind(example_X,
    `Pr(Y=1)`=predict(f1, example_X, type="response"))
```

{ END solution }








{ BEGIN exercise }
(\*) Fit and assess a logistic regression model for `girls_rule_maths`
featuring all `LO.PISA.REA*` and `LO.PISA.SCI*` as independent variables.
{ END exercise }

{ BEGIN solution }


Model fitting:

```{r edstatslogi6}
(f2 <- glm(girls_rule_maths~LO.PISA.REA+LO.PISA.REA.FE+LO.PISA.REA.MA+
                            LO.PISA.SCI+LO.PISA.SCI.FE+LO.PISA.SCI.MA,
    data=edstats_subset, family=binomial("logit")))
```

The mysterious `fitted probabilities numerically 0 or 1 occurred` warning
denotes convergence problems of the underlying optimisation (fitting) procedure:
at least one of the model coefficients has had a fairly large order
of magnitude and hence the fitted probabilities has come very close to 0 or 1.
Recall that the probabilities are modelled by means of the logistic sigmoid
function applied on the output of a linear combination of the dependent variables.
Moreover, cross-entropy features a logarithm, and $\log 0 = -\infty$.

This can be due to the fact that all the variables in the model are very
correlated with each other (multicollinearity; an ill-conditioned problem).
The obtained solution might be unstable -- there might be many local optima
and hence, different parameter vectors might be equally good.
Moreover, it is likely that a small change in one of the inputs might
lead to large change in the estimated  model
(\* normally, we would attack this problem by employing
some regularisation techniques).

<!--
#(f2 <- glm(girls_rule_maths~LO.PISA.REA+LO.PISA.REA.FE+LO.PISA.REA.MA+
#                            LO.PISA.SCI+LO.PISA.SCI.FE+LO.PISA.SCI.MA,
#    data=edstats_subset, family=binomial("logit"), start=runif(7,-0.1,0.1)))
-->

Of course, the model's performance metrics can still be computed,
but then it's better if we treat it as a black box. Or, even better,
reduce the number of independent variables and come up with a simpler
model that serves its purpose better than this one.

```{r edstatslogi7}
Y_pred <- as.numeric(predict(f2, edstats_subset, type="response")>0.5)
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```

{ END solution }


<!--

standardise variables:  [too little benefit for the reader? omit]

```{r edstatslogis1}
#standardise <- function(x) {
#    (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
#}
```

```{r edstatslogis2}
#sedstats_subset <- edstats_subset # copy
#for (i in 1:(ncol(sedstats_subset)-1)) {
#    # standardise every column of sedstats_subset but
#    # the girls_rule_maths one (it must remain a binary==0/1 variable)
#    sedstats_subset[[i]] <- standardise(sedstats_subset[[i]])
#}
```

```{r edstatslogis3}
#(f1s <- glm(girls_rule_maths~LO.PISA.REA.MA+LO.PISA.SCI,
#    data=sedstats_subset, family=binomial("logit")))
#Y_pred <- as.numeric(predict(f1s, sedstats_subset, type="response")>0.5)
#get_metrics(Y_pred, sedstats_subset$girls_rule_maths)
```

```{r edstatslogis4}
#(f2s <- glm(girls_rule_maths~LO.PISA.REA+LO.PISA.REA.FE+LO.PISA.REA.MA+
#                            LO.PISA.SCI+LO.PISA.SCI.FE+LO.PISA.SCI.MA,
#    data=sedstats_subset, family=binomial("logit")))
#Y_pred <- as.numeric(predict(f2s, sedstats_subset, type="response")>0.5)
#get_metrics(Y_pred, sedstats_subset$girls_rule_maths)
```

-->




### EdStats -- Variable Selection in Binary Logistic Regression (\*)


Back to our `girls_rule_maths` example, we still  have so much to learn!

```{r edstatsvarsel1}
edstats <- read.csv("datasets/edstats_2019_wide.csv",
    comment.char="#")
edstats$girls_rule_maths <-
    factor(as.numeric(
        edstats$LO.PISA.MAT.FE>edstats$LO.PISA.MAT.MA
    ))
edstats_subset <- edstats[!(names(edstats) %in%
    c("CountryName", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA"))]
```


{ BEGIN exercise }
Construct a binary logistic regression model via forward selection
of variables.
{ END exercise }

{ BEGIN solution }
Just as in the linear regression case, we can rely on the `step()` function.

```{r edstatsvarsel5,error=TRUE}
model_empty <- girls_rule_maths~1
(model_full <- formula(model.frame(girls_rule_maths~.,
    data=edstats_subset)))
f <- step(glm(model_empty, data=edstats_subset, family=binomial("logit")),
    scope=model_full, direction="forward")
```

Melbourne, we have a problem!
Our dataset has too many missing values, and those cannot be present
in a logistic regression model (it's based on a linear combination of variables,
and sums/products involving `NA`s yield `NA`s...).

Looking at the manual of `?step`, we see that the default
`NA` handling is via `na.omit()`, and that, when applied on a data frame,
results in the removal of all the *rows*, where there is at least one `NA`.
Sadly, it's too invasive.

We should get rid of the data blanks manually.
First, definitely, we should remove all the rows where
`girls_rule_maths` is unknown:

```{r edstatsvarsel2}
edstats_subset <-
    edstats_subset[!is.na(edstats_subset$girls_rule_maths),]
```

We are about to apply the forward selection process, whose purpose is
to choose variables for a model. Therefore, instead of removing any more
rows, we should remove the... columns with missing data:

```{r edstatsvarsel3}
edstats_subset <-
    edstats_subset[,colSums(sapply(edstats_subset, is.na))==0]
```

(\*) Alternatively, we could apply some techniques of missing data imputation;
this is beyond the scope of this book. For instance, `NA`s could be
replaced by the averages of their respective columns.


We are ready now to make use of `step()`.


```{r edstatsvarsel6}
model_empty <- girls_rule_maths~1
(model_full <- formula(model.frame(girls_rule_maths~.,
    data=edstats_subset)))
f <- step(glm(model_empty, data=edstats_subset, family=binomial("logit")),
    scope=model_full, direction="forward")
print(f)
Y_pred <- as.numeric(predict(f, edstats_subset, type="response")>0.5)
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```



{ END solution }







{ BEGIN exercise }
Choose a model via backward elimination.
{ END exercise }

{ BEGIN solution }
Having a dataset with missing values removed, this is easy now:


<!--
makes no difference:

# standardise <- function(x) {
#     (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
# }
# for (i in 1:(ncol(edstats_subset)-1)) {
#    # standardise every column of sedstats_subset but
#    # the girls_rule_maths one (it must remain a binary==0/1 variable)
#    edstats_subset[[i]] <- standardise(edstats_subset[[i]])
# }
-->

```{r edstatsvarsel8}
f <- suppressWarnings( # yeah, yeah, yeah...
        # fitted probabilities numerically 0 or 1 occurred
    step(glm(model_full, data=edstats_subset, family=binomial("logit")),
        scope=model_empty, direction="backward")
)
```

The obtained model and its quality metrics:

```{r edstatsvarsel9}
print(f)
Y_pred <- as.numeric(predict(f, edstats_subset, type="response")>0.5)
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```

Note that we got a better (lower) AIC than in the forward selection
case, which means that backward elimination was better this time.
On the other hand, we needed to suppress the
`fitted probabilities numerically 0 or 1 occurred` warnings.
The returned model is perhaps *unstable* as well and consists of too
many variables.

{ END solution }













## Outro

### Remarks




Other prominent classification algorithms:

- Naive Bayes and other probabilistic approaches,
- Support Vector Machines (SVMs) and other kernel methods,
- (Artificial) (Deep) Neural Networks.





Interestingly, in the next chapter we will  note that the logistic regression model
is a special case of a *feed-forward single layer neural network*.

We will also generalise the binary logistic regression to the case of
a multiclass classification.


The state-of-the art classifiers called
*Random Forests* and *XGBoost* (see also: *AdaBoost*) are based on decision trees.
They tend to be more accurate but -- at the same time -- they fail to
exhibit the decision trees' important feature: interpretability.

Trees can also be used for regression tasks, see R package `rpart`.



### Further Reading

Recommended further reading: [@islr: Chapters 4 and 8]

Other: [@esl: Chapters 4 and 7 as well as (\*) Chapters 9,  10,  13, 15]
