# Clustering

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->



## Unsupervised Learning

### Introduction





In **unsupervised learning** (learning without a teacher),
the input data points $\mathbf{x}_{1,\cdot},\dots,\mathbf{x}_{n,\cdot}$
are not assigned any reference labels (compare Figure {@fig:unsupervised}).




```{r unsupervised,echo=FALSE,fig.cap="Unsupervised learning: \"But what it is exactly that I have to do here?\""}
plot(iris[,3], iris[,2], ann=FALSE)
```

Our aim now is to discover the **underlying structure in the data**,
whatever that means.






### Main Types of Unsupervised Learning Problems


It turns out, however, that certain classes of unsupervised learning
problems are not only intellectually stimulating,
but practically useful at the same time.




In particular, in **dimensionality reduction** we seek a meaningful
*projection* of a high dimensional
space (think: many variables/columns).

```{r princomp,echo=FALSE,fig.cap="Principal component analysis (a dimensionality reduction technique) applied on three features of red wines",fig.height=6}
wines <- read.csv("datasets/winequality-all.csv", comment="#")
wines <- wines[wines$color=='red',]
p <- princomp(wines[,c(11, 4, 12)])
biplot(p, col=c("#ff000033",1), pch=1, xlim=c(-0.05, 0.23), main=NA)
```

For instance, Figure {@fig:princomp} reveals that the
"alcohol", "response" and "residual.sugar" dimensions of the Wine Quality
dataset that we have studied earlier on can actually be nicely depicted (with
no much loss of information) on a two-dimensional plot.
It turns out that the wine experts' opinion on a wine's quality is highly
correlated with the amount of... alcohol in a bottle.
On the other hand, sugar is orthogonal (unrelated) to these two.

Amongst example dimensionality reduction methods we find:

- Multidimensional scaling (MDS)
- Principal component analysis (PCA)
- Kernel PCA
- t-SNE
- Autoencoders (deep learning)

See, for example, [@esl] for more details.

. . .



Furthermore, in **anomaly detection**, our task is to identify rare, suspicious, ab-normal
or out-standing items.
For example, these can be cars on walkways in a park's security camera footage.


```{r anomaly_detection,echo=FALSE,fig.cap="Outliers can be thought of anomalies of some sort"}
plot(iris[,3], iris[,2], ann=FALSE)
idx <- c(61, 42, 16, 118, 132)
points(iris[idx,3], iris[idx,2], cex=3, pch=4, col=2)
```


. . .

Finally, the aim of **clustering** is to automatically discover some *naturally occurring*
subgroups in the data set.
For example, these may be customers having different shopping patterns
(such as "young parents", "students", "boomers").



```{r clustering_illustration,echo=FALSE,cache=TRUE,fig.cap="NEWS FLASH! SCIENTISTS SHOWED (by writing about it) THAT SOME VERY IMPORTANT THING (Iris dataset) COMES IN THREE DIFFERENT FLAVOURS (by applying the 3-means clustering algorithm)!"}
km <- kmeans(iris[,c(3,2)], centers=3, nstart=10)
plot(iris[,3], iris[,2], ann=FALSE,
    col=km$cluster, pch=km$cluster)
```






### Definitions




Formally, given $K\ge 2$, **clustering** aims is to find a *special kind*
of a **$K$-partition** of the input data set $\mathbf{X}$.

Definition.

:   We say that $\mathcal{C}=\{C_1,\dots,C_K\}$ is a **$K$-partition**
    of  $\mathbf{X}$ of size $n$,
    whenever:

    - $C_k\neq\emptyset$ for all $k$ (each set is nonempty),
    - $C_k\cap C_l=\emptyset$ for all $k\neq l$ (sets are pairwise disjoint),
    - $\bigcup_{k=1}^K C_k=\mathbf{X}$ (no point is neglected).


This can also be thought of as assigning each point a unique label $\{1,\dots,K\}$
(think: colouring of the points, where each number has a colour).
We will consider the point $\mathbf{x}_{i,\cdot}$ as labelled $j$
if and only if it belongs to cluster $C_j$, i.e., $\mathbf{x}_{i,\cdot}\in C_j$.






Example applications of clustering:


- *taxonomisation*: e.g.,
 partition the consumers to more "uniform"
 groups to better understand who they are and what do they need,
- *image processing*:
 e.g., object detection, like tumour tissues on medical images,
- *complex networks analysis*:
 e.g., detecting communities in friendship,
 retweets and other networks,
- *fine-tuning supervised learning algorithms*:
  e.g., recommender systems indicating content
  that was rated highly by users from the same group
  or learning multiple manifolds in a dimension reduction task.








The number of possible $K$-partitions of a set with $n$ elements is given by
*the Stirling number of the second kind*:

\[
\left\{{n \atop K}\right\}={\frac  {1}{K!}}\sum _{{j=0}}^{{K}}(-1)^{{K-j}}{\binom  {K}{j}}j^{n};
\]

e.g., already $\left\{{n \atop 2}\right\}=2^{n-1}-1$
and $\left\{{n \atop 3}\right\}=O(3^n)$ -- that is a lot.
Certainly, we are not just interested in "any" partition -- some of them
will be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:



"Definition".

: Clustering concerns "segmenting a collection of objects into subsets
so that those within each cluster are more **closely related**
to one another than objects assigned to different clusters" [@esl].

It is not uncommon <!-- TODO: cite -->
to equate the general definition of data clustering problems with... the
particular outputs yield by specific clustering algorithms. It some sense,
that sounds fair. From this perspective, we might be interested in
identifying the two main types of clustering algorithms:


- **parametric** (model-based):
    - find clusters of specific shapes or following specific multidimensional
    probability distributions,
    - e.g., $K$-means, expectation-maximisation for Gaussian mixtures (EM),
    average linkage agglomerative clustering;
- **nonparametric** (model-free):
    - identify high-density or well-separable regions,
    perhaps in the presence of noise points,
    - e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.



In this chapter we'll take a look at two classical approaches to clustering:

- *K-means clustering* that looks for a specific number of clusters,
- *(agglomerative) hierarchical clustering* that outputs a whole hierarchy
of nested data partitions.








## K-means Clustering


### Example in R




Let us start by applying the $K$-means clustering method to find $K=3$ groups
in the famous Fisher's `iris` data set (variables `Sepal.Width`
and `Petal.Length` variables only):

```{r kmeans1,echo=-1}
set.seed(123)
X <- as.matrix(iris[,c(3,2)])
# never forget to set nstart>>1!
km <- kmeans(X, centers=3, nstart=10)
km$cluster # labels assigned to each of 150 points:
```


Remark.

: Later we'll see that `nstart` is responsible for random restarting the
(local) optimisation procedure, just as we did in the previous chapter.





Let's draw a scatter plot that depicts the detected clusters:

```{r kmeans12, fig.cap="3-means clustering on a projection of the Iris dataset"}
plot(X, col=km$cluster)
```

The colours in Figure {@fig:kmeans12} indicate the detected clusters.
The left group is clearly well-separated from the other two.


What can we do with this information? Well, if we were experts on plants
(in the 1930s), that'd definitely be something ground-breaking.
Figure {@fig:kmeans123} is a version of the aforementioned scatter plot
now with the true iris species added.

```{r kmeans123, fig.cap="3-means clustering (colours) vs true Iris species (shapes)"}
plot(X, col=km$cluster, pch=as.numeric(iris$Species))
```


Here is a contingency table for detected clusters vs. true iris species:

```{r kmeans1234}
(C <- table(km$cluster, iris$Species))
```

It turns out that the discovered partition matches the original iris
species very well. We have just made a "discovery" in the field of
botany (actually some research fields classify their objects of study
into families, genres etc. by means of such tools).

Were the actual Iris species what we had hoped to match?
Was that our aim? Well, surely we have had begun our journey with
"clear minds" (yet with hungry eyes). Note that the true class labels
were not used during the clustering procedure -- we're dealing with
an unsupervised learning problem here. The result turned useful,
it's a win.

<!-- OK, this is not the best measure

sum(apply(C, 1, max))/sum(C) # "accuracy"

TODO: mention ARI and NMI

cite Comparing partitions

genieclust.compare_partitions ?

-->





### Problem Statement




The aim of $K$-means clustering is to find $K$ "good" cluster centres
$\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$.


Then, a point $\mathbf{x}_{i,\cdot}$ will be assigned to the
cluster represented by the closest centre.

Closest == w.r.t. the squared Euclidean distance.

Assuming all the points are in a $p$-dimensional space, $\mathbb{R}^p$,
\[
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}) = \| \mathbf{x}_{i,\cdot} - \boldsymbol\mu_{k,\cdot} \|^2 =  \sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\]





The $i$-th point's cluster is determined by:
\[
\mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}),
\]
where $\mathrm{arg}\min$ == argument minimum == the index $k$ that minimises
the given expression.







In the previous example, we have:

```{r}
km$centers
plot(X, col=km$cluster, asp=1) # asp=1 gives the same scale on both axes
points(km$centers, cex=2, col=4, pch=16)
```





Here is the partition of the whole $\mathbb{R}^2$ space
into clusters based on the closeness to the three cluster centres:

```{r echo=FALSE}
x <- seq(0, 8, length.out=250)
y <- seq(1.5, 5.0, length.out=250)
z <- matrix(NA_real_, length(x), length(y))
for (i in 1:length(x))
    for (j in 1:length(y))
        z[i,j] <- which.min(colSums(((t(km$centers)-c(x[i],y[j]))^2)))
image(x, y, z, ann=FALSE, asp=1,
    axes=FALSE, col=c("#00000044", "#ff000044", "#00ff0044"))
points(X, col=km$cluster)
points(km$centers, cex=2, col=4, pch=16)
```

Remark.

: (*) For the interested, see "Voronoi diagrams".




To compute the pairwise distances, we may call `pdist::pdist()`:

```{r}
library("pdist")
D <- as.matrix(pdist(X, km$centers))
head(D) # D[i,j] - distance between x[i,] and Âµ[j,]
```

...




Therefore, the cluster memberships ($\mathrm{arg}\min$s) can be determined by:

```{r}
(idx <- apply(D, 1, which.min))
all(km$cluster == idx) # sanity check
```


### Algorithms for the K-means Problem




How to find "good" cluster centres?

In the $K$-means clustering, we wish to find $\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$
that minimise the total within-cluster distances (distances from each point
to each own cluster centre):
\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{C(i),\cdot}),
\]
Note that the $\boldsymbol\mu$s are also "hidden" inside the point-to-cluster
belongingness mapping, $\mathrm{C}$.


Expanding the above yields:
\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]
Unfortunately, the $\min$ operator in the objective function
makes this optimisation problem not tractable
with the methods discussed in the previous chapter.





The above problem is *hard* to solve.


Remark.

: (*) More precisely, it is an NP-hard problem.


Therefore, in practice we use various heuristics to solve it.

`kmeans()` in R implements the Hartigan-Wong, Lloyd, Forgy and MacQueen
algorithms.



Remark.

: (*) Technically, there is no such thing as "the K-means algorithm" --
all the aforementioned methods are particular heuristic
approaches to solving the K-means
clustering problem as specified by the aforementioned optimisation task.
By setting `nstart = 10` above, we ask the (Hartigan-Wong) algorithm
to find 10 solution candidates obtained by considering different random
initial clusterings and choose the best one (with respect to the sum
of within-cluster distances) amongst them. This does not guarantee
finding the optimal solution, especially in high-dimensional spaces,
but increases the likelihood of such.





Lloyd's algorithm (1957) is sometimes referred to as "the" K-means algorithm:

1. Start with random cluster centres $\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$.

2. For each point $\mathbf{x}_{i,\cdot}$, determine its closest centre $C(i)\in\{1,\dots,K\}$.

3. For each cluster $k\in\{1,\dots,K\}$, compute the new cluster centre $\boldsymbol\mu_{k,\cdot}$ as the componentwise arithmetic mean
of the coordinates of all the point indices $i$ such that $C(i)=k$.

4. If the cluster centres changed since the last iteration, go to step 2, otherwise stop and return the result.




(*) Example implementation:

```{r echo=-1}
set.seed(123)
K <- 3

# Random initial cluster centres
M <- jitter(X[sample(1:nrow(X), K),])
M
```

```{r}
# Let D[i,k] bet the distance between the i-th point
# and the k-th centre:
D <- as.matrix(pdist(X, M))
# Let idx[i] be the centre closest to the i-th point
idx <- apply(D, 1, which.min)
```





```{r}
repeat {
    # Previous cluster belongingness:
    old_idx <- idx
    # Split X into a list of K data frames based on old_idx info
    X_split <- split(as.data.frame(X), old_idx)
    # Compute componentwise arithmetic means of each data frame - new centres
    M <- t(sapply(X_split, colMeans))
    # Recompute cluster belongingness
    D <- as.matrix(pdist(X, M))
    idx <- apply(D, 1, which.min)
    # Check if converged already:
    if (all(idx == old_idx)) break
}
```




```{r}
M # our result
km$center # result of kmeans()
```







<!--

# TODO: k-medoids???

## TODO




TODO maybe k-medoids, nice for optimisation
plus they allow for different metrics

alternatively, DBSCAN or PCA or MDS or ...



-->




## Hierarchical Methods


### Introduction




In K-means, we need to specify the number of clusters, $K$, in advance.

What if we don't know it?

There is no guarantee that a $(K+1)$-partition is "similar" to the $K$-one.

Hierarchical methods, on the other hand, output a whole hierarchy
of mutually *nested* partitions, which increase the interpretability of the results.

A $K$-partition for any $K$ can be extracted later.





Here we are interested in **agglomerative** algorithms.

At the lowest level of the hierarchy, each point belongs to its own
cluster (there are $n$ singletons).

At the highest level of the hierarchy, there is one cluster containing all the points.


Moving from the $i$-th to the $(i+1)$-th level,
we select a pair of clusters to be merged.



### Example in R




```{r dist_hclust_complete}
# Distances between all pairs of points:
D <- dist(X)
# Apply Complete Linkage (the default, see below):
h <- hclust(D) # method="complete"
print(h)
```





```{r cutree}
cutree(h, k=3) # extract the 3-partition
```





Different cuts of the hierarchy:

```{r complete_linkage_hier5_intro,fig.height=6}
par(mfrow=c(2,2))
plot(X, col=cutree(h, k=5), ann=FALSE)
legend("top", legend="k=5", bg="white")
plot(X, col=cutree(h, k=4), ann=FALSE)
legend("top", legend="k=4", bg="white")
plot(X, col=cutree(h, k=3), ann=FALSE)
legend("top", legend="k=3", bg="white")
plot(X, col=cutree(h, k=2), ann=FALSE)
legend("top", legend="k=2", bg="white")
```





A **dendrogram** depicts the distances (* as defined by the linkage function)
between the clusters merged at every stage. This can provide us with the insight
into the underlying data structure.

```{r complete_linkage_hier_intro}
plot(h, labels=FALSE, xlab=NA, main=NA); box()
```


### Agglomerative Hierarchical Clustering





Initially,  $\mathcal{C}^{(0)}=\{\{1\},\dots,\{n\}\}$,
i.e., each point is a member of its own cluster.

While a **hierarchical agglomerative clustering** algorithm is being computed,
there are $n-k$ clusters at the $k$-th step of the procedure,
$\mathcal{C}^{(k)}=\{C_1^{(k)},\dots,C_{n-k}^{(k)}\}$.


When proceeding from step $k$ to $k+1$, we determine $C_u^{(k)}$ and $C_v^{(k)}$, $u<v$,
to be **merged** together so that we get:
\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]


Thus, $(\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})$ is a sequence
of **nested** partitions of $\{1,2,\dots,n\}$
with $\mathcal{C}^{(n-1)}=\left\{ \{1,2,\dots,n\} \right\}$.


### Linkage Functions




A pair of clusters $C_u^{(k)}$ and $C_v^{(k)}$ to be merged with each other
is determined by:
\[
\mathrm{arg}\min_{u < v} d^*(C_u^{(k)}, C_v^{(k)}),
\]
where $d^*(A,B)$ is the *distance* between two clusters $A$ and $B$.

Note that we usually only consider the distances between single points,
not sets of points.

$d^*$ is a suitable extension of a pointwise distance $d$ (usually the Euclidean metric)
to whole sets.

We will assume that $d^*(\{\mathbf{x}_{i,\cdot}\},\{\mathbf{x}_{j,\cdot}\})=
d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})$, i.e., the distance between
singleton clusters is the same as the distance between the points themselves.





There are many popular choices of $d^*$ (which in the context of
hierarchical clustering we call a **linkage function**):


- single linkage:
\[
d_S^*(A,B) = \min_{\mathbf{a}\in A, \mathbf{b}\in B} d(\mathbf{a},\mathbf{b}),
\]
- complete linkage:
\[
d_C^*(A,B) = \max_{\mathbf{a}\in A, \mathbf{b}\in B} d(\mathbf{a},\mathbf{b}),
\]
- average linkage:
\[
d_A^*(A,B) = \frac{1}{|A| |B|} \sum_{\mathbf{a}\in A}\sum_{\mathbf{b}\in B} d(\mathbf{a},\mathbf{b}).
\]


An illustration of the way different linkages are computed
is given in Figure {@fig:linkages}.

```{r linkages,echo=FALSE,fig.cap="In single linkage, we find the closest pair of points; in complete linkage, we seek the furthest pair; in average linkage, we determine the average pairwise distance"}

set.seed(123)
X <- jitter(t(as.matrix(iris[,2:3])))
y <- as.numeric(iris[,5])
X[,y==3] <- c(2, 1)*X[,y==3]+c(-0.5,-0.5)
y[y==3] <- 4

d <- as.matrix(dist(t(X)))
d12 <- d[1:50, 51:100]
d23 <- d[51:100, 101:150]
d13 <- d[1:50, 101:150]

kol1 <- rgb(0,0.2,0,0.2)
kol2 <- rgb(0,0.5,0.0,0.5)
kol3 <- rgb(0,0.9,0.0,0.9)

par(mfrow=c(1,3))
# single
plot(X[1,], X[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
# stopifnot(min(d23) < min(d13) && min(d23) < min(d12))
IDX_single <- which(min(d23) == d, arr.ind=TRUE)[1,]
lines(X[1,IDX_single], X[2,IDX_single], col=kol3, lwd=3)
IDX_single <- which(min(d13) == d, arr.ind=TRUE)[1,]
lines(X[1,IDX_single], X[2,IDX_single], col=kol1, lwd=3)
IDX_single <- which(min(d12) == d, arr.ind=TRUE)[1,]
lines(X[1,IDX_single], X[2,IDX_single], col=kol2, lwd=3)
legend("bottomright", legend="single linkage", bg="white")
points(X[1,], X[2,], col=y, pch=y)

# complete
plot(X[1,], X[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
IDX_single <- which(max(d23) == d, arr.ind=TRUE)[1,]
lines(X[1,IDX_single], X[2,IDX_single], col=kol2, lwd=3)
IDX_single <- which(max(d13) == d, arr.ind=TRUE)[1,]
lines(X[1,IDX_single], X[2,IDX_single], col=kol1, lwd=3)
IDX_single <- which(max(d12) == d, arr.ind=TRUE)[1,]
lines(X[1,IDX_single], X[2,IDX_single], col=kol3, lwd=3)
legend("bottomright", legend="complete linkage", bg="white")
points(X[1,], X[2,], col=y, pch=y)

# average
plot(X[1,], X[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
#c(mean(d12), mean(d13), mean(d23))
for (i in sample(1:50)) {
    segments(X[1,0+i], X[2,0+i], X[1,51:100], X[2, 51:100], col=kol1)
    segments(X[1,0+i], X[2,0+i], X[1,101:150], X[2, 101:150], col=kol2)
    segments(X[1,50+i], X[2, 50+i], X[1,101:150], X[2, 101:150], col=kol3)
}
legend("bottomright", legend="average linkage", bg="white")
points(X[1,], X[2,], col=y, pch=y)
```








Complete linkage (again):

```{r complete_linkage_hier5,fig.height=6}
X <- as.matrix(iris[,c(3,2)])
D <- dist(X)

h <- hclust(D, method="complete")
par(mfrow=c(2, 2))
plot(X, col=cutree(h, k=5), ann=FALSE)
legend("top", legend="k=5", bg="white")
plot(X, col=cutree(h, k=4), ann=FALSE)
legend("top", legend="k=4", bg="white")
plot(X, col=cutree(h, k=3), ann=FALSE)
legend("top", legend="k=3", bg="white")
plot(X, col=cutree(h, k=2), ann=FALSE)
legend("top", legend="k=2", bg="white")
```





```{r complete_linkage_hier}
plot(h, labels=FALSE, xlab=NA, main=NA); box()
```




Average linkage:

```{r average_linkage_hier5,fig.height=6}
h <- hclust(D, method="average")
par(mfrow=c(2,2))
plot(X, col=cutree(h, k=5), ann=FALSE)
legend("top", legend="k=5", bg="white")
plot(X, col=cutree(h, k=4), ann=FALSE)
legend("top", legend="k=4", bg="white")
plot(X, col=cutree(h, k=3), ann=FALSE)
legend("top", legend="k=3", bg="white")
plot(X, col=cutree(h, k=2), ann=FALSE)
legend("top", legend="k=2", bg="white")
```





```{r average_linkage_hier}
plot(h, labels=FALSE, xlab=NA, main=NA); box()
```





Single linkage (this is a typical behaviour!):

```{r single_linkage_hier5,fig.height=6}
h <- hclust(D, method="single")
par(mfrow=c(2, 2))
plot(X, col=cutree(h, k=5), ann=FALSE)
legend("top", legend="k=5", bg="white")
plot(X, col=cutree(h, k=4), ann=FALSE)
legend("top", legend="k=4", bg="white")
plot(X, col=cutree(h, k=3), ann=FALSE)
legend("top", legend="k=3", bg="white")
plot(X, col=cutree(h, k=2), ann=FALSE)
legend("top", legend="k=2", bg="white")
```





```{r single_linkage_hier}
plot(h, labels=FALSE, xlab=NA, main=NA); box()
```




<!--Single linkage clustering --
a {\color{blue2}\bf hierarchical agglomerative clustering algorithm}.


\bigskip
{\color{pwsloneczny}\hrule height 2px}
\bigskip

\begin{itemize}


   \item




   \pause\item


% \vspace*{1em}
\end{itemize}-->

<!--Single linkage criterion:
% -- for some \textit{extension} $\tilde{\mathsf{d}}$ of $\mathsf{d}$ to $2^\mathcal{X}$:
$
\displaystyle\argmin_{(u,v), u < v}\ \min\left\{ \mathsf{d}\left(\mathbf{x}^{(i_u)}, \mathbf{x}^{(i_v)}\right):
\mathbf{x}^{(i_u)}\in C_u^{(j)},
\mathbf{x}^{(i_v)}\in C_v^{(j)}
\right\}.
$-->




## Outro

### Remarks




Unsupervised learning is often performed during the data pre-processing
and exploration stage.
Assessing the quality of clustering is particularly challenging as,
unlike in a supervised setting,
we have no access to "ground truth" information.



In practice, we often apply different clustering algorithms
and just see where they lead us. There's no teacher that would
tell us what we should do, so whatever we do is awesome, right?
Well, not precisely. Most frequently, you, my dear reader, will work
for some party that's genuinely
interested in your explaining why did you spent the last month coming up
with nothing useful at all. Thus, the main body of work related to proving
the use-ful*l*/less-ness will be on us.



The aim of K-means is to find
$K$ clusters based on the notion of the points' closeness
to the cluster centres. Remember that $K$ must be set in advance.

By definition (\* via its relation to Voronoi diagrams), all clusters will be of convex shapes.

However, we may try applying $K'$-means for $K' \gg  K$
to obtained a "fine grained" data compression and then
combine the (sub)clusters into more meaningful groups
using other methods.

Iterative K-means algorithms are very fast even for large data sets,
but they may fail to find a desirable solution, see the next chapter
for discussion.





On the other hand, hierarchical methods output
a whole family of mutually nested partitions, which may provide us
with insight into the underlying data structure.

Unfortunately, there is no easy way to assign new points
to existing clusters.

Linkage scheme must be chosen with care.

These are generally slow -- $O(n^2)$ to $O(n^3)$ complexity.


Remark.

: Note that the `fastcluster` package provides a more efficient
implementation of some methods available via a call to `hclust()`.
See also the `genie` package for a robust algorithm
based on the minimum spanning tree, which can be computed quickly.

```{r echo=FALSE,message=FALSE}
library("fastcluster")
```




Moreover, some unsupervised methods do not easily generalise
to unobserved data. However, you can then build a classifier (e.g., trees, neural networks) that learns the discovered labels.

Also many clustering methods are based on the notion of pairwise
distances but these tend to behave weirdly in high-dimensional
spaces ("the curse of dimensionality").

However, clustering methods can aid with supervised tasks
-- instead of fitting a single "large model", it might be
useful to fit separate models to each cluster.




### Other Noteworthy Clustering Algorithms

Other noteworthy clustering approaches:

- Genie (see R package `genie`) [@genie; @genieowa]
- ITM [@itm]
- DBSCAN, HDBSCAN* [@pre_dbscan; @dbscan; @hdbscan]
- K-medoids, K-medians
- Fuzzy C-means (a.k.a. weighted K-means) [@cmeans]
- Spectral clustering; e.g., [@spectral_nips]
- BIRCH [@birch]




### Further Reading


Recommended further reading: [@islr: Section 10.3]

Other: [@esl: Section 14.3]
